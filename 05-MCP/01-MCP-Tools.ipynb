{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔧 LangGraph MCP (Model Context Protocol) 완전 정복 튜토리얼\n",
    "\n",
    "## 📚 개요\n",
    "\n",
    "**MCP(Model Context Protocol)** 는 **AI 에이전트가 외부 도구와 소통하는 표준화된 방법** 을 제공하는 혁신적인 프로토콜입니다! 🌐\n",
    "\n",
    "### 🏗️ MCP가 혁신적인 이유\n",
    "\n",
    "기존 AI 시스템에서는 각각의 도구나 서비스마다 다른 연결 방식을 사용해야 했습니다. 하지만 MCP는 다릅니다:\n",
    "\n",
    "- **🔧 표준화된 인터페이스**: 모든 도구가 동일한 방식으로 연결\n",
    "- **⚡ 동적 도구 발견**: 런타임에 사용 가능한 도구를 자동으로 찾아서 활용\n",
    "- **🌐 다양한 전송 방식**: stdio, HTTP, WebSocket 등 환경에 맞는 연결 방법 지원\n",
    "- **🔄 확장 가능성**: 새로운 도구를 쉽게 추가하고 관리\n",
    "\n",
    "### 🎯 이 튜토리얼에서 배울 것들\n",
    "\n",
    "1. **🌟 MCP 기본 개념** - MCP가 무엇이고 왜 중요한지 이해하기\n",
    "2. **🏗️ MCP 서버 구축** - 실제 MCP 서버를 만들어보기\n",
    "3. **🤖 MultiServerMCPClient 활용** - 여러 서버를 효율적으로 관리하기\n",
    "4. **🚀 React Agent 통합** - LangGraph의 React Agent와 MCP 결합하기\n",
    "5. **⚙️ ToolNode 활용** - 세밀한 워크플로우 제어하기\n",
    "6. **🌐 실전 예제** - 복잡한 AI 에이전트 시스템 구축하기\n",
    "\n",
    "### 🎮 실생활 비유로 이해하기\n",
    "\n",
    "**MCP**를 **스마트폰의 앱 스토어**에 비유해보세요:\n",
    "\n",
    "```\n",
    "📱 스마트폰 (AI 에이전트)\n",
    "├── 📦 앱 스토어 (MCP 프로토콜) - 표준화된 앱 설치 방법\n",
    "├── 🛠️ 다양한 앱들 (MCP 서버들) - 각각의 기능을 제공\n",
    "└── 🔗 연결 관리자 (MultiServerMCPClient) - 모든 앱을 효율적으로 관리\n",
    "```\n",
    "\n",
    "### 💡 핵심 가치\n",
    "\n",
    "> **\\\"하나의 표준으로 모든 도구와 연결하기\\\"**\n",
    "> \n",
    "> _MCP를 통해 AI는 무한한 확장성을 갖게 됩니다_ ⚡\n",
    "\n",
    "## 📋 목차\n",
    "\n",
    "1. **🌟 MCP 개요 및 환경 설정**\n",
    "2. **🏗️ 기본 MCP 서버 이해하기**\n",
    "3. **🤖 MultiServerMCPClient 마스터하기**\n",
    "4. **🚀 React Agent와 MCP 완벽 통합**\n",
    "5. **⚙️ ToolNode로 고급 워크플로우 구축**\n",
    "6. **🌐 외부 MCP 서버 활용하기**\n",
    "7. **💡 실전 예제 - 복합 에이전트 시스템**\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 준비되셨나요?\n",
    "\n",
    "MCP의 강력한 세계로 함께 떠나봅시다! 🎯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MCP 핵심 개념 완전 이해하기 🌟\n",
    "\n",
    "### MCP(Model Context Protocol)란? 🤔\n",
    "\n",
    "**MCP**는 **AI 애플리케이션과 외부 도구 간의 소통을 표준화**한 혁신적인 오픈 프로토콜입니다. \n",
    "\n",
    "### 🏢 기업 업무 시스템으로 이해하기\n",
    "\n",
    "현대 기업에서 다양한 부서가 협업하는 것을 생각해보세요:\n",
    "\n",
    "#### 🚫 **기존 방식의 문제점**\n",
    "```\n",
    "📞 영업부 ←→ 개발팀: 이메일로 소통\n",
    "💬 마케팅 ←→ 디자인: 슬랙으로 소통  \n",
    "📋 기획팀 ←→ 운영팀: 회의실에서 소통\n",
    "```\n",
    "**결과**: 각기 다른 소통 방식으로 인한 혼란과 비효율성 😵\n",
    "\n",
    "#### ✅ **MCP 적용 후**\n",
    "```\n",
    "🌐 통합 업무 플랫폼 (MCP)\n",
    "├── 📊 모든 부서가 동일한 인터페이스 사용\n",
    "├── ⚡ 실시간 정보 공유 및 업데이트\n",
    "└── 🔄 새로운 부서 추가도 쉽게 연결\n",
    "```\n",
    "**결과**: 효율적이고 확장 가능한 협업 시스템 🚀\n",
    "\n",
    "### 🔧 MCP의 4가지 핵심 특징\n",
    "\n",
    "#### 1️⃣ **표준화된 도구 인터페이스** 🛠️\n",
    "- **의미**: 모든 외부 도구가 동일한 방식으로 연결됨\n",
    "- **장점**: 새로운 도구 추가 시 별도 학습 불필요\n",
    "- **예시**: 날씨 API든 데이터베이스든 동일한 방식으로 호출\n",
    "\n",
    "#### 2️⃣ **다양한 전송 메커니즘** 🌐  \n",
    "- **stdio**: 로컬 프로세스 간 통신 (빠르고 안정적)\n",
    "- **HTTP**: 웹 서비스와 연결 (범용성 높음)\n",
    "- **WebSocket**: 실시간 양방향 통신 (채팅, 실시간 데이터)\n",
    "\n",
    "#### 3️⃣ **동적 도구 검색** 🔍\n",
    "- **의미**: 프로그램 실행 중에 사용 가능한 도구를 자동으로 찾아냄\n",
    "- **장점**: 도구 추가/제거 시 코드 수정 불필요\n",
    "- **비유**: 스마트폰에서 새 앱을 설치하면 자동으로 앱 목록에 나타나는 것과 같음\n",
    "\n",
    "#### 4️⃣ **확장 가능한 아키텍처** 🏗️\n",
    "- **멀티 서버**: 여러 MCP 서버를 동시에 연결 가능\n",
    "- **모듈화**: 각 기능별로 독립적인 서버 구성\n",
    "- **확장성**: 시스템 성장에 따라 필요한 기능만 추가\n",
    "\n",
    "### 💡 왜 MCP가 게임 체인저인가?\n",
    "\n",
    "#### ❌ **기존 방식의 한계**\n",
    "```python\n",
    "# 각 서비스마다 다른 연결 방법 필요\n",
    "weather_api = WeatherAPI(key=\"abc123\")\n",
    "database = PostgreSQL(host=\"localhost\")  \n",
    "file_system = FileManager(\"/home/user\")\n",
    "\n",
    "# 각각 다른 호출 방식\n",
    "weather_data = weather_api.get_current_weather(\"Seoul\")\n",
    "db_result = database.query(\"SELECT * FROM users\") \n",
    "files = file_system.list_files()\n",
    "```\n",
    "\n",
    "#### ✅ **MCP 방식의 혁신**\n",
    "```python\n",
    "# 하나의 클라이언트로 모든 도구 관리\n",
    "mcp_client = MultiServerMCPClient(server_configs)\n",
    "tools = await mcp_client.get_tools()\n",
    "\n",
    "# 통일된 방식으로 모든 도구 사용\n",
    "for tool in tools:\n",
    "    result = await tool.invoke({\"query\": \"원하는 작업\"})\n",
    "```\n",
    "\n",
    "### 🚀 설치 및 준비사항\n",
    "\n",
    "MCP를 사용하기 위해 필요한 주요 패키지들을 이미 설치해두었습니다:\n",
    "\n",
    "```bash\n",
    "# 이미 설치된 패키지들\n",
    "- langchain-mcp-adapters  # MCP 클라이언트\n",
    "- langgraph              # 워크플로우 엔진  \n",
    "- langchain-openai       # OpenAI 모델 연동\n",
    "```\n",
    "\n",
    "이제 실제 코드를 통해 MCP의 강력함을 체험해봅시다! 💪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비동기 처리를 위한 nest_asyncio (Jupyter 환경에서 필수)\n",
    "import nest_asyncio\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# OpenAI LLM 모델 사용을 위한 import\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangGraph의 사전 구축된 React 에이전트 import\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# 메모리 기반 체크포인터 (대화 상태 저장)\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# MCP 클라이언트 - 여러 MCP 서버를 동시에 관리하는 핵심 컴포넌트\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 비동기 호출 활성화\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LangChain-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: MCP 서버 아키텍처 이해하기 🏗️\n",
    "\n",
    "**MCP 서버**는 **특정 기능을 제공하는 독립적인 프로세스**입니다. 마치 **전문 부서**처럼 각각 고유한 역할을 담당합니다.\n",
    "\n",
    "### 🏢 회사 조직도로 이해하기\n",
    "\n",
    "```\n",
    "🏛️ 본사 (Main Application)\n",
    "├── 📊 데이터분석팀 (mcp_server_rag.py) - RAG 검색 전담\n",
    "├── 🌤️ 기상정보팀 (mcp_server_local.py) - 날씨 정보 제공  \n",
    "└── ⏰ 시간관리팀 (mcp_server_remote.py) - 시간 관련 서비스\n",
    "```\n",
    "\n",
    "### 📁 서버 파일 구성\n",
    "\n",
    "우리는 세 가지 타입의 MCP 서버를 준비했습니다:\n",
    "\n",
    "#### 📍 **`server/mcp_server_local.py`** (로컬 서버)\n",
    "- **역할**: 로컬에서 실행되는 기본 기능 제공\n",
    "- **전송방식**: stdio (표준 입출력)  \n",
    "- **예시 기능**: 날씨 조회, 파일 관리 등\n",
    "- **장점**: 빠른 응답속도, 안정적 연결\n",
    "\n",
    "#### 🌐 **`server/mcp_server_remote.py`** (원격 서버)\n",
    "- **역할**: HTTP 엔드포인트로 서비스 제공\n",
    "- **전송방식**: HTTP/WebSocket\n",
    "- **예시 기능**: 현재 시간 조회, API 연동 등  \n",
    "- **장점**: 네트워크를 통한 원격 접속 가능\n",
    "\n",
    "#### 🔍 **`server/mcp_server_rag.py`** (검색 서버)  \n",
    "- **역할**: RAG(Retrieval-Augmented Generation) 검색 전담\n",
    "- **전송방식**: stdio\n",
    "- **예시 기능**: 문서 검색, 벡터 유사도 검색 등\n",
    "- **장점**: 대용량 문서에서 정확한 정보 검색\n",
    "\n",
    "### 💡 왜 서버를 분리할까?\n",
    "\n",
    "✅ **모듈성**: 각 기능을 독립적으로 개발하고 관리  \n",
    "✅ **확장성**: 필요한 기능만 선택적으로 추가  \n",
    "✅ **안정성**: 하나의 서버에 문제가 생겨도 다른 서버는 정상 작동  \n",
    "✅ **성능**: 각 서버를 개별적으로 최적화 가능\n",
    "\n",
    "### 🚀 다음 단계\n",
    "\n",
    "이제 이 서버들을 **MultiServerMCPClient**로 연결하여 통합 관리해보겠습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: MultiServerMCPClient - 🎯 통합 관제센터\n",
    "\n",
    "**MultiServerMCPClient**는 **여러 MCP 서버를 하나의 인터페이스로 관리**하는 핵심 컴포넌트입니다!\n",
    "\n",
    "### 🏢 IT 관제센터로 이해하기\n",
    "\n",
    "대기업의 IT 관제센터를 상상해보세요:\n",
    "\n",
    "```\n",
    "🖥️ IT 관제센터 (MultiServerMCPClient)\n",
    "├── 📊 모니터링 화면 - 모든 서버 상태 한눈에 파악\n",
    "├── 🎮 통합 제어판 - 하나의 인터페이스로 모든 서버 제어\n",
    "├── ⚡ 자동 로드밸런싱 - 부하 분산 자동 처리\n",
    "└── 🔧 에러 복구 시스템 - 장애 발생 시 자동 복구\n",
    "```\n",
    "\n",
    "### 🔧 MultiServerMCPClient의 핵심 기능\n",
    "\n",
    "#### 1️⃣ **서버 구성 관리** 📝\n",
    "```python\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",                    # 실행 명령어\n",
    "        \"args\": [\"run\", \"python\", \"서버.py\"], # 인자 전달  \n",
    "        \"transport\": \"stdio\",               # 통신 방식\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2️⃣ **동적 도구 로딩** 🔄\n",
    "- 각 서버에서 제공하는 도구들을 자동으로 수집\n",
    "- 중복 제거 및 충돌 방지\n",
    "- 실시간 도구 목록 업데이트\n",
    "\n",
    "#### 3️⃣ **통합 인터페이스** 🎯\n",
    "- 모든 서버의 도구를 하나의 리스트로 통합  \n",
    "- 동일한 방식으로 모든 도구 호출 가능\n",
    "- 서버별 차이점을 추상화\n",
    "\n",
    "#### 4️⃣ **에러 처리 및 복구** 🛡️\n",
    "- 개별 서버 장애 시 다른 서버는 정상 작동 유지\n",
    "- 재연결 시도 및 자동 복구\n",
    "- 상세한 에러 로깅 및 디버깅 정보 제공\n",
    "\n",
    "### 💡 왜 MultiServerMCPClient가 필수인가?\n",
    "\n",
    "#### ❌ **개별 서버 직접 관리 시**\n",
    "```python\n",
    "# 각 서버마다 별도 연결 및 관리 필요\n",
    "weather_client = WeatherMCPClient()\n",
    "time_client = TimeMCPClient() \n",
    "rag_client = RAGMCPClient()\n",
    "\n",
    "# 서버마다 다른 연결 관리 로직 필요 😵\n",
    "```\n",
    "\n",
    "#### ✅ **MultiServerMCPClient 사용 시**\n",
    "```python\n",
    "# 하나의 클라이언트로 모든 서버 관리\n",
    "client = MultiServerMCPClient(all_server_configs)\n",
    "tools = await client.get_tools()  # 모든 도구 한번에 로드\n",
    "\n",
    "# 통일된 방식으로 모든 도구 사용 😊\n",
    "```\n",
    "\n",
    "### 🚀 실습 준비\n",
    "\n",
    "지금부터 실제 코드로 MultiServerMCPClient의 강력함을 체험해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiServerMCPClient 설정 예제\n",
    "async def setup_mcp_client(server_configs: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    MCP 클라이언트를 설정하고 도구를 가져옵니다.\n",
    "\n",
    "    Args:\n",
    "        server_configs: 서버 설정 정보 리스트\n",
    "\n",
    "    Returns:\n",
    "        tuple: (MCP 클라이언트, 도구 리스트)\n",
    "    \"\"\"\n",
    "\n",
    "    # MultiServerMCPClient 인스턴스 생성 - 여러 MCP 서버를 통합 관리\n",
    "    client = MultiServerMCPClient(server_configs)\n",
    "\n",
    "    # 모든 연결된 서버로부터 사용 가능한 도구들을 수집\n",
    "    tools = await client.get_tools()\n",
    "\n",
    "    # 로드된 도구 정보를 콘솔에 출력 (디버깅 및 확인용)\n",
    "    print(f\"✅ {len(tools)} 개의 MCP 도구가 로드되었습니다:\")\n",
    "    for tool in tools:\n",
    "        print(f\"  - {tool.name}\")  # 각 도구의 이름 출력\n",
    "\n",
    "    return client, tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서버 구성 정의 - 로컬 날씨 서버 설정\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",  # uv 패키지 매니저를 사용하여 실행\n",
    "        \"args\": [\n",
    "            \"run\",\n",
    "            \"python\",\n",
    "            \"server/mcp_server_local.py\",\n",
    "        ],  # 날씨 서버 스크립트 실행\n",
    "        \"transport\": \"stdio\",  # 표준 입출력 방식으로 통신\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP 클라이언트 생성 및 도구 로딩\n",
    "client, tools = await setup_mcp_client(server_configs=server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI GPT 모델을 사용하는 LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)  # 일관된 결과를 위해 temperature=0\n",
    "\n",
    "# LangGraph의 사전 구축된 React Agent 생성\n",
    "# - llm: 사용할 언어 모델\n",
    "# - tools: MCP에서 로드한 도구들\n",
    "# - checkpointer: 대화 상태를 메모리에 저장하여 연속 대화 지원\n",
    "agent = create_react_agent(\n",
    "    llm, tools, checkpointer=InMemorySaver()  # 상태 저장을 위한 체크포인터\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력과 랜덤 UUID 생성을 위한 유틸리티 함수 import\n",
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 대화 스레드를 구분하기 위한 고유 ID 생성\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 에이전트 실행 - 서울 날씨 조회 요청\n",
    "# astream_graph: 그래프 실행 결과를 스트리밍으로 출력 (실시간으로 처리 과정 확인 가능)\n",
    "response = await astream_graph(\n",
    "    agent,  # 실행할 에이전트\n",
    "    inputs={\n",
    "        \"messages\": [(\"human\", \"안녕하세요. 서울의 날씨를 알려주세요.\")]\n",
    "    },  # 사용자 입력 메시지\n",
    "    config=config,  # 대화 설정 (스레드 ID 포함)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌐 HTTP 전송 방식 - 원격 서버 연결하기\n",
    "\n",
    "**HTTP 전송 방식**은 네트워크를 통해 원격 MCP 서버와 연결하는 방법입니다. 마치 **인터넷을 통한 API 호출**과 같은 방식이죠!\n",
    "\n",
    "#### 🏢 온라인 비즈니스로 이해하기\n",
    "\n",
    "```\n",
    "🏠 우리 사무실 (Local Client)\n",
    "     ↕️ 인터넷 연결\n",
    "🏢 파트너 회사 (Remote MCP Server)\n",
    "```\n",
    "\n",
    "#### 📡 HTTP vs stdio 비교\n",
    "\n",
    "| 구분 | **stdio** | **HTTP** |\n",
    "|------|-----------|----------|\n",
    "| 🏠 **위치** | 같은 컴퓨터 내부 | 네트워크로 연결된 원격지 |\n",
    "| ⚡ **속도** | 매우 빠름 | 네트워크 지연 존재 |\n",
    "| 🛡️ **보안** | 내부 통신 | HTTPS로 암호화 가능 |\n",
    "| 📈 **확장성** | 제한적 | 무제한 확장 |\n",
    "\n",
    "### 🚀 원격 서버 실행하기\n",
    "\n",
    "원격 서버를 사용하려면 먼저 서버를 구동해야 합니다:\n",
    "\n",
    "```bash\n",
    "# 터미널에서 다음 명령어 실행\n",
    "uv run python server/mcp_server_remote.py\n",
    "```\n",
    "\n",
    "이 명령어를 실행하면:\n",
    "- ✅ HTTP 서버가 `http://127.0.0.1:8002/mcp` 주소로 실행됩니다\n",
    "- ✅ MCP 프로토콜을 HTTP로 제공하기 시작합니다  \n",
    "- ✅ 네트워크를 통한 원격 접속이 가능해집니다\n",
    "\n",
    "### 💡 언제 HTTP를 사용할까?\n",
    "\n",
    "- **🌐 분산 시스템**: 여러 서버에 기능을 분산 배치\n",
    "- **☁️ 클라우드 서비스**: AWS, GCP 등의 클라우드 연결  \n",
    "- **🔄 마이크로서비스**: 서비스별로 독립적인 서버 운영\n",
    "- **👥 팀 협업**: 팀원들과 공유 서버 사용\n",
    "\n",
    "지금부터 실제 HTTP MCP 서버에 연결해보겠습니다! 🎯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP 기반 MCP 서버 설정 예제\n",
    "http_server_config = {\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",  # 원격 MCP 서버의 HTTP 엔드포인트\n",
    "        \"transport\": \"streamable_http\",  # HTTP 스트리밍 방식으로 통신\n",
    "    },\n",
    "}\n",
    "\n",
    "# HTTP MCP 서버에 연결하여 클라이언트 생성\n",
    "client, http_tools = await setup_mcp_client(server_configs=http_server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP 서버용 LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)  # 일관성을 위해 동일한 모델 사용\n",
    "\n",
    "# HTTP 도구를 사용하는 React Agent 생성\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    http_tools,\n",
    "    checkpointer=InMemorySaver(),  # HTTP로 로드된 도구들과 메모리 저장소 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 스레드로 HTTP 서버와 상호작용\n",
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 새로운 대화 세션을 위한 고유 ID 생성\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# HTTP MCP 서버를 통해 현재 시간 조회 요청\n",
    "response = await astream_graph(\n",
    "    agent,  # HTTP 도구가 연결된 에이전트\n",
    "    inputs={\n",
    "        \"messages\": [(\"human\", \"안녕하세요. 현재 시간을 알려주세요.\")]\n",
    "    },  # 시간 조회 요청\n",
    "    config=config,  # 대화 세션 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 MCP Inspector - 개발자를 위한 디버깅 도구\n",
    "\n",
    "**MCP Inspector**는 MCP 서버를 **웹 브라우저에서 직접 테스트하고 디버깅**할 수 있는 강력한 개발 도구입니다!\n",
    "\n",
    "#### 🛠️ Visual Studio Code의 디버거처럼\n",
    "\n",
    "개발자라면 누구나 코드 디버거를 사용해본 경험이 있을 것입니다. MCP Inspector는 **MCP 서버 전용 디버거**라고 생각하면 됩니다:\n",
    "\n",
    "```\n",
    "🖥️ MCP Inspector (웹 인터페이스)\n",
    "├── 📊 서버 상태 실시간 모니터링\n",
    "├── 🎮 도구 직접 테스트 및 실행\n",
    "├── 📝 요청/응답 로그 상세 확인  \n",
    "└── 🐛 에러 발생 시 디버깅 정보 제공\n",
    "```\n",
    "\n",
    "#### 🚀 MCP Inspector 실행 방법\n",
    "\n",
    "터미널에서 다음 명령어를 실행하세요:\n",
    "\n",
    "```bash\n",
    "npx @modelcontextprotocol/inspector\n",
    "```\n",
    "\n",
    "실행하면:\n",
    "- ✅ 웹 브라우저가 자동으로 열립니다\n",
    "- ✅ MCP 서버들을 시각적으로 탐색할 수 있습니다  \n",
    "- ✅ 각 도구의 기능을 직접 테스트해볼 수 있습니다\n",
    "- ✅ 실시간으로 서버 상태를 확인할 수 있습니다\n",
    "\n",
    "#### 💡 언제 MCP Inspector를 사용할까?\n",
    "\n",
    "- **🔧 서버 개발 중**: 새로 만든 MCP 서버가 올바르게 작동하는지 확인\n",
    "- **🐛 문제 해결**: 도구 호출이 실패할 때 원인 분석\n",
    "- **📊 성능 측정**: 서버 응답 시간 및 리소스 사용량 확인\n",
    "- **📚 API 문서화**: 서버가 제공하는 도구들의 스펙 확인\n",
    "\n",
    "다음 이미지에서 MCP Inspector의 실제 화면을 확인해보세요! 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mcp_inspector](./assets/mcp-inspector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG(검색 증강 생성) 서버 설정\n",
    "http_server_config = {\n",
    "    \"rag\": {\n",
    "        \"command\": \"uv\",  # uv 패키지 매니저 사용\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_rag.py\"],  # RAG 전용 서버 실행\n",
    "        \"transport\": \"stdio\",  # 로컬 서버이므로 stdio 방식 사용\n",
    "    },\n",
    "}\n",
    "\n",
    "# RAG MCP 서버에 연결하여 검색 도구 로딩\n",
    "client, rag_tools = await setup_mcp_client(server_configs=http_server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 서버용 LLM 설정\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1\", temperature=0\n",
    ")  # 정확한 검색 결과를 위해 temperature=0\n",
    "\n",
    "# RAG 도구를 활용하는 React Agent 생성\n",
    "rag_agent = create_react_agent(\n",
    "    llm, rag_tools, checkpointer=InMemorySaver()  # RAG 검색 도구와 메모리 저장소 연결\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 에이전트를 사용한 문서 검색 예제\n",
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# RAG 전용 대화 세션 생성\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 삼성 가우스 AI에 대한 정보를 MCP RAG 서버에서 검색\n",
    "# MCP 서버를 명시적으로 사용하도록 요청하여 정확한 정보 검색 유도\n",
    "_ = await astream_graph(\n",
    "    rag_agent,  # RAG 검색 도구가 연결된 에이전트\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"삼성전자가 개발한 생성형 AI 의 이름은? mcp 서버를 사용해서 검색해주세요.\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,  # RAG 세션 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일한 RAG 세션에서 추가 검색 수행\n",
    "# 이전 대화 맥락을 유지하면서 구글의 Anthropic 투자에 대한 정보 검색\n",
    "_ = await astream_graph(\n",
    "    rag_agent,  # 동일한 RAG 에이전트 사용\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"구글이 Anthropic 에 투자하기로 한 금액을 검색해줘\",  # 후속 질문\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,  # 동일한 대화 세션 유지 (이전 맥락 보존)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: React Agent와 MCP 완벽 통합 🚀\n",
    "\n",
    "**React Agent**는 **추론(Reason)**과 **행동(Act)**을 반복하는 지능형 에이전트 패턴입니다. MCP 도구와 결합하면 **자율적으로 문제를 해결하는 강력한 AI 시스템**을 구축할 수 있습니다!\n",
    "\n",
    "### 🧠 인간의 문제 해결 과정으로 이해하기\n",
    "\n",
    "일상에서 우리가 복잡한 문제를 해결하는 과정을 떠올려보세요:\n",
    "\n",
    "```\n",
    "🤔 문제 상황: \"내일 부산 출장, 날씨가 어떨까?\"\n",
    "\n",
    "1️⃣ 🧠 추론(Reason): \"날씨 정보가 필요하다\"\n",
    "2️⃣ 🎯 행동(Act): 날씨 앱을 열어 부산 날씨 확인\n",
    "3️⃣ 🧠 추론(Reason): \"비가 올 것 같다. 우산이 필요하겠다\"\n",
    "4️⃣ 🎯 행동(Act): 우산을 가방에 챙김\n",
    "5️⃣ 🧠 추론(Reason): \"준비 완료!\"\n",
    "```\n",
    "\n",
    "### ⚙️ React Agent의 작동 원리\n",
    "\n",
    "#### 🔄 **ReAct 패턴의 핵심**\n",
    "\n",
    "```\n",
    "📝 사용자 질문 → 🤖 Agent 시작\n",
    "     ↓\n",
    "🧠 [Reason] \"무엇이 필요한가?\" → 상황 분석 및 계획 수립\n",
    "     ↓  \n",
    "🎯 [Act] 필요한 도구 실행 → MCP 도구 호출\n",
    "     ↓\n",
    "📊 결과 분석 → 목표 달성했나?\n",
    "     ↓\n",
    "🔄 아직 미완료라면 다시 Reason & Act\n",
    "     ↓\n",
    "✅ 목표 달성 시 최종 답변 제공\n",
    "```\n",
    "\n",
    "#### 🎯 **MCP + React Agent의 시너지**\n",
    "\n",
    "| 구성요소 | 역할 | 예시 |\n",
    "|----------|------|------|\n",
    "| 🤖 **React Agent** | 논리적 추론 및 계획 수립 | \"날씨와 시간 정보가 모두 필요하다\" |\n",
    "| 🔧 **MCP Tools** | 구체적인 작업 실행 | `get_weather()`, `get_time()` |\n",
    "| 🧠 **LLM** | 자연어 이해 및 생성 | 사용자 질문 해석, 답변 생성 |\n",
    "| 💾 **Memory** | 대화 맥락 유지 | 이전 정보를 기억하며 연속 대화 |\n",
    "\n",
    "### 💡 왜 React Agent를 사용해야 할까?\n",
    "\n",
    "✅ **자율성**: 복잡한 작업을 단계적으로 분해하여 자동 처리  \n",
    "✅ **유연성**: 상황에 따라 다른 도구를 적절히 선택  \n",
    "✅ **투명성**: 각 단계의 추론 과정을 명확히 볼 수 있음  \n",
    "✅ **확장성**: 새로운 도구 추가 시 자동으로 활용  \n",
    "\n",
    "### 🚀 실습 예제\n",
    "\n",
    "지금부터 날씨와 시간 정보를 모두 활용하는 **멀티 도구 React Agent**를 만들어보겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# React Agent와 MCP를 통합하는 함수 정의\n",
    "async def create_mcp_react_agent(server_configs: dict):\n",
    "    \"\"\"\n",
    "    MCP 도구를 사용하는 React Agent를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        server_configs: MCP 서버 설정 딕셔너리\n",
    "\n",
    "    Returns:\n",
    "        LangGraph 에이전트: MCP 도구가 연결된 React Agent\n",
    "    \"\"\"\n",
    "\n",
    "    # MCP 클라이언트 생성 및 모든 서버의 도구를 통합 로딩\n",
    "    client, tools = await setup_mcp_client(server_configs=server_configs)\n",
    "\n",
    "    # 통일된 GPT 모델 사용 (일관성 유지)\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "\n",
    "    # React 패턴을 구현하는 에이전트 생성\n",
    "    # - 추론(Reason): LLM이 상황을 분석하고 다음 행동 계획\n",
    "    # - 행동(Act): MCP 도구를 호출하여 구체적 작업 실행\n",
    "    agent = create_react_agent(\n",
    "        llm, tools, checkpointer=InMemorySaver()  # 상태 저장을 위한 체크포인터\n",
    "    )\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 MCP 서버 구성 - 날씨와 시간 서비스 통합\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",  # 로컬 날씨 서버\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",  # 빠른 로컬 통신\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",  # 원격 시간 서버\n",
    "        \"transport\": \"streamable_http\",  # HTTP 스트리밍 통신\n",
    "    },\n",
    "}\n",
    "\n",
    "# 다중 서버를 통합한 MCP React Agent 생성\n",
    "agent = await create_mcp_react_agent(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# React Agent 통합 테스트 - 다중 서버 활용\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, astream_graph\n",
    "\n",
    "# 멀티 도구 테스트를 위한 대화 세션 생성\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 1차 테스트: 시간 정보 요청 (HTTP 서버 사용)\n",
    "await astream_graph(\n",
    "    agent, inputs={\"messages\": [(\"human\", \"현재 시간을 알려주세요\")]}, config=config\n",
    ")\n",
    "\n",
    "# 2차 테스트: 날씨 정보 요청 (로컬 서버 사용)\n",
    "# 동일한 세션을 사용하여 대화 맥락 유지\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [(\"human\", \"현재 서울의 날씨도 알려주세요\")]},\n",
    "    config=config,  # 이전 대화와 연결된 동일한 세션 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ToolNode로 고급 워크플로우 구축 ⚙️\n",
    "\n",
    "**ToolNode**는 **더 세밀한 제어가 가능한 커스텀 워크플로우**를 만들 때 사용하는 고급 컴포넌트입니다. React Agent가 **자동차**라면, ToolNode는 **레이싱카**라고 할 수 있습니다!\n",
    "\n",
    "### 🏎️ 자동차 vs 레이싱카 비교\n",
    "\n",
    "#### 🚗 **React Agent (자동차)**\n",
    "```\n",
    "🚗 편리한 자동 운전\n",
    "├── ✅ 사용하기 쉬움 - 목적지만 말하면 알아서 이동\n",
    "├── ✅ 안전함 - 검증된 경로로만 이동\n",
    "└── ❌ 세부 제어 불가 - 운전자가 직접 조작 어려움\n",
    "```\n",
    "\n",
    "#### 🏎️ **ToolNode (레이싱카)**\n",
    "```\n",
    "🏎️ 전문가용 수동 운전  \n",
    "├── ⚡ 최고 성능 - 원하는 정확한 제어 가능\n",
    "├── 🎯 맞춤형 설계 - 복잡한 로직 구현 가능\n",
    "└── 🛠️ 개발 노력 필요 - 더 많은 설정과 코드 작성 필요\n",
    "```\n",
    "\n",
    "### 🏗️ ToolNode의 핵심 장점\n",
    "\n",
    "#### 1️⃣ **워크플로우 커스터마이징** 🎨\n",
    "- 복잡한 조건부 로직 구현 가능\n",
    "- 도구 실행 순서를 세밀하게 제어\n",
    "- 에러 처리 및 복구 전략 자유롭게 설계\n",
    "\n",
    "#### 2️⃣ **성능 최적화** ⚡\n",
    "- 불필요한 LLM 호출 최소화  \n",
    "- 병렬 처리로 실행 속도 향상\n",
    "- 메모리 사용량 효율적 관리\n",
    "\n",
    "#### 3️⃣ **고급 패턴 구현** 🎯\n",
    "- 조건부 분기 처리\n",
    "- 루프 및 반복 작업\n",
    "- 복합적인 데이터 변환 파이프라인\n",
    "\n",
    "### 💡 언제 ToolNode를 사용해야 할까?\n",
    "\n",
    "#### ✅ **ToolNode를 사용하는 경우**\n",
    "- **복잡한 비즈니스 로직**: 다단계 승인 프로세스\n",
    "- **성능이 중요한 시스템**: 실시간 처리 요구사항  \n",
    "- **정교한 에러 처리**: 장애 복구 및 재시도 로직\n",
    "- **특수한 워크플로우**: 기존 패턴으로 해결 불가한 요구사항\n",
    "\n",
    "#### ❌ **React Agent를 사용하는 경우**  \n",
    "- **간단한 태스크**: 단순 질문-답변\n",
    "- **빠른 프로토타이핑**: 개념 증명 단계\n",
    "- **일반적인 용도**: 표준적인 AI 어시스턴트 기능\n",
    "\n",
    "### 🎯 실습 목표\n",
    "\n",
    "지금부터 **MCP + ToolNode + Tavily 검색**을 결합한 **하이브리드 검색 시스템**을 구축해보겠습니다:\n",
    "\n",
    "1. **🔍 웹 검색**: Tavily로 최신 정보 수집  \n",
    "2. **📊 문서 검색**: MCP RAG로 내부 문서 검색\n",
    "3. **⏰ 시간 정보**: MCP 시간 서버로 컨텍스트 추가\n",
    "4. **🧠 통합 분석**: 모든 정보를 종합하여 최종 답변 생성\n",
    "\n",
    "### 🚀 구현 시작\n",
    "\n",
    "정교하게 설계된 워크플로우의 힘을 경험해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToolNode 기반 고급 워크플로우 구현\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import Annotated, List, Dict, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# 에이전트의 상태를 정의하는 스키마\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    에이전트 상태 - 워크플로우 전반에 걸쳐 유지되는 데이터\n",
    "    \"\"\"\n",
    "\n",
    "    messages: Annotated[List[BaseMessage], add_messages]  # 대화 메시지 (자동 누적)\n",
    "    context: Dict[str, Any] = None  # 추가 컨텍스트 정보 (검색 결과, 메타데이터 등)\n",
    "\n",
    "\n",
    "async def create_mcp_workflow(server_configs: dict):\n",
    "    \"\"\"\n",
    "    MCP 도구를 활용하는 고급 커스텀 워크플로우 생성\n",
    "\n",
    "    Args:\n",
    "        server_configs: MCP 서버 설정 딕셔너리\n",
    "\n",
    "    Returns:\n",
    "        컴파일된 LangGraph 애플리케이션\n",
    "    \"\"\"\n",
    "\n",
    "    # MCP 클라이언트에서 도구들을 로드\n",
    "    client, tools = await setup_mcp_client(server_configs=server_configs)\n",
    "\n",
    "    # Tavily 웹 검색 도구를 MCP 도구 목록에 추가 (하이브리드 검색 시스템 구성)\n",
    "    tool = TavilySearch(max_results=2)  # 최대 2개 검색 결과로 제한\n",
    "    tools.append(tool)\n",
    "\n",
    "    # 통일된 GPT 모델과 도구 바인딩\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "    llm_with_tools = llm.bind_tools(tools)  # LLM에 모든 도구를 연결\n",
    "\n",
    "    # StateGraph로 커스텀 워크플로우 정의\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # 에이전트 노드: LLM이 상황을 분석하고 도구 호출을 결정\n",
    "    async def agent_node(state: AgentState):\n",
    "        \"\"\"\n",
    "        LLM을 호출하여 응답 생성 및 도구 사용 결정\n",
    "\n",
    "        Args:\n",
    "            state: 현재 에이전트 상태\n",
    "\n",
    "        Returns:\n",
    "            업데이트된 상태 (새 메시지 추가)\n",
    "        \"\"\"\n",
    "        response = await llm_with_tools.ainvoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # ToolNode 생성: 실제 도구 실행을 담당\n",
    "    tool_node = ToolNode(tools)\n",
    "\n",
    "    # 워크플로우 그래프 구성\n",
    "    workflow.add_node(\"agent\", agent_node)  # 추론 및 계획 수립 노드\n",
    "    workflow.add_node(\"tools\", tool_node)  # 도구 실행 노드\n",
    "\n",
    "    # 엣지 연결 (워크플로우 흐름 정의)\n",
    "    workflow.add_edge(START, \"agent\")  # 시작 -> 에이전트\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\", tools_condition\n",
    "    )  # 조건부: 도구 호출 필요시 tools로, 아니면 END로\n",
    "    workflow.add_edge(\"tools\", \"agent\")  # 도구 실행 후 -> 에이전트로 돌아가서 결과 분석\n",
    "    workflow.add_edge(\"agent\", END)  # 최종 완료시 -> 종료\n",
    "\n",
    "    # 워크플로우 컴파일 (메모리 저장소 포함)\n",
    "    app = workflow.compile(checkpointer=InMemorySaver())\n",
    "\n",
    "    # 워크플로우 구조를 시각적으로 확인\n",
    "    visualize_graph(app)\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToolNode 워크플로우를 위한 다중 MCP 서버 설정\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",  # 로컬 날씨 서버\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",  # 빠른 로컬 통신\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",  # 원격 시간 서버\n",
    "        \"transport\": \"streamable_http\",  # HTTP 스트리밍 통신\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고급 MCP 워크플로우 애플리케이션 생성\n",
    "# - MCP 도구들과 Tavily 웹 검색을 통합한 하이브리드 시스템\n",
    "mcp_app = await create_mcp_workflow(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToolNode 워크플로우 테스트 - 시간 정보 요청\n",
    "# 커스텀 워크플로우가 MCP 시간 서버를 올바르게 활용하는지 확인\n",
    "_ = await astream_graph(\n",
    "    mcp_app, inputs={\"messages\": [(\"human\", \"현재 시간을 알려주세요\")]}, config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복합 워크플로우 테스트 - 시간 + 웹 검색 통합 작업\n",
    "# MCP 시간 서버와 Tavily 웹 검색을 연계한 고급 작업 수행\n",
    "_ = await astream_graph(\n",
    "    mcp_app,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\"human\", \"오늘 뉴스를 검색해주세요. 검색시 시간을 조회한 뒤 처리하세요.\")\n",
    "        ]\n",
    "    },\n",
    "    config=config,  # 동일한 세션으로 연속 대화\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 외부 MCP 서버 생태계 활용하기 🌐\n",
    "\n",
    "**3rd Party MCP 서버**를 활용하면 이미 검증된 외부 서비스들을 손쉽게 연결할 수 있습니다. 마치 **앱 스토어에서 필요한 앱을 다운로드**하는 것과 같습니다!\n",
    "\n",
    "### 🏪 Smithery AI - MCP 생태계의 허브\n",
    "\n",
    "**Smithery AI**는 **MCP 서버들의 중앙 집중형 허브**로, 다양한 검증된 MCP 서비스들을 제공합니다.\n",
    "\n",
    "#### 🌐 **Smithery AI 웹사이트**\n",
    "- **URL**: https://smithery.ai/\n",
    "- **역할**: MCP 서버의 **GitHub 마켓플레이스**\n",
    "- **특징**: 커뮤니티에서 개발한 다양한 MCP 서버들을 한곳에서 관리\n",
    "\n",
    "### 🏢 기업 IT 인프라로 이해하기\n",
    "\n",
    "```\n",
    "🏛️ 우리 회사 (Main Application)\n",
    "     ↕️ MCP 프로토콜\n",
    "🏪 Smithery AI (외부 서비스 허브)\n",
    "├── 🔍 검색 서비스 (Context7)\n",
    "├── 📊 데이터 분석 서비스  \n",
    "├── 💬 소셜 미디어 연동\n",
    "├── 📧 이메일 자동화\n",
    "└── 🛠️ 개발 도구 통합\n",
    "```\n",
    "\n",
    "### ⭐ Context7 MCP 서버 - 지능형 문서 검색\n",
    "\n",
    "**Context7**은 Smithery AI에서 제공하는 **고성능 RAG 검색 서비스**입니다:\n",
    "\n",
    "#### 🔍 **핵심 기능**\n",
    "- **📚 대규모 문서 검색**: LangGraph, LangChain 등 최신 기술 문서\n",
    "- **⚡ 실시간 업데이트**: 최신 라이브러리 문서를 실시간 반영  \n",
    "- **🎯 정확한 컨텍스트**: 검색 결과의 정확성과 관련성 최적화\n",
    "- **🌐 다국어 지원**: 영어, 한국어 등 다양한 언어 문서 지원\n",
    "\n",
    "#### 💡 **사용 시나리오**\n",
    "- **📖 최신 문서 검색**: \"LangGraph의 새로운 기능은?\"\n",
    "- **🔧 코드 예제 찾기**: \"React Agent 구현 예제 찾아줘\"\n",
    "- **🐛 문제 해결**: \"특정 에러 메시지 해결 방법은?\"\n",
    "\n",
    "### 🚀 외부 서버 연동의 장점\n",
    "\n",
    "#### ✅ **개발 시간 단축**\n",
    "- 검증된 서비스를 바로 활용\n",
    "- 복잡한 인프라 구축 불필요\n",
    "\n",
    "#### ✅ **전문성 활용**  \n",
    "- 각 분야 전문가가 개발한 최적화된 서비스\n",
    "- 지속적인 업데이트와 개선\n",
    "\n",
    "#### ✅ **확장성**\n",
    "- 필요에 따라 새로운 서비스 추가\n",
    "- 스케일링 부담 없음\n",
    "\n",
    "### 🎯 실습 목표\n",
    "\n",
    "지금부터 **Context7 MCP 서버**를 연결하여:\n",
    "1. **📚 LangGraph 최신 문서 검색**\n",
    "2. **🤖 검색 결과 기반 React Agent 자동 생성**  \n",
    "3. **⚡ 내부 MCP + 외부 MCP 통합 활용**\n",
    "\n",
    "외부 생태계의 힘을 경험해봅시다! 🌟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부 MCP 서버와 내부 서버 통합 구성\n",
    "server_configs = {\n",
    "    # 내부 서버들\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",  # 로컬 날씨 서버\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",  # 원격 시간 서버\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "    # 외부 3rd Party 서버 (Smithery AI의 Context7)\n",
    "    \"context7-mcp\": {\n",
    "        \"command\": \"npx\",  # Node.js 패키지 매니저를 통해 실행\n",
    "        \"args\": [\n",
    "            \"-y\",  # 자동 설치 승인\n",
    "            \"@smithery/cli@latest\",  # Smithery AI CLI 도구\n",
    "            \"run\",\n",
    "            \"@upstash/context7-mcp\",  # Context7 MCP 서버\n",
    "            \"--key\",\n",
    "            \"7c5b4b8f-cb2a-4c2b-b0e0-d3cc49fc7a85\",  # Context7 서비스 API 키\n",
    "        ],\n",
    "        \"transport\": \"stdio\",  # 로컬 프로세스로 실행\n",
    "    },\n",
    "}\n",
    "\n",
    "# 내부 + 외부 MCP 서버를 통합한 하이브리드 워크플로우 생성\n",
    "mcp_app = await create_mcp_workflow(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이브리드 MCP 시스템 종합 테스트\n",
    "# 내부 MCP + 외부 Context7 + Tavily 웹 검색을 모두 활용한 복합 작업\n",
    "await astream_graph(\n",
    "    mcp_app,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"최신 LangGraph 도큐먼트에서 ReAct Agent 관련 내용을 검색하세요. \"\n",
    "                \"그런 다음 Tavily 검색을 수행하는 ReAct Agent를 생성하세요. \"\n",
    "                \"사용 LLM=gpt-4.1\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,  # 연속 대화 세션 유지\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
