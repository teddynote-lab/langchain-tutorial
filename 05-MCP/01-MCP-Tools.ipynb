{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP(Model Context Protocol)\n",
    "\n",
    "### MCP(Model Context Protocol) 정의\n",
    "\n",
    "**MCP**는 **AI 애플리케이션과 외부 도구 간의 상호작용을 표준화**한 오픈 프로토콜입니다.  \n",
    "즉, LLM이 다양한 외부 시스템(API, 데이터베이스, 도구 등)과 일관된 방식으로 통신할 수 있도록 하는 **표준 인터페이스 계층**입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### MCP의 4가지 핵심 특징\n",
    "\n",
    "| 특징 | 설명 | 장점 | 예시 |\n",
    "|------|------|------|------|\n",
    "| **표준화된 도구 인터페이스** | 모든 외부 도구가 동일한 프로토콜로 연결 | 새로운 도구 추가 시 별도 학습 불필요 | 날씨 API, DB, 파일 시스템 등 동일 방식 호출 |\n",
    "| **다양한 전송 메커니즘** | **공식: `stdio`, `Streamable HTTP` (구 `HTTP+SSE`는 deprecated)** | 환경(로컬/원격)에 맞는 최적 방식 선택 | 로컬은 `stdio`, 원격은 `Streamable HTTP` |\n",
    "| **동적 도구 검색** | 런타임 시 사용 가능한 도구 자동 탐색 | 코드 수정 없이 확장 가능 | 새로운 플러그인 자동 인식 |\n",
    "| **확장 가능한 아키텍처** | 모듈형 구조 및 멀티 서버 지원 | 시스템 규모 확장에 유연 | 필요 기능만 선택적 추가 |\n",
    "\n",
    "---\n",
    "\n",
    "### MCP 전송 메커니즘 비교\n",
    "\n",
    "| 구분 | **stdio** | **HTTP+SSE (레거시)** | **Streamable HTTP (현행)** |\n",
    "|------|------------|------------------------|-----------------------------|\n",
    "| **전송 계층** | 표준입출력 파이프(로컬 IPC) | GET(SSE 수신) + POST(송신) 2 엔드포인트 | 단일 HTTP 엔드포인트(POST/GET), **SSE로 응답 스트리밍** |\n",
    "| **통신 구조** | 한 프로세스 내 파이프 기반 | S→C는 SSE, C→S는 POST로 분리 | C→S: POST / S→C: **SSE 스트림** → 실질적 양방향 통신 |\n",
    "| **실시간성 / 재개 지원** | 빠름(로컬 IPC 수준), 재개 불가 | 단방향 스트림만 가능, 재개 미지원 | **Event ID / `Last-Event-ID` 기반 스트림 재개 가능** |\n",
    "| **적합 환경** | 로컬 도구·IDE 플러그인 | 과거 원격 MCP 서버 호환 | **최신 원격 MCP 서버 / 실시간 스트리밍 서비스** |\n",
    "| **표준 지위** | 지속 지원 | **Deprecated (2025-03-26)** | **Current Standard (Streamable HTTP)** |\n",
    "\n",
    "> ⚙️ **보안 주의사항:**  \n",
    "> - Streamable HTTP 사용 시 **Origin 검증 및 로컬(127.0.0.1) 바인딩** 권장  \n",
    "> - 외부 네트워크 통신 시 **토큰 기반 인증(OAuth 등)** 필수  \n",
    "> - HTTP+SSE는 호환성 목적 외에는 더 이상 권장되지 않음\n",
    "\n",
    "---\n",
    "\n",
    "### 요약\n",
    "\n",
    "- **stdio** → 로컬 환경용, 빠르고 안정적  \n",
    "- **Streamable HTTP** → 현대적 원격 통신 표준, SSE 기반 실시간 양방향 스트리밍 지원  \n",
    "- **HTTP+SSE** → 과거 임시 방식으로 사용되었으나 **2025년 3월 이후 폐기됨**\n",
    "\n",
    "---\n",
    "\n",
    "### 핵심 포인트\n",
    "\n",
    "> 🔎 **MCP는 \"AI와 도구를 연결하는 표준 인터페이스\"**  \n",
    "> - 도구 검색, 요청·응답 포맷, 스트리밍, 인증·보안 정책을 통합적으로 관리  \n",
    "> - 로컬(예: IDE 확장)에는 `stdio`,  \n",
    ">   원격(예: 클라우드 도구 서버)에는 `Streamable HTTP`를 사용하는 것이 권장됨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 비동기 호출 활성화\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LangChain-Tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows MCP stdio stderr 버그 워크어라운드\n",
    "# 이슈: https://github.com/modelcontextprotocol/python-sdk/issues/1103\n",
    "# Windows에서 sys.stderr를 사용할 때 발생하는 문제를 해결하기 위한 패치\n",
    "\n",
    "import sys\n",
    "\n",
    "print(\"=== Windows MCP stdio 패치 적용 ===\\n\")\n",
    "\n",
    "try:\n",
    "    # MCP SDK의 stdio_client 함수를 패치\n",
    "    import mcp.client.stdio as stdio_module\n",
    "    from mcp.client.stdio import StdioServerParameters\n",
    "    from contextlib import asynccontextmanager\n",
    "\n",
    "    # 원본 stdio_client 함수 백업\n",
    "    _original_stdio_client = stdio_module.stdio_client\n",
    "\n",
    "    # errlog=None을 강제하는 래퍼 함수 생성\n",
    "    @asynccontextmanager\n",
    "    async def patched_stdio_client(server, **kwargs):\n",
    "        \"\"\"Windows에서 stderr 문제를 우회하기 위해 errlog=None을 강제\"\"\"\n",
    "        # errlog 인자를 제거하고 None으로 설정\n",
    "        kwargs[\"errlog\"] = None\n",
    "        async with _original_stdio_client(server, **kwargs) as streams:\n",
    "            yield streams\n",
    "\n",
    "    # 원본 함수를 패치된 버전으로 교체\n",
    "    stdio_module.stdio_client = patched_stdio_client\n",
    "\n",
    "    print(\"✅ Windows MCP stdio 패치가 성공적으로 적용되었습니다.\")\n",
    "    print(\"   - mcp.client.stdio.stdio_client 함수가 패치되었습니다.\")\n",
    "    print(\"   - errlog=None이 자동으로 적용됩니다.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ MCP 패치 적용 실패: {e}\")\n",
    "    print(f\"   타입: {type(e).__name__}\")\n",
    "    print(\"   기본 설정으로 계속 진행합니다.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP 다양한 서버 제작 실습\n",
    "\n",
    "### 서버 파일 구성 및 특징\n",
    "\n",
    "이 튜토리얼에서는 세 가지 타입의 **MCP 서버**를 사용합니다.  \n",
    "각 서버는 전송 방식과 역할이 다르며, `MultiServerMCPClient`로 통합 관리할 수 있습니다.\n",
    "\n",
    "| 서버 유형 | 파일명 | 전송 방식 | 주요 기능 | 사용 시나리오 |\n",
    "|-----------|---------|------------|------------|----------------|\n",
    "| **로컬 서버** | `mcp_server_local.py` | stdio | 파일 관리, 로컬 데이터 조회 | 빠른 응답이 필요한 기본 기능 |\n",
    "| **원격 서버** | `mcp_server_remote.py` | Streamable HTTP | 시간 조회, 외부 API 연동 | 네트워크를 통한 원격 서비스 |\n",
    "| **검색 서버** | `mcp_server_rag.py` | stdio | RAG 문서 검색, 벡터 유사도 검색 | 대용량 문서에서 정확한 정보 검색 |\n",
    "\n",
    "> 💡 **참고:** MCP 공식 스펙 기준으로 `WebSocket`은 지원 예시로 언급될 수 있지만,  \n",
    "> 실제 표준 전송 방식은 `stdio`와 `Streamable HTTP` 두 가지입니다.  \n",
    "> (`HTTP+SSE`는 2025년 3월부로 폐기됨)\n",
    "\n",
    "---\n",
    "\n",
    "### 서버별 상세 특징\n",
    "\n",
    "#### 🖥️ 로컬 서버 (`stdio`)\n",
    "- **장점**  \n",
    "  - 빠른 응답속도  \n",
    "  - 안정적 연결 (네트워크 불필요)  \n",
    "  - OS 파일 접근이 용이  \n",
    "- **단점**  \n",
    "  - 동일 머신 내에서만 실행 가능  \n",
    "- **적용 분야**  \n",
    "  - 로컬 파일 시스템 제어, 오프라인 데이터 처리, IDE 통합 도구\n",
    "\n",
    "---\n",
    "\n",
    "#### ☁️ 원격 서버 (`Streamable HTTP`)\n",
    "- **장점**  \n",
    "  - 네트워크를 통한 원격 호출 가능  \n",
    "  - 확장성과 보안 설정 용이 (토큰 인증, Origin 검증 등)  \n",
    "  - SSE 기반 **스트리밍 응답 및 재개 지원**  \n",
    "- **단점**  \n",
    "  - 네트워크 지연 및 연결 관리 필요  \n",
    "- **적용 분야**  \n",
    "  - 외부 API 연동, 클라우드 서비스 연결, 실시간 스트리밍 응답 제공\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔍 검색 서버 (`stdio + RAG`)\n",
    "- **장점**  \n",
    "  - 대용량 문서 처리 및 의미 기반 검색 지원  \n",
    "  - 벡터 유사도 검색으로 정교한 컨텍스트 제공  \n",
    "- **단점**  \n",
    "  - 초기 임베딩 구축 및 리소스 사용량 높음  \n",
    "- **적용 분야**  \n",
    "  - 사내 지식베이스 검색, 문서 QA 시스템, LLM 보조 검색 엔진\n",
    "\n",
    "---\n",
    "\n",
    "### 서버 분리의 장점\n",
    "\n",
    "| 장점 | 설명 | 실제 적용 예시 |\n",
    "|------|------|----------------|\n",
    "| **모듈성** | 각 기능을 독립적으로 개발·배포 가능 | 날씨 서버 오류 시에도 시간 서버는 정상 작동 |\n",
    "| **확장성** | 필요한 기능만 선택적으로 추가 가능 | 번역 서버 추가 시 기존 서버 영향 없음 |\n",
    "| **안정성** | 장애가 다른 서버로 전이되지 않음 | 검색 서버 장애 시에도 나머지 서비스 유지 |\n",
    "| **성능 최적화** | 서버별 리소스 설정 조정 가능 | 검색 서버는 GPU, 시간 서버는 CPU 경량 운영 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiServerMCPClient\n",
    "\n",
    "**`MultiServerMCPClient`** 는 **여러 MCP 서버를 하나의 통합 인터페이스로 관리**하는 핵심 컴포넌트입니다.  \n",
    "각 서버(`stdio`, `Streamable HTTP`)에서 제공하는 도구들을 자동으로 수집하고,  \n",
    "단일 클라이언트에서 일관된 방식으로 호출할 수 있도록 설계되어 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "### MultiServerMCPClient의 핵심 기능\n",
    "\n",
    "| 기능 | 설명 | 코드 예시 | 장점 |\n",
    "|------|------|-----------|------|\n",
    "| **서버 구성 관리** | 각 MCP 서버 설정을 딕셔너리로 정의·관리 | `server_configs = {\"weather\": {...}}` | 설정 변경 및 버전 관리 용이 |\n",
    "| **동적 도구 로딩** | 각 서버가 제공하는 도구를 자동 탐색·통합 | `tools = await client.get_tools()` | 코드 수정 없이 실시간 도구 업데이트 |\n",
    "| **통합 인터페이스** | 모든 서버의 도구를 하나의 통합 리스트로 제공 | `for tool in tools: result = await tool.invoke()` | 서버별 호출 방식 차이 제거 |\n",
    "| **에러 처리 및 복구** | 특정 서버 장애 시 다른 서버는 정상 작동 | 내부 재연결 및 로깅 처리 | 시스템 안정성 향상 |\n",
    "\n",
    "---\n",
    "\n",
    "### 서버 구성 설정 상세\n",
    "\n",
    "#### 기본 구성 형태\n",
    "```python\n",
    "server_configs = {\n",
    "    \"서버명\": {\n",
    "        \"command\": \"실행명령어\",      # uv, python, node 등\n",
    "        \"args\": [\"인자1\", \"인자2\"],   # 실행 시 전달할 인자\n",
    "        \"transport\": \"전송방식\",      # stdio 또는 streamable_http\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "📘 stdio 전송 방식 예시\n",
    "```python\n",
    "\"local_server\": {\n",
    "    \"command\": \"uv\",\n",
    "    \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "    \"transport\": \"stdio\",\n",
    "}\n",
    "```\n",
    "\n",
    "🌐 Streamable HTTP 전송 방식 예시\n",
    "```python\n",
    "\"remote_server\": {\n",
    "    \"url\": \"http://127.0.0.1:8002/mcp\",\n",
    "    \"transport\": \"streamable_http\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "# MultiServerMCPClient 설정 예제\n",
    "async def setup_mcp_client(server_configs: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    MCP 클라이언트를 설정하고 도구를 가져옵니다.\n",
    "\n",
    "    Args:\n",
    "        server_configs: 서버 설정 정보 리스트\n",
    "\n",
    "    Returns:\n",
    "        tuple: (MCP 클라이언트, 도구 리스트)\n",
    "    \"\"\"\n",
    "\n",
    "    # MultiServerMCPClient 인스턴스 생성 - 여러 MCP 서버를 통합 관리\n",
    "    client = MultiServerMCPClient(server_configs)\n",
    "\n",
    "    # 모든 연결된 서버로부터 사용 가능한 도구들을 수집\n",
    "    tools = await client.get_tools()\n",
    "\n",
    "    # 로드된 도구 정보를 콘솔에 출력 (디버깅 및 확인용)\n",
    "    print(f\"✅ {len(tools)} 개의 MCP 도구가 로드되었습니다:\")\n",
    "    for tool in tools:\n",
    "        print(f\"  - {tool.name}\")  # 각 도구의 이름 출력\n",
    "\n",
    "    return client, tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로컬 MCP 서버 방식 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP 클라이언트 - 여러 MCP 서버를 동시에 관리하는 핵심 컴포넌트\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# 서버 구성 정의 - 로컬 날씨 서버 설정\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",  # uv 패키지 매니저를 사용하여 실행\n",
    "        \"args\": [\n",
    "            \"run\",\n",
    "            \"python\",\n",
    "            \"server/mcp_server_local.py\",\n",
    "        ],  # 날씨 서버 스크립트 실행\n",
    "        \"transport\": \"stdio\",  # 표준 입출력 방식으로 통신\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP 클라이언트 생성 및 도구 로딩\n",
    "client, tools = await setup_mcp_client(server_configs=server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenRouter를 통한 GPT 모델 사용 설정\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-4.1\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    ")  # 일관된 결과를 위해 temperature=0\n",
    "\n",
    "# LangGraph의 사전 구축된 React Agent 생성\n",
    "# - llm: 사용할 언어 모델\n",
    "# - tools: MCP에서 로드한 도구들\n",
    "# - checkpointer: 대화 상태를 메모리에 저장하여 연속 대화 지원\n",
    "agent = create_react_agent(\n",
    "    llm, tools, checkpointer=InMemorySaver()  # 상태 저장을 위한 체크포인터\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력과 랜덤 UUID 생성을 위한 유틸리티 함수 import\n",
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 대화 스레드를 구분하기 위한 고유 ID 생성\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 에이전트 실행 - 서울 날씨 조회 요청\n",
    "# astream_graph: 그래프 실행 결과를 스트리밍으로 출력 (실시간으로 처리 과정 확인 가능)\n",
    "response = await astream_graph(\n",
    "    agent,  # 실행할 에이전트\n",
    "    inputs={\n",
    "        \"messages\": [(\"human\", \"안녕하세요. 서울의 날씨를 알려주세요.\")]\n",
    "    },  # 사용자 입력 메시지\n",
    "    config=config,  # 대화 설정 (스레드 ID 포함)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamable HTTP 전송 방식 사용\n",
    "\n",
    "원격 서버를 사용하는 경우 먼저 Remote MCP 서버를 구동해야 합니다.\n",
    "\n",
    "```bash\n",
    "uv run python server/mcp_server_remote.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# MCP 서버를 백그라운드에서 실행합니다.\n",
    "# 프로세스 객체를 반환하므로, 필요시 종료할 수 있습니다.\n",
    "mcp_server_process = subprocess.Popen(\n",
    "    [\"uv\", \"run\", \"python\", \"server/mcp_server_remote.py\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")\n",
    "\n",
    "# 서버 실행 대기\n",
    "time.sleep(2)\n",
    "\n",
    "# 종료시\n",
    "# mcp_server_process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP 기반 MCP 서버 설정 예제\n",
    "http_server_config = {\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",  # 원격 MCP 서버의 HTTP 엔드포인트\n",
    "        \"transport\": \"streamable_http\",  # HTTP 스트리밍 방식으로 통신\n",
    "    },\n",
    "}\n",
    "\n",
    "# HTTP MCP 서버에 연결하여 클라이언트 생성\n",
    "client, http_tools = await setup_mcp_client(server_configs=http_server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP 서버용 LLM 설정 (OpenRouter 사용)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-4.1\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    ")  # 일관성을 위해 동일한 모델 사용\n",
    "\n",
    "# HTTP 도구를 사용하는 React Agent 생성\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    http_tools,\n",
    "    checkpointer=InMemorySaver(),  # HTTP로 로드된 도구들과 메모리 저장소 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 스레드로 HTTP 서버와 상호작용\n",
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 새로운 대화 세션을 위한 고유 ID 생성\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# HTTP MCP 서버를 통해 현재 시간 조회 요청\n",
    "response = await astream_graph(\n",
    "    agent,  # HTTP 도구가 연결된 에이전트\n",
    "    inputs={\n",
    "        \"messages\": [(\"human\", \"안녕하세요. 현재 시간을 알려주세요.\")]\n",
    "    },  # 시간 조회 요청\n",
    "    config=config,  # 대화 세션 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG MCP 활용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP 서버를 백그라운드에서 실행합니다.\n",
    "# 프로세스 객체를 반환하므로, 필요시 종료할 수 있습니다.\n",
    "mcp_server_process = subprocess.Popen(\n",
    "    [\"uv\", \"run\", \"python\", \"server/mcp_server_rag.py\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")\n",
    "\n",
    "# 서버 실행 대기\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG(검색 증강 생성) 서버 설정\n",
    "http_server_config = {\n",
    "    \"rag_mcp\": {\n",
    "        \"url\": \"http://127.0.0.1:8005/mcp\",  # 원격 MCP 서버의 HTTP 엔드포인트\n",
    "        \"transport\": \"streamable_http\",  # HTTP 스트리밍 방식으로 통신\n",
    "    },\n",
    "}\n",
    "\n",
    "# RAG MCP 서버에 연결하여 검색 도구 로딩\n",
    "client, rag_tools = await setup_mcp_client(server_configs=http_server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 서버용 LLM 설정 (OpenRouter 사용)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-4.1\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    ")  # 정확한 검색 결과를 위해 temperature=0\n",
    "\n",
    "# RAG 도구를 활용하는 React Agent 생성\n",
    "rag_agent = create_react_agent(\n",
    "    llm, rag_tools, checkpointer=InMemorySaver()  # RAG 검색 도구와 메모리 저장소 연결\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 에이전트를 사용한 문서 검색 예제\n",
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# RAG 전용 대화 세션 생성\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 정보 검색 요청\n",
    "_ = await astream_graph(\n",
    "    rag_agent,  # RAG 검색 도구가 연결된 에이전트\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"미드저니 V1에 대한 내용을 검색해 주세요.\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,  # RAG 세션 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## React Agent와 MCP 통합\n",
    "\n",
    "React Agent는 추론(Reason)과 행동(Act)을 반복하는 패턴을 구현합니다. MCP 도구와 함께 사용하면 강력한 에이전트를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# React Agent와 MCP를 통합하는 함수 정의\n",
    "async def create_mcp_react_agent(server_configs: dict):\n",
    "    \"\"\"\n",
    "    MCP 도구를 사용하는 React Agent를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        server_configs: MCP 서버 설정 딕셔너리\n",
    "\n",
    "    Returns:\n",
    "        LangGraph 에이전트: MCP 도구가 연결된 React Agent\n",
    "    \"\"\"\n",
    "\n",
    "    # MCP 클라이언트 생성 및 모든 서버의 도구를 통합 로딩\n",
    "    client, tools = await setup_mcp_client(server_configs=server_configs)\n",
    "\n",
    "    # OpenRouter를 통한 GPT 모델 사용 (일관성 유지)\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"openai/gpt-4.1\",\n",
    "        temperature=0,\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_BASE_URL\"),\n",
    "    )\n",
    "\n",
    "    # React 패턴을 구현하는 에이전트 생성\n",
    "    # - 추론(Reason): LLM이 상황을 분석하고 다음 행동 계획\n",
    "    # - 행동(Act): MCP 도구를 호출하여 구체적 작업 실행\n",
    "    agent = create_react_agent(\n",
    "        llm, tools, checkpointer=InMemorySaver()  # 상태 저장을 위한 체크포인터\n",
    "    )\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 MCP 서버 구성 - 날씨와 시간 서비스 통합\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",  # 로컬 날씨 서버\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",  # 빠른 로컬 통신\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",  # 원격 시간 서버\n",
    "        \"transport\": \"streamable_http\",  # HTTP 스트리밍 통신\n",
    "    },\n",
    "}\n",
    "\n",
    "# 다중 서버를 통합한 MCP React Agent 생성\n",
    "agent = await create_mcp_react_agent(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# React Agent 통합 테스트 - 다중 서버 활용\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, astream_graph\n",
    "\n",
    "# 멀티 도구 테스트를 위한 대화 세션 생성\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 1차 테스트: 시간 정보 요청 (HTTP 서버 사용)\n",
    "await astream_graph(\n",
    "    agent, inputs={\"messages\": [(\"human\", \"현재 시간을 알려주세요\")]}, config=config\n",
    ")\n",
    "\n",
    "# 2차 테스트: 날씨 정보 요청 (로컬 서버 사용)\n",
    "# 동일한 세션을 사용하여 대화 맥락 유지\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [(\"human\", \"현재 서울의 날씨도 알려주세요\")]},\n",
    "    config=config,  # 이전 대화와 연결된 동일한 세션 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 외부 MCP 서버에서 3rd Party 도구 사용하기\n",
    "\n",
    "**Smithery AI란?**\n",
    "\n",
    "- 사이트: https://smithery.ai/\n",
    "\n",
    "- Smithery AI는 AI 에이전트 서비스의 허브 역할을 하는 플랫폼입니다. \n",
    "\n",
    "에이전트형 AI(예: 대형 언어 모델)가 외부 도구나 정보와 효율적으로 연결될 수 있도록 설계된 MCP 서버들을 검색하고 배포하는 역할을 수행합니다. 즉, AI가 다양한 외부 서비스와 손쉽게 통신할 수 있게 하는 중개자이자 생태계 허브입니다.\n",
    "\n",
    "이 모든 서비스의 연결은 **MCP(Model Context Protocol)** 표준 프로토콜을 따릅니다. 이를 통해 AI가 검색, 프로그래밍, 파일 관리, 다양한 API 연동 등 여러 외부 기능을 하나의 허브(플랫폼)만으로 편리하게 활용할 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 설치 (npx)\n",
    "\n",
    "1. 터미널을 엽니다.\n",
    "2. `cd 05-MCP` 로 디렉토리를 이동합니다.\n",
    "3. `chmod +x install_node_npx.sh`\n",
    "4. `./install_node_npx.sh`\n",
    "\n",
    "`npx` 설치가 완료된 후 다음을 진행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부 MCP 서버와 내부 서버 통합 구성\n",
    "server_configs = {\n",
    "    # 내부 서버들\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",  # 로컬 날씨 서버\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "   \"rag_mcp\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",  # rag mcp\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "    # 외부 3rd Party 서버 (Smithery AI)\n",
    "    \"desktop-commander\": {\n",
    "        \"command\": \"npx\",\n",
    "        \"args\": [\n",
    "            \"-y\",\n",
    "            \"@smithery/cli@latest\",\n",
    "            \"run\",\n",
    "            \"@wonderwhy-er/desktop-commander\",\n",
    "            # \"--key\",\n",
    "            # \"your-smithery-key-here\"\n",
    "        ],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# 내부 + 외부 MCP 서버를 통합한 하이브리드 워크플로우 생성\n",
    "mcp_app = await create_mcp_react_agent(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 대화 세션 생성\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()}, recursion_limit=100)\n",
    "\n",
    "_ = await astream_graph(\n",
    "    mcp_app,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\"human\", \"현재 directory 를 조회하고, 폴더 리스트를 나열하세요. \")\n",
    "        ]\n",
    "    },\n",
    "    config=config,  # 연속 대화 세션 유지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = await astream_graph(\n",
    "    mcp_app,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"01-MCP-Tools.ipynb 파일을 읽고 내용을 정리하여 현재 폴더에더에 summary.txt 파일로 저장해 주세요.\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
