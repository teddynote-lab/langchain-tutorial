{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846b4bd7",
   "metadata": {},
   "source": [
    "# 문서 요약 (Summarization)\n",
    "\n",
    "이번 튜토리얼에서는 **LangChain을 활용한 문서 요약 기법** 을 학습합니다.\n",
    "\n",
    "문서 요약은 대용량 텍스트 데이터에서 핵심 정보를 추출하는 기술로, 비즈니스 환경에서 보고서 분석, 뉴스 요약, 연구 논문 정리 등에 활용됩니다.\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "- **다양한 요약 방식** 의 특징과 적용 시나리오 이해\n",
    "- **실무 적용 가능한 요약 시스템** 구축 방법 습득  \n",
    "- **효율적인 대용량 문서 처리** 기법 학습\n",
    "\n",
    "## 목차\n",
    "\n",
    "1. [기본 개념](#기본-개념) - 문서 요약의 핵심 과제와 해결 방안\n",
    "2. [실습 데이터 소개](#실습-데이터-소개) - 소프트웨어정책연구소 보고서 활용\n",
    "3. [환경 설정](#환경-설정) - 필수 라이브러리 및 API 키 설정\n",
    "4. [Stuff 방식](#stuff-방식) - 전체 문서를 한 번에 요약\n",
    "5. [Map-Reduce 방식](#map-reduce-방식) - 분할 요약 후 병합\n",
    "6. [Map-Refine 방식](#map-refine-방식) - 점진적 개선을 통한 고품질 요약\n",
    "7. [Chain of Density 방식](#chain-of-density-방식) - 반복적 개선을 통한 밀도 높은 요약\n",
    "8. [Clustering-Map-Refine 방식](#clustering-map-refine-방식) - 클러스터링을 활용한 대용량 문서 처리\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb767264",
   "metadata": {},
   "source": "## 기본 개념\n\n문서 요약에서 핵심적인 과제는 **긴 문서를 LLM의 컨텍스트 윈도우에 어떻게 효율적으로 전달할 것인가** 입니다.\n\n### 컨텍스트 윈도우 제한 문제\n\n대부분의 LLM은 **컨텍스트 윈도우 제한** 이 있습니다:\n\n| 모델 | 컨텍스트 윈도우 | 대략적인 단어 수 |\n|------|----------------|------------------|\n| GPT-4 | 128K 토큰 | 96,000 단어 |\n| Claude | 200K 토큰 | 150,000 단어 |\n\n실제 업무에서는 이보다 훨씬 긴 문서들을 처리해야 할 경우가 많습니다:\n- 연간 보고서 (수백 페이지)\n- 연구 논문 모음\n- 다수의 뉴스 기사\n\n### 문서 요약 전략 개요\n\n문서 요약을 위한 **4가지 검증된 전략** 을 소개합니다:\n\n| 전략 | 처리 방식 | 적합한 문서 길이 | 품질 | 처리 속도 |\n|------|-----------|------------------|------|-----------|\n| **Stuff** | 전체 문서 일괄 처리 | 짧음 | 높음 | 매우 빠름 |\n| **Map-Reduce** | 분할 후 병렬 처리 | 긴 | 보통 | 빠름 |\n| **Map-Refine** | 분할 후 순차 개선 | 긴 | 높음 | 보통 |\n| **Chain of Density** | 반복적 밀도 증가 | 중간 | 매우 높음 | 느림 |\n\n### 각 전략의 동작 원리\n\n#### Stuff 방식\n```\n전체 문서 → LLM → 요약 결과\n```\n\n#### Map-Reduce 방식  \n```\n문서1 → 요약1 ↘\n문서2 → 요약2 → 최종 요약\n문서3 → 요약3 ↗\n```\n\n#### Map-Refine 방식\n```\n문서1 → 요약1 → 요약1+문서2 → 개선된 요약 → ...\n```\n\n#### Chain of Density 방식\n```\n초기 요약 → 엔티티 추가 → 더 밀도 높은 요약 → 반복\n```\n\n### 전략 선택 가이드\n\n| 상황 | 문서 길이 | 품질 요구도 | 처리 시간 | 추천 전략 |\n|------|-----------|-------------|-----------|-----------|\n| 빠른 프로토타이핑 | 짧음 | 보통 | 빠름 | **Stuff** |\n| 대용량 처리 | 긴 | 보통 | 빠름 | **Map-Reduce** |\n| 고품질 요약 | 긴 | 높음 | 보통 | **Map-Refine** |\n| 최고품질 요구 | 중간~긴 | 최고 | 느림 | **Chain of Density** |\n\n### 각 전략의 특징 분석\n\n#### Stuff 방식\n- **장점**: 간단하고 직관적, 빠른 처리, 비용 효율적, 맥락 보존\n- **단점**: 큰 문서에는 사용 불가\n- **적용 사례**: 짧은 문서, 빠른 프로토타이핑\n\n#### Map-Reduce 방식  \n- **장점**: 병렬 처리 가능, 확장성 우수\n- **단점**: 문서간 연결성 손실 가능\n- **적용 사례**: 독립적인 챕터들, 뉴스 모음\n\n#### Map-Refine 방식\n- **장점**: 문서 순서와 맥락 유지, 점진적 개선\n- **단점**: 순차 처리로 인한 시간 소요\n- **적용 사례**: 스토리가 있는 문서, 순서가 중요한 자료\n\n#### Chain of Density 방식\n- **장점**: 정보 손실 최소화, 고품질 요약\n- **단점**: 여러 번의 LLM 호출로 비용 증가  \n- **적용 사례**: 고품질이 중요한 보고서, 학술 자료"
  },
  {
   "cell_type": "markdown",
   "id": "2d041c4b",
   "metadata": {},
   "source": [
    "## 실습 데이터 소개\n",
    "\n",
    "실습에서는 **실제 정부 기관 보고서** 를 활용하여 현실적인 문서 요약 시나리오를 경험합니다.\n",
    "\n",
    "### 사용 데이터\n",
    "\n",
    "**소프트웨어정책연구소(SPRi) AI Brief - 2023년 12월호**\n",
    "\n",
    "- **저자**: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- **출처**: https://spri.kr/posts/view/23669\n",
    "- **파일명**: `SPRI_AI_Brief_2023년12월호_F.pdf`\n",
    "\n",
    "### 파일 준비\n",
    "\n",
    "실습을 위해 다음 파일들을 `data` 폴더에 준비해주세요:\n",
    "\n",
    "| 파일명 | 용도 | 실습 방식 |\n",
    "|--------|------|----------|\n",
    "| `SPRI_AI_Brief_2023년12월호_F.pdf` | 정부 보고서 | Map-Reduce, Map-Refine |\n",
    "| `news.txt` | 뉴스 텍스트 | Stuff 방식 |\n",
    "\n",
    "### 데이터 선택 이유\n",
    "\n",
    "**현실적인 업무 시나리오** 를 반영한 데이터를 선택했습니다:\n",
    "\n",
    "- **현실적인 길이**: 일반적인 업무 문서와 비슷한 규모  \n",
    "- **구조화된 내용**: 명확한 섹션 구분으로 요약 결과 평가 용이  \n",
    "- **전문 용어 포함**: 실제 비즈니스 문서의 특성 반영  \n",
    "- **공개 문서**: 라이선스 문제 없이 활용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mz38ksth3of",
   "metadata": {},
   "source": [
    "## 환경 설정\n",
    "\n",
    "문서 요약 실습을 시작하기 전에 필요한 **API 키와 추적 도구** 를 설정합니다.\n",
    "\n",
    "### 필요한 설정\n",
    "\n",
    "1. **OpenRouter API Key**: GPT 모델 사용을 위해 필요\n",
    "2. **LangSmith 추적**: 실행 과정을 모니터링하고 디버깅을 위해 사용\n",
    "\n",
    "이 설정들이 완료되면 모든 LangChain 체인의 실행 과정을 **LangSmith 대시보드** 에서 실시간으로 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51304f",
   "metadata": {},
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LangChain-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e2e5f",
   "metadata": {},
   "source": "## Stuff 방식\n\n**Stuff 방식** 은 문서 요약의 가장 기본적이고 직관적인 접근 방법입니다.\n\n### 동작 원리\n\n**\"Stuff\"** 라는 이름은 **모든 문서를 프롬프트 안에 집어넣는다** 는 의미에서 유래했습니다.\n\n```\n전체 문서 → 단일 프롬프트 → LLM → 요약 결과\n```\n\n### 기술적 특징\n\n| 항목 | 설명 |\n|------|------|\n| **처리 방식** | 전체 문서를 한 번에 LLM에 전달 |\n| **API 호출 수** | 1회 (최소) |\n| **병렬 처리** | 불가능 |\n| **맥락 보존** | 완벽 (전체 문서 맥락 유지) |\n\n### 장점과 제약 사항\n\n#### 장점\n- **단순성**: 가장 이해하기 쉬운 방식\n- **속도**: 단 한 번의 LLM 호출로 완료\n- **비용 효율성**: API 호출 최소화\n- **맥락 보존**: 전체 문서의 맥락을 완벽히 유지\n\n#### 제약 사항  \n- **크기 제한**: LLM의 컨텍스트 윈도우 크기에 의존\n- **확장성 부족**: 대용량 문서에는 적용 불가\n\n### 적용 기준\n\n#### 높은 적합성\n- **짧은 문서**: 뉴스 기사, 블로그 포스트\n- **빠른 프로토타이핑**: 개념 검증용 개발  \n- **비용 절약**: 예산이 제한적인 프로젝트\n- **단순 요약**: 복잡한 처리가 불필요한 경우\n\n#### 낮은 적합성\n- **대용량 문서**: 컨텍스트 윈도우를 초과하는 문서\n- **복잡한 구조**: 여러 섹션으로 구성된 긴 보고서\n- **병렬 처리 필요**: 다수의 문서를 동시에 처리해야 하는 경우\n\n### 실무 활용 통계\n\n실제로 **업무 문서의 80% 이상** 은 Stuff 방식으로도 충분히 처리 가능합니다:\n\n| 문서 유형 | 평균 길이 | Stuff 방식 적용 가능성 |\n|----------|-----------|----------------------|\n| 뉴스 기사 | 1-3 페이지 | 매우 높음 |\n| 블로그 포스트 | 2-5 페이지 | 높음 |\n| 업무 보고서 | 5-10 페이지 | 보통 |\n| 연구 논문 | 10-30 페이지 | 낮음 |"
  },
  {
   "cell_type": "markdown",
   "id": "e911836c",
   "metadata": {},
   "source": "### 데이터 로드\n\n실습을 위해 **뉴스 텍스트 파일** 을 로드합니다. Stuff 방식은 상대적으로 짧은 문서에 적합하므로, PDF보다는 텍스트 파일을 사용합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce28f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# 뉴스데이터 로드\n",
    "loader = TextLoader(\"data/news.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 총 글자수 출력\n",
    "print(f\"총 글자수: {len(docs[0].page_content)}\")\n",
    "print(\"\\n========= 앞부분 미리보기 =========\\n\")\n",
    "\n",
    "# 문서의 앞부분 500자 미리보기\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46d1cc",
   "metadata": {},
   "source": "### 프롬프트 설정\n\n문서 요약을 위한 **전용 프롬프트** 를 로드합니다. 이 프롬프트는 한국어 요약에 최적화되어 있으며, 구조화된 형태로 가독성을 높입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcf878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# 한국어 요약 전용 프롬프트 로드\n",
    "prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "\n",
    "# 프롬프트 구조 출력 및 확인\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e58ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"\"\"Please summarize the sentence according to the following REQUEST.\n",
    "# REQUEST:\n",
    "# 1. Summarize the main points in bullet points in KOREAN.\n",
    "# 2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n",
    "# 3. Use various emojis to make the summary more interesting.\n",
    "# 4. Translate the summary into KOREAN if it is written in ENGLISH.\n",
    "# 5. DO NOT translate any technical terms.\n",
    "# 6. DO NOT include any unnecessary information.\n",
    "\n",
    "# CONTEXT:\n",
    "# {context}\n",
    "\n",
    "# SUMMARY:\"\n",
    "# \"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebce2c0",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_teddynote.callbacks import StreamingCallback\n\n# ChatOpenAI 모델 설정 (OpenRouter 사용)\nllm = ChatOpenAI(\n    model=\"openai/gpt-4.1\",  # OpenRouter에서 지원하는 모델명\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter 기본 URL\n    streaming=True,  # 스트리밍 출력 활성화\n    temperature=0,  # 출력의 일관성을 위해 temperature를 0으로 설정\n    callbacks=[StreamingCallback()],  # 실시간 출력을 위한 콜백\n)\n\n# Stuff 방식의 문서 요약 체인 생성\nstuff_chain = create_stuff_documents_chain(llm, prompt)\n\n# 문서를 입력으로 요약 실행\nanswer = stuff_chain.invoke({\"context\": docs})"
  },
  {
   "cell_type": "markdown",
   "id": "af0d63d3",
   "metadata": {},
   "source": "## Map-Reduce 방식\n\n**Map-Reduce 방식** 은 대용량 문서를 효율적으로 처리하기 위한 분할 정복 전략입니다.\n\n### 동작 원리\n\nMap-Reduce는 **2단계 처리 과정** 으로 구성됩니다:\n\n#### 1단계: Map (분할 처리)\n```\n긴 문서 → [청크1, 청크2, 청크3] → [요약1, 요약2, 요약3]\n```\n\n#### 2단계: Reduce (결과 통합)  \n```\n[요약1, 요약2, 요약3] → LLM → 최종 통합 요약\n```\n\n### 기술적 특징\n\n| 항목 | 설명 |\n|------|------|\n| **처리 방식** | 분할 후 병렬 처리 |\n| **API 호출 수** | N+1회 (N=청크 수) |\n| **병렬 처리** | 가능 (Map 단계) |\n| **맥락 보존** | 부분적 (청크 간 연결 제한) |\n\n### 장점과 제약 사항\n\n#### 장점\n- **병렬 처리**: 각 청크를 동시에 처리하여 시간 단축\n- **확장성**: 문서 길이에 관계없이 처리 가능\n- **비용 효율성**: 각 청크별로 독립적 처리\n\n#### 제약 사항\n- **맥락 연결**: 청크 간의 맥락 연결이 약해질 수 있음\n- **순서 의존성**: 순서가 중요한 문서에서는 정보 손실 가능\n\n### 성능 비교\n\n| 특성 | Map-Reduce | Stuff | Map-Refine |\n|------|------------|-------|------------|\n| **처리 속도** | 빠름 | 매우 빠름 | 느림 |\n| **맥락 보존** | 보통 | 높음 | 높음 |\n| **확장성** | 높음 | 낮음 | 높음 |\n| **비용** | 보통 | 낮음 | 높음 |\n| **병렬 처리** | 가능 | 불가능 | 불가능 |\n\n### 최적 활용 시나리오\n\n#### 높은 적합성\n| 문서 유형 | 적용도 | 이유 |\n|----------|--------|------|\n| 독립적인 챕터들 | 매우 높음 | 각 섹션이 상대적으로 독립적 |\n| 뉴스 모음 | 높음 | 여러 기사를 하나로 요약 |\n| 월간 보고서 모음 | 높음 | 각 월별 내용이 독립적 |\n| 제품 리뷰 모음 | 높음 | 각 리뷰가 독립적 |\n\n#### 낮은 적합성\n| 문서 유형 | 적용도 | 이유 |\n|----------|--------|------|\n| 연속된 스토리 | 낮음 | 맥락 연결 중요 |\n| 시계열 분석 | 낮음 | 순서와 연관성 중요 |\n| 논증형 문서 | 낮음 | 논리적 흐름 보존 필요 |\n\n### 실무 구현 고려사항\n\n#### Map 단계 최적화\n- **청크 크기 조정**: 너무 작으면 맥락 손실, 너무 크면 처리 지연\n- **중복 구간 설정**: 청크 간 연결성 확보\n- **핵심 정보 추출**: 요약보다는 핵심 내용 추출 권장\n\n#### Reduce 단계 최적화\n- **정보 통합**: 중복 제거 및 일관성 확보\n- **구조화**: 논리적 흐름으로 재구성\n- **품질 검증**: 전체 맥락에서의 일관성 확인"
  },
  {
   "cell_type": "markdown",
   "id": "2f6f0cce",
   "metadata": {},
   "source": "### 데이터 로드\n\n이번에는 **정부 연구기관의 AI 보고서** 를 활용합니다. Stuff 방식과 달리 **더 긴 문서** 를 처리하므로 PDF 파일을 사용합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# PDF 파일 로드\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2025_08.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 실습을 위해 문서의 일부만 사용 (3-8페이지)\n",
    "docs = docs[3:8]\n",
    "\n",
    "# 총 페이지 수 출력\n",
    "print(f\"총 페이지수: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a790030",
   "metadata": {},
   "source": "### Map 단계 - 분할 처리\n\n**Map 단계** 에서는 각 문서 청크에 대해 **핵심 내용을 추출** 합니다.\n\n#### 전략적 선택\n\n일반적으로 Map 단계에서는 **요약** 을 생성하지만, 여기서는 **핵심 내용 추출** 방식을 사용합니다:\n\n**핵심 내용 추출의 장점**\n- **정보 손실 최소화**: 중요한 세부사항 보존\n- **Reduce 단계 효율성**: 더 나은 최종 요약 품질\n- **유연성**: 다양한 문서 타입에 적응 가능\n\n**요약 vs 핵심 내용 추출**\n- **요약**: 각 청크를 짧게 요약하여 정보 압축\n- **핵심 내용 추출**: 중요한 정보만 선별하여 정보 보존\n\n**실무 팁**: 최종 품질이 중요하다면 핵심 내용 추출을, 빠른 처리가 필요하다면 요약을 선택하세요."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665b4e7",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom langchain import hub\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\n# ChatOpenAI 모델 설정 (Map 단계용, OpenRouter 사용)\nllm = ChatOpenAI(\n    model=\"openai/gpt-4.1\",  # OpenRouter에서 지원하는 모델명\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter 기본 URL\n    temperature=0,  # 일관된 추출을 위해 temperature 0 설정\n)\n\n# Map 단계용 프롬프트 다운로드\nmap_prompt = hub.pull(\"teddynote/map-prompt\")\n\n# 프롬프트 구조 출력 및 확인\nmap_prompt.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "id": "a0a45e03",
   "metadata": {},
   "source": "### Map Chain 생성\n\nMap 프롬프트와 LLM을 연결하여 **핵심 내용 추출 체인** 을 만듭니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 체인 생성: 프롬프트 → LLM → 문자열 출력파서\n",
    "map_chain = map_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f07bcb",
   "metadata": {},
   "source": "### 병렬 처리 실행\n\n**batch() 메서드** 를 사용하여 **모든 문서를 동시에 처리** 합니다. 이는 Map-Reduce의 핵심 장점인 병렬 처리를 활용하는 부분입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98913c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 문서에 대해 병렬로 핵심 내용 추출 실행\n",
    "doc_summaries = map_chain.batch(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출된 핵심 내용의 개수 확인\n",
    "len(doc_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76317f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 문서의 핵심 내용 확인\n",
    "print(doc_summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf1fe4",
   "metadata": {},
   "source": "### Reduce 단계 - 결과 통합\n\n**Reduce 단계** 에서는 각 문서에서 추출한 **핵심 내용들을 하나의 완전한 요약** 으로 통합합니다.\n\n#### Reduce 단계의 역할\n\n1. **정보 종합**: 분산된 핵심 내용들을 연결\n2. **중복 제거**: 반복되는 내용을 정리\n3. **구조화**: 논리적으로 일관된 요약 생성\n4. **최종 다듬기**: 읽기 쉬운 형태로 가공"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce 단계용 프롬프트 다운로드\n",
    "reduce_prompt = hub.pull(\"teddynote/reduce-prompt\")\n",
    "\n",
    "# 프롬프트 구조 출력 및 확인\n",
    "reduce_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf997a",
   "metadata": {},
   "source": "### Reduce Chain 생성\n\n여러 핵심 내용들을 **최종 요약으로 통합** 하는 Reduce 체인을 생성합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e3fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce 체인 생성: 프롬프트 → LLM → 문자열 출력파서\n",
    "reduce_chain = reduce_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d365133",
   "metadata": {},
   "source": "### 스트리밍 결과 확인\n\nReduce Chain을 실행하여 **실시간으로 최종 요약 결과** 를 확인해봅시다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ddc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# Reduce 체인을 스트리밍 모드로 실행\n",
    "answer = reduce_chain.stream(\n",
    "    {\n",
    "        \"doc_summaries\": \"\\n\".join(doc_summaries),\n",
    "        \"language\": \"Korean\",\n",
    "    }  # 추출된 핵심내용들을 하나로 합쳐서 입력\n",
    ")\n",
    "\n",
    "# 실시간 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c060863",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom langchain_core.runnables import chain\n\n\n@chain\ndef map_reduce_chain(docs):\n    \"\"\"Map-Reduce 전체 과정을 하나의 체인으로 통합한 함수\"\"\"\n\n    # Map 단계용 LLM 설정 (OpenRouter 사용)\n    map_llm = ChatOpenAI(\n        model=\"openai/gpt-4.1\",  # OpenRouter에서 지원하는 모델명\n        api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n        base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter 기본 URL\n        temperature=0,\n    )\n\n    # Map 프롬프트 다운로드\n    map_prompt = hub.pull(\"teddynote/map-prompt\")\n\n    # Map 체인 생성\n    map_chain = map_prompt | map_llm | StrOutputParser()\n\n    # 첫 번째 단계: 모든 문서에서 핵심 내용 추출 (Map)\n    doc_summaries = map_chain.batch(docs)\n\n    # Reduce 프롬프트 다운로드\n    reduce_prompt = hub.pull(\"teddynote/reduce-prompt\")\n\n    # Reduce 단계용 LLM 설정 (스트리밍 포함, OpenRouter 사용)\n    reduce_llm = ChatOpenAI(\n        model=\"openai/gpt-4.1\",  # OpenRouter에서 지원하는 모델명\n        api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n        base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter 기본 URL\n        temperature=0,\n        callbacks=[StreamingCallback()],  # 실시간 출력\n        streaming=True,\n    )\n\n    # Reduce 체인 생성\n    reduce_chain = reduce_prompt | reduce_llm | StrOutputParser()\n\n    # 두 번째 단계: 핵심 내용들을 최종 요약으로 통합 (Reduce)\n    return reduce_chain.invoke(\n        {\"doc_summaries\": \"\\n\".join(doc_summaries), \"language\": \"Korean\"}\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31de2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map-Reduce 통합 체인 실행 (전체 과정을 한번에)\n",
    "answer = map_reduce_chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e5d84",
   "metadata": {},
   "source": [
    "## Map-Refine 방식\n",
    "\n",
    "**Map-Refine 방식** 은 점진적 개선을 통해 고품질 요약을 생성하는 전략입니다.\n",
    "\n",
    "### 동작 원리\n",
    "\n",
    "Map-Refine은 **순차적 개선 과정** 으로 진행됩니다:\n",
    "\n",
    "#### Map 단계 - 초기 요약 생성\n",
    "```\n",
    "각 문서 청크 → LLM → 개별 요약들\n",
    "```\n",
    "\n",
    "#### Refine 단계 - 점진적 개선\n",
    "```\n",
    "요약1 + 문서2 → LLM → 개선된 요약\n",
    "개선된 요약 + 문서3 → LLM → 더 개선된 요약\n",
    "...\n",
    "```\n",
    "\n",
    "### 핵심 특징\n",
    "\n",
    "#### 장점\n",
    "- **맥락 보존**: 문서 순서와 연결성 유지\n",
    "- **점진적 개선**: 각 단계에서 품질 향상\n",
    "- **정보 누적**: 이전 정보를 잃지 않고 새 정보 추가\n",
    "\n",
    "#### 주의사항\n",
    "- **순차 처리**: 병렬 처리 불가로 시간 소요\n",
    "- **비용 증가**: Map-Reduce보다 많은 LLM 호출\n",
    "- **순서 의존성**: 문서 순서가 결과에 영향\n",
    "\n",
    "### Map-Reduce vs Map-Refine 비교\n",
    "\n",
    "| 특성 | Map-Reduce | Map-Refine |\n",
    "|------|------------|------------|\n",
    "| **처리 방식** | 병렬 처리 | 순차 처리 |\n",
    "| **맥락 보존** | 보통 | 우수 |\n",
    "| **처리 속도** | 빠름 | 느림 |\n",
    "| **품질** | 좋음 | 더 좋음 |\n",
    "| **비용** | 저렴 | 비쌈 |\n",
    "| **적용 상황** | 독립적 문서 | 연속적 문서 |\n",
    "\n",
    "### 최적 활용 시나리오\n",
    "\n",
    "#### 높은 적용도\n",
    "- **연속된 스토리**: 소설, 연대기, 일지 등\n",
    "- **시계열 데이터**: 월별/분기별 보고서의 추이 분석\n",
    "- **상세한 분석**: 품질이 속도보다 중요한 경우\n",
    "\n",
    "#### 낮은 적용도  \n",
    "- **독립적 섹션**: 각 챕터가 독립적인 문서\n",
    "- **긴급한 처리**: 빠른 결과가 필요한 경우\n",
    "- **제한된 예산**: 비용 절약이 중요한 상황"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c83b1d",
   "metadata": {},
   "source": "### Map 단계 - 개별 요약 생성\n\n**Map-Refine의 Map 단계** 에서는 각 문서 청크에 대해 **개별 요약** 을 생성합니다.\n\n#### Map-Reduce와의 차이점\n\n- **Map-Reduce**: 핵심 내용 추출 (키워드, 중요 정보)\n- **Map-Refine**: 완성된 요약 생성 (읽을 수 있는 요약문)\n\nMap-Refine에서는 각 청크마다 **완전한 요약문** 을 만드는 것이 중요합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70abc7",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom langchain import hub\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Map 단계용 LLM 생성 (OpenRouter 사용)\nmap_llm = ChatOpenAI(\n    model=\"openai/gpt-4.1\",  # OpenRouter에서 지원하는 모델명\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter 기본 URL\n    temperature=0,  # 일관된 요약을 위해 temperature 0 설정\n)\n\n# Map-Refine 전용 요약 프롬프트 다운로드\nmap_summary = hub.pull(\"teddynote/map-summary-prompt\")\n\n# 프롬프트 구조 출력 및 확인\nmap_summary.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "id": "2d9cbde9",
   "metadata": {},
   "source": "### Map Chain 생성\n\n요약용 프롬프트와 LLM을 연결하여 **개별 문서 요약 체인** 을 만듭니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 체인 생성: 프롬프트 → LLM → 문자열 출력파서\n",
    "map_chain = map_summary | map_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f61f4e",
   "metadata": {},
   "source": "### 단일 문서 요약 테스트\n\n첫 번째 문서에 대한 요약을 생성하여 **Map 단계의 출력 형태** 를 확인해봅시다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 문서의 개별 요약 생성 및 출력\n",
    "print(map_chain.invoke({\"documents\": docs[0], \"language\": \"Korean\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6804299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 처리를 위해 모든 문서를 동일한 형태로 변환\n",
    "input_doc = [{\"documents\": doc, \"language\": \"Korean\"} for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 입력 형태 확인\n",
    "input_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e847c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 문서에 대한 개별 요약 생성 (병렬 처리)\n",
    "print(map_chain.batch(input_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac15984",
   "metadata": {},
   "source": "### Refine 단계 - 점진적 개선\n\n**Refine 단계** 에서는 **이전 요약 + 새 요약** 을 합쳐서 **더 나은 요약** 으로 점진적으로 개선합니다.\n\n#### Refine 과정\n\n1. **첫 번째**: 요약1을 기준으로 시작\n2. **두 번째**: 요약1 + 요약2 → 개선된 요약A  \n3. **세 번째**: 개선된 요약A + 요약3 → 개선된 요약B\n4. **반복**: 모든 요약이 통합될 때까지 계속\n\n이 과정을 통해 **정보의 누적과 정제** 가 동시에 일어납니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine 단계용 프롬프트 다운로드\n",
    "refine_prompt = hub.pull(\"teddynote/refine-prompt\")\n",
    "\n",
    "# 프롬프트 구조 출력 및 확인\n",
    "refine_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97282d5f",
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Refine 단계용 LLM 생성 (OpenRouter 사용)\nrefine_llm = ChatOpenAI(\n    model=\"openai/gpt-4.1\",  # OpenRouter에서 지원하는 모델명\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter 기본 URL\n    temperature=0,  # 일관된 개선을 위해 temperature 0 설정\n)\n\n# Refine 체인 생성: 프롬프트 → LLM → 문자열 출력파서\nrefine_chain = refine_prompt | refine_llm | StrOutputParser()"
  },
  {
   "cell_type": "markdown",
   "id": "c09e3467",
   "metadata": {},
   "source": "### Map-Refine 통합 체인\n\n지금까지의 **Map과 Refine 과정을 하나의 체인** 으로 통합합니다.\n\n#### 전체 과정\n\n1. **Map**: 각 문서의 개별 요약 생성\n2. **Initialize**: 첫 번째 요약을 기준으로 설정  \n3. **Refine Loop**: 나머지 요약들을 순차적으로 통합\n4. **Final**: 최종 통합 요약 완성"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb389f",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom langchain_core.runnables import chain\n\n\n@chain\ndef map_refine_chain(docs):\n    \"\"\"Map-Refine 전체 과정을 하나의 체인으로 통합한 함수\"\"\"\n\n    # Map 단계: 각 문서에 대한 개별 요약 생성\n    map_summary = hub.pull(\"teddynote/map-summary-prompt\")\n\n    map_chain = (\n        map_summary\n        | ChatOpenAI(\n            model=\"openai/gpt-4.1\",  # OpenRouter에서 지원하는 모델명\n            api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n            base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter 기본 URL\n            temperature=0,\n        )\n        | StrOutputParser()\n    )\n\n    # 모든 문서를 배치 처리용 형태로 변환\n    input_doc = [{\"documents\": doc.page_content, \"language\": \"Korean\"} for doc in docs]\n\n    # 첫 번째 단계: 모든 문서에 대한 개별 요약 생성 (Map)\n    doc_summaries = map_chain.batch(input_doc)\n\n    # Refine 단계 설정\n    refine_prompt = hub.pull(\"teddynote/refine-prompt\")\n\n    refine_llm = ChatOpenAI(\n        model=\"openai/gpt-4.1\",  # OpenRouter에서 지원하는 모델명\n        api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n        base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter 기본 URL\n        temperature=0,\n        callbacks=[StreamingCallback()],  # 실시간 출력\n        streaming=True,\n    )\n\n    refine_chain = refine_prompt | refine_llm | StrOutputParser()\n\n    # 두 번째 단계: 순차적 개선 과정 (Refine)\n    previous_summary = doc_summaries[0]  # 첫 번째 요약으로 시작\n\n    # 나머지 요약들을 순차적으로 통합\n    for current_summary in doc_summaries[1:]:\n\n        previous_summary = refine_chain.invoke(\n            {\n                \"previous_summary\": previous_summary,  # 이전까지의 통합된 요약\n                \"current_summary\": current_summary,  # 새로 추가할 요약\n                \"language\": \"Korean\",\n            }\n        )\n        print(\"\\n\\n-----------------\\n\\n\")  # 각 단계 구분을 위한 출력\n\n    return previous_summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fc54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map-Refine 통합 체인 실행 (점진적 개선 과정을 실시간으로 확인)\n",
    "refined_summary = map_refine_chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e2068",
   "metadata": {},
   "source": "## Chain of Density 방식\n\n**Chain of Density (CoD)** 는 MIT 연구팀이 개발한 혁신적인 요약 기법으로, 반복적으로 요약의 **정보 밀도를 높여가는** 방식입니다.\n\n### 연구 배경\n\n#### 학술적 기반\n- **논문**: [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/pdf/2309.04269)\n- **연구 기관**: MIT\n- **핵심 발견**: 한 번에 완벽한 요약보다는 점진적 개선이 더 효과적\n\n#### 개발 목적\n**정보량과 가독성의 최적 균형점** 을 찾는 것이 주요 목표입니다.\n\n### 동작 원리\n\nCoD는 **4단계의 점진적 개선** 으로 진행됩니다:\n\n| 단계 | 과정 | 목표 |\n|------|------|------|\n| **1단계** | 초기 요약 생성 | 간단하고 이해하기 쉬운 요약 |\n| **2단계** | 중요 엔티티 식별 | 누락된 핵심 정보 파악 |\n| **3단계** | 밀도 증가 | 길이는 유지하며 정보량 증대 |\n| **4단계** | 반복 개선 | 최적 밀도까지 반복 |\n\n### 기술적 특징\n\n| 항목 | 설명 |\n|------|------|\n| **처리 방식** | 반복적 밀도 증가 |\n| **API 호출 수** | 3-5회 (반복 횟수에 따라) |\n| **병렬 처리** | 불가능 (순차적 개선) |\n| **정보 보존** | 매우 높음 (단계적 보완) |\n\n### 핵심 매개변수\n\n#### 주요 설정값\n| 매개변수 | 기본값 | 설명 | 조정 가이드 |\n|----------|--------|------|-------------|\n| **max_words** | 80 | 요약 최대 단어 수 | 문서 복잡도에 따라 60-120 |\n| **entity_range** | 1-3 | 한 번에 추가할 엔티티 수 | 신중함 vs 속도의 균형 |\n| **iterations** | 3-5 | 반복 횟수 | 품질 요구도에 따라 조정 |\n| **content_category** | Article | 콘텐츠 유형 | 문서 특성에 맞게 설정 |\n\n#### 매개변수 조합 전략\n| 문서 복잡도 | max_words | iterations | entity_range | 적용 사례 |\n|-------------|-----------|------------|--------------|-----------|\n| **낮음** | 60 | 3 | 1-2 | 뉴스, 간단한 글 |\n| **보통** | 80 | 3-4 | 1-3 | 일반 보고서, 기사 |\n| **높음** | 100 | 4 | 1-3 | 연구 논문, 전문 자료 |\n| **매우 높음** | 120 | 4-5 | 1-4 | 복잡한 학술 자료 |\n\n### 성능 및 효과\n\n#### 검증된 효과\n- **정보 밀도**: 일반 GPT-4 요약보다 높은 정보 밀도 달성\n- **품질**: 인간이 작성한 요약과 비슷한 수준\n- **Lead bias 감소**: 문서 앞부분 편향성 해결\n\n#### 다른 방식과의 비교\n| 특성 | Chain of Density | Map-Reduce | Map-Refine | Stuff |\n|------|------------------|------------|------------|-------|\n| **정보 밀도** | 최고 | 보통 | 높음 | 높음 |\n| **처리 시간** | 느림 | 빠름 | 보통 | 매우 빠름 |\n| **비용** | 높음 | 보통 | 높음 | 낮음 |\n| **품질 일관성** | 매우 높음 | 보통 | 높음 | 높음 |\n\n### 최적 활용 시나리오\n\n#### 높은 적합성\n- **중요 보고서**: 정보 손실을 최소화해야 하는 문서\n- **뉴스 요약**: 핵심 사실들을 놓치지 않고 전달\n- **연구 논문**: 복잡한 내용을 정확히 압축\n- **비즈니스 문서**: 의사결정에 필요한 모든 정보 보존\n\n#### 제한 사항\n- **처리 시간**: 다른 방식 대비 시간 소요\n- **비용**: 여러 번의 API 호출로 비용 증가\n- **문서 길이**: 매우 긴 문서에는 적합하지 않음\n\n### 실무 적용 팁\n\n#### 최적화 전략\n- **문서 전처리**: 불필요한 내용 제거 후 적용\n- **배치 처리**: 유사한 문서들을 묶어서 처리\n- **품질 모니터링**: 각 단계별 개선 효과 추적\n- **비용 관리**: 중요도에 따른 선택적 적용"
  },
  {
   "cell_type": "markdown",
   "id": "85d15ad3",
   "metadata": {},
   "source": "### 프롬프트 확인\n\n실제 CoD에서 사용하는 프롬프트는 [LangSmith Hub](https://smith.langchain.com/prompts/chain-of-density-prompt/4582aae0?organizationId=8c9eeb3c-2665-5405-bc50-0767fdf4ca8f)에서 확인할 수 있습니다."
  },
  {
   "cell_type": "markdown",
   "id": "36d2da33",
   "metadata": {},
   "source": "### 입력 파라미터 상세 설명\n\nCoD 체인의 성능을 좌우하는 핵심 매개변수들을 살펴봅시다:\n\n#### **content_category** (콘텐츠 유형)\n- **설명**: 요약할 콘텐츠의 종류를 명시\n- **기본값**: `\"Article\"`\n- **예시**: \"Article\", \"Research Paper\", \"News\", \"Report\"\n- **효과**: 콘텐츠 유형에 맞는 요약 스타일 적용\n\n#### **content** (요약할 내용)\n- **설명**: 실제 요약하고자 하는 텍스트 전문\n- **형태**: 문자열 (string)\n- **길이**: 제한 없음 (단, LLM 컨텍스트 윈도우 고려)\n\n#### **entity_range** (엔티티 개수 범위) \n- **설명**: 각 반복에서 추가할 엔티티 수\n- **기본값**: `\"1-3\"`  \n- **조정 가이드**:\n  - `\"1-2\"`: 신중한 개선 (더 정확, 더 느림)\n  - `\"1-3\"`: 균형잡힌 개선 (추천)  \n  - `\"2-4\"`: 빠른 개선 (더 빠름, 품질 저하 가능)\n\n#### **max_words** (최대 단어 수)\n- **설명**: 생성될 요약의 최대 길이 제한\n- **기본값**: `80`\n- **조정 가이드**:\n  - **50-80**: 간결한 요약 (빠른 파악용)\n  - **80-120**: 표준 요약 (일반적 용도)\n  - **120-150**: 상세 요약 (전문적 분석용)\n\n#### **iterations** (반복 횟수)\n- **설명**: 밀도 개선을 수행할 총 횟수\n- **기본값**: `3`  \n- **조정 가이드**:\n  - **2-3회**: 간단한 문서, 빠른 처리\n  - **3-4회**: 일반적인 문서 (추천)\n  - **4-5회**: 복잡한 문서, 최고 품질 필요\n  - **5회 이상**: 일반적으로 비추천 (수익 감소)\n\n### 매개변수 조합 전략\n\n| 문서 복잡도 | max_words | iterations | entity_range | 용도 |\n|-------------|-----------|------------|--------------|------|\n| **낮음** | 60 | 3 | 1-2 | 뉴스, 간단한 글 |\n| **보통** | 80 | 3 | 1-3 | 일반 보고서, 기사 |  \n| **높음** | 100 | 4 | 1-3 | 연구 논문, 전문 자료 |\n| **매우 높음** | 120 | 4-5 | 1-4 | 복잡한 학술 자료 |"
  },
  {
   "cell_type": "markdown",
   "id": "a9b267d4",
   "metadata": {},
   "source": "### CoD 체인 구성\n\n이제 **Chain of Density 체인** 을 실제로 구현해봅시다.\n\n#### 체인 설계\n\n1. **기본 체인**: 전체 과정(중간 결과 포함) 출력  \n2. **최종 요약 체인**: 최종 결과만 깔끔하게 추출\n\n#### 핵심 아이디어\n\n- **JSON 출력**: 각 반복 단계별 결과를 구조화된 형태로 저장\n- **점진적 개선**: 이전 결과를 바탕으로 더 나은 요약 생성  \n- **실시간 모니터링**: 각 단계별 개선 과정을 관찰 가능"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain of Density 전용 프롬프트 다운로드\n",
    "cod_prompt = hub.pull(\"teddynote/chain-of-density-prompt\")\n",
    "\n",
    "# 프롬프트 구조 출력 및 확인\n",
    "cod_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737020b",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport textwrap\nfrom langchain import hub\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import SimpleJsonOutputParser\n\n# CoD 체인의 모든 입력 파라미터에 대한 기본값 설정\ncod_chain_inputs = {\n    \"content\": lambda d: d.get(\"content\"),  # 요약할 내용 (필수)\n    \"content_category\": lambda d: d.get(\"content_category\", \"Article\"),  # 콘텐츠 유형\n    \"entity_range\": lambda d: d.get(\"entity_range\", \"1-3\"),  # 엔티티 개수 범위\n    \"max_words\": lambda d: int(d.get(\"max_words\", 80)),  # 최대 단어 수\n    \"iterations\": lambda d: int(d.get(\"iterations\", 5)),  # 반복 횟수\n}\n\n# Chain of Density 프롬프트 다운로드\ncod_prompt = hub.pull(\"teddynote/chain-of-density-prompt\")\n\n# 기본 CoD 체인 생성 (전체 과정 출력, OpenRouter 사용)\ncod_chain = (\n    cod_chain_inputs\n    | cod_prompt\n    | ChatOpenAI(\n        model=\"openai/gpt-4.1\",  # OpenRouter에서 지원하는 모델명\n        api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n        base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter 기본 URL\n        temperature=0,\n    )\n    | SimpleJsonOutputParser()  # JSON 형태로 파싱\n)\n\n# 최종 요약만 추출하는 체인 생성\ncod_final_summary_chain = cod_chain | (\n    lambda output: output[-1].get(\n        \"denser_summary\", '오류: 마지막 딕셔너리에 \"denser_summary\" 키가 없습니다'\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "id": "e6643d8b",
   "metadata": {},
   "source": "### 요약 대상 데이터 확인\n\nCoD 실습을 위해 **두 번째 페이지** 의 내용을 사용하겠습니다. 이는 적절한 길이와 복잡도를 가진 텍스트입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 번째 문서(페이지)의 내용을 CoD 입력으로 사용\n",
    "content = docs[1].page_content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566ef21",
   "metadata": {},
   "source": "### 실시간 CoD 과정 관찰\n\nCoD의 **점진적 개선 과정** 을 실시간으로 관찰할 수 있습니다.\n\n#### 스트리밍 동작 원리\n\n- **JSON 스트리밍**: 각 단계별 결과가 실시간으로 업데이트\n- **캐리지 리턴** (`\\r`): 같은 줄에 계속 덮어쓰며 진행 상황 표시  \n- **점진적 추가**: 새로운 반복이 완료될 때마다 결과 추가\n\n#### 출력 형태\n\n각 반복마다 다음 정보가 표시됩니다:\n- **missing_entities**: 새로 추가된 엔티티들\n- **denser_summary**: 개선된 요약문"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 빈 리스트 초기화\n",
    "results: list[dict[str, str]] = []\n",
    "\n",
    "# CoD 체인을 스트리밍 모드로 실행하여 점진적 개선 과정 실시간 관찰\n",
    "for partial_json in cod_chain.stream(\n",
    "    {\"content\": content, \"content_category\": \"Article\"}  # 입력 콘텐츠와 유형 지정\n",
    "):\n",
    "    # 각 반복마다 results를 업데이트 (점진적으로 더 많은 요약이 추가됨)\n",
    "    results = partial_json\n",
    "\n",
    "    # 현재까지의 결과를 같은 줄에 출력 (캐리지 리턴으로 덮어씀)\n",
    "    print(results, end=\"\\r\", flush=True)\n",
    "\n",
    "# 스트리밍 완료 후 줄바꿈\n",
    "print(\"\\n\")\n",
    "\n",
    "# 생성된 총 요약 단계 수 계산\n",
    "total_summaries = len(results)\n",
    "\n",
    "# 각 요약 단계별로 상세 분석 및 출력\n",
    "i = 1\n",
    "for cod in results:\n",
    "    # 새로 추가된 엔티티들을 쉼표로 구분하여 정리\n",
    "    added_entities = \", \".join(\n",
    "        [\n",
    "            ent.strip()\n",
    "            for ent in cod.get(\n",
    "                \"missing_entities\", 'ERR: \"missing_entities\" key not found'\n",
    "            ).split(\";\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 해당 단계의 개선된 요약 추출\n",
    "    summary = cod.get(\"denser_summary\", 'ERR: missing key \"denser_summary\"')\n",
    "\n",
    "    # 단계별 헤더 출력 (몇 번째 단계인지, 추가된 엔티티 표시)\n",
    "    print(\n",
    "        f\"### CoD Summary {i}/{total_summaries}, 추가된 엔티티(entity): {added_entities}\"\n",
    "        + \"\\n\"\n",
    "    )\n",
    "\n",
    "    # 요약 내용을 80자 너비로 줄바꿈하여 가독성 있게 출력\n",
    "    print(textwrap.fill(summary, width=80) + \"\\n\")\n",
    "    i += 1\n",
    "\n",
    "# 최종 결과 구분선과 함께 출력\n",
    "print(\"\\n============== [최종 요약] =================\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba91c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 요약 결과 재확인\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee272d",
   "metadata": {},
   "source": [
    "## Clustering-Map-Refine 방식\n",
    "\n",
    "**Clustering-Map-Refine** 은 대용량 문서 처리의 혁신적 접근법으로, 비용 효율성과 품질을 동시에 달성하는 지능형 요약 전략입니다.\n",
    "\n",
    "### 개발 배경 및 철학\n",
    "\n",
    "이 방법은 **gkamradt** 가 제안한 창의적 솔루션으로, 다음과 같은 문제의식에서 출발했습니다:\n",
    "\n",
    "#### 기존 방식의 문제점\n",
    "- **Map-Reduce**: 빠르지만 맥락 손실 가능\n",
    "- **Map-Refine**: 고품질이지만 시간과 비용이 많이 소요\n",
    "- **CoD**: 최고 품질이지만 매우 비쌈\n",
    "\n",
    "#### 해결 아이디어\n",
    "> **\"모든 문서를 처리할 필요가 있을까? 대표적인 문서들만 골라서 고품질로 처리하면 어떨까?\"**\n",
    "\n",
    "### 동작 원리\n",
    "\n",
    "Clustering-Map-Refine은 **3단계 지능형 처리** 로 진행됩니다:\n",
    "\n",
    "#### 1. 문서 분할 & 임베딩\n",
    "```\n",
    "대용량 문서 → [청크1, 청크2, ..., 청크N] → [벡터1, 벡터2, ..., 벡터N]\n",
    "```\n",
    "\n",
    "#### 2. 클러스터링  \n",
    "```\n",
    "N개 벡터 → KMeans → K개 클러스터 → 각 클러스터의 중심점 문서 선택\n",
    "```\n",
    "\n",
    "#### 3. 선별된 문서로 Map-Refine\n",
    "```\n",
    "대표 문서들 → Map-Refine → 고품질 최종 요약\n",
    "```\n",
    "\n",
    "### 핵심 아이디어\n",
    "\n",
    "#### 대표성 기반 선택\n",
    "- **클러스터 중심**: 각 클러스터에서 가장 **대표적인 문서** 선택\n",
    "- **의미적 유사성**: 비슷한 내용끼리 그룹핑하여 **중복 최소화**\n",
    "- **효율적 샘플링**: 전체 문서의 **핵심 정보만 추출**\n",
    "\n",
    "#### 처리 효율성\n",
    "- **문서 수 감소**: 79개 → 10개 (약 87% 감소)  \n",
    "- **비용 절감**: LLM 호출 횟수 대폭 감소\n",
    "- **시간 단축**: 처리 시간 현저히 단축\n",
    "\n",
    "### 성능 비교\n",
    "\n",
    "| 방식 | 문서 수 | 처리 시간 | 비용 | 품질 | 적용 상황 |\n",
    "|------|---------|-----------|------|------|-----------| \n",
    "| **전체 Map-Refine** | 79개 전체 | 매우 느림 | 매우 비쌈 | 최고 | 연구용 |\n",
    "| **Clustering-Map-Refine** | 10개 선별 | 보통 | 합리적 | 우수 | **실무용** |\n",
    "| **Map-Reduce** | 79개 전체 | 빠름 | 저렴 | 보통 | 빠른 처리용 |\n",
    "\n",
    "### 최적 활용 시나리오\n",
    "\n",
    "- **대용량 보고서**: 수십, 수백 페이지의 종합 보고서\n",
    "- **뉴스 아카이브**: 장기간 축적된 뉴스 모음 요약\n",
    "- **연구 논문 컬렉션**: 특정 주제의 다수 논문 통합 분석\n",
    "- **기업 문서**: 월별/분기별 보고서 통합\n",
    "\n",
    "### 기술적 구현 요소\n",
    "\n",
    "1. **임베딩 모델**: 문서의 의미를 벡터로 변환\n",
    "2. **클러스터링**: KMeans 알고리즘으로 유사 문서 그룹핑  \n",
    "3. **중심점 선택**: 각 클러스터에서 가장 대표적인 문서 식별\n",
    "4. **Map-Refine 적용**: 선별된 문서들로 고품질 요약 생성\n",
    "\n",
    "### 실무 적용 팁\n",
    "\n",
    "#### 최적화 전략\n",
    "- **클러스터 수 조정**: 문서 복잡도에 따라 5-15개 사이에서 조정  \n",
    "- **임베딩 모델 선택**: 도메인에 특화된 모델 사용 시 성능 향상  \n",
    "- **품질 검증**: 선택된 대표 문서들이 전체를 잘 대표하는지 확인  \n",
    "- **하이브리드 접근**: 중요도가 높은 문서는 별도로 추가 처리\n",
    "\n",
    "#### 성공 사례\n",
    "원 저자 gkamradt의 실험 결과:\n",
    "- **비용**: 기존 대비 80% 절감\n",
    "- **품질**: 전체 처리 대비 90% 수준 유지  \n",
    "- **만족도**: 실무진들의 높은 활용도\n",
    "\n",
    "### 참고 자료\n",
    "- [gkamradt의 원본 튜토리얼](https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20-%20Novice%20To%20Expert.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb12b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# 대용량 PDF 문서 로드 (전체 페이지)\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2025_08.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 총 페이지 수 확인\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746a903",
   "metadata": {},
   "source": "### 문서 통합\n\n**페이지 단위 구분을 제거** 하고 전체 내용을 하나의 연속된 텍스트로 통합합니다. \n\n#### 통합하는 이유\n- **의미적 연속성**: 페이지 경계를 넘나드는 내용을 자연스럽게 연결\n- **클러스터링 효과**: 의미적으로 관련된 내용끼리 더 정확한 그룹핑  \n- **청킹 최적화**: 내용 기반 분할로 더 의미있는 청크 생성"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca715d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 페이지의 내용을 하나의 연속된 텍스트로 통합\n",
    "texts = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 통합된 텍스트의 총 글자 수 확인\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a3162",
   "metadata": {},
   "source": "### 지능형 텍스트 분할\n\n**RecursiveCharacterTextSplitter** 를 사용하여 통합된 텍스트를 **의미 있는 청크** 로 분할합니다.\n\n#### 분할 매개변수\n- **chunk_size**: 500자 - 적절한 의미 단위 보장\n- **chunk_overlap**: 100자 - 청크 간 연속성 유지"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a11656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 텍스트 분할기 설정 (500자 청크, 100자 중복)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# 통합된 텍스트를 의미 있는 청크들로 분할\n",
    "split_docs = text_splitter.split_text(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8fef7",
   "metadata": {},
   "source": "### 분할 결과 확인\n\n**28K글자** 의 대용량 문서가 **79개의 의미있는 청크** 로 분할되었습니다. 이제 이 청크들을 클러스터링하여 대표 문서들을 선별하겠습니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7697377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할된 문서 청크의 총 개수 확인\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ddb9e",
   "metadata": {},
   "source": "### 벡터 임베딩 생성\n\n각 문서 청크를 **의미적 벡터** 로 변환합니다. 여기서는 두 가지 임베딩 모델 옵션을 제시합니다:\n\n1. **Upstage Embeddings**: 한국어에 최적화된 모델\n2. **OpenAI Embeddings**: 범용성이 뛰어난 모델\n\n실습에서는 **OpenAI Embeddings** 를 사용하겠습니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "# Upstage 임베딩 모델 설정 (한국어 최적화)\n",
    "embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "\n",
    "# 모든 문서 청크를 벡터로 변환\n",
    "vectors = embeddings.embed_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 설정 (범용성 우수)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 모든 문서 청크를 벡터로 변환\n",
    "vectors = embeddings.embed_documents(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287413d3",
   "metadata": {},
   "source": "### KMeans 클러스터링\n\n**79개 문서** 를 **10개 클러스터** 로 그룹핑합니다. \n\n#### 클러스터 개수 선택 기준\n- **너무 적으면**: 다양성 부족, 정보 손실  \n- **너무 많으면**: 효율성 감소, 중복 증가\n- **10개**: 79개 문서에 대한 **최적 균형점** (약 13% 선별)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 클러스터 수 설정 (문서 내용과 복잡도에 따라 조정 가능)\n",
    "num_clusters = 10\n",
    "\n",
    "# KMeans 클러스터링 수행 (random_state로 재현 가능한 결과 보장)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=123).fit(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193b358",
   "metadata": {},
   "source": "### 클러스터링 결과 확인\n\n각 문서가 **어느 클러스터에 속하는지** 라벨링된 결과를 확인합니다. 0-9까지의 숫자가 각 클러스터를 나타냅니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서가 할당된 클러스터 번호 확인 (0-9)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 경고 메시지 제거\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# t-SNE를 사용하여 고차원 벡터를 2차원으로 축소 (시각화를 위함)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_data_tsne = tsne.fit_transform(np.array(vectors))\n",
    "\n",
    "# seaborn 스타일 설정\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# 클러스터별로 색깔을 다르게 하여 2차원 산점도 그리기\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x=reduced_data_tsne[:, 0],  # X축: 첫 번째 차원\n",
    "    y=reduced_data_tsne[:, 1],  # Y축: 두 번째 차원\n",
    "    hue=kmeans.labels_,  # 색상: 클러스터 번호에 따라 구분\n",
    "    palette=\"deep\",  # 색상 팔레트\n",
    "    s=100,  # 점 크기\n",
    ")\n",
    "\n",
    "# 그래프 레이블 및 제목 설정\n",
    "plt.xlabel(\"Dimension 1\", fontsize=12)\n",
    "plt.ylabel(\"Dimension 2\", fontsize=12)\n",
    "plt.title(\"Clustered Embeddings\", fontsize=16)\n",
    "plt.legend(title=\"Cluster\", title_fontsize=12)\n",
    "\n",
    "# 배경색 설정\n",
    "plt.gcf().patch.set_facecolor(\"white\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ef9f9",
   "metadata": {},
   "source": "### 클러스터 대표 문서 선별\n\n각 클러스터에서 **중심점에 가장 가까운 문서** 를 찾아 대표 문서로 선택합니다.\n\n#### 선별 원리\n- **클러스터 중심점**: 해당 그룹의 평균적 특성을 나타내는 지점\n- **최소 거리**: 중심점과 가장 가까운 = 가장 대표적인 문서\n- **효율적 샘플링**: 전체 정보를 유지하면서 처리량 대폭 감소"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 각 클러스터의 중심점에 가장 가까운 문서를 저장할 빈 리스트 생성\n",
    "closest_indices = []\n",
    "\n",
    "# 각 클러스터별로 중심점에 가장 가까운 문서 찾기\n",
    "for i in range(num_clusters):\n",
    "\n",
    "    # 현재 클러스터의 중심점과 모든 문서 간의 유클리드 거리 계산\n",
    "    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
    "\n",
    "    # 거리가 가장 짧은(가장 가까운) 문서의 인덱스 찾기\n",
    "    closest_index = np.argmin(distances)\n",
    "\n",
    "    # 해당 인덱스를 대표 문서 리스트에 추가\n",
    "    closest_indices.append(closest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc00874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선별된 대표 문서들의 인덱스 확인\n",
    "closest_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6933f",
   "metadata": {},
   "source": "### 순서 정렬\n\n**Map-Refine 방식** 에서는 문서의 순서가 중요하므로, 선택된 인덱스를 **오름차순으로 정렬** 하여 원래 문서의 흐름을 유지합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b23e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 순서 유지를 위해 인덱스를 오름차순으로 정렬\n",
    "selected_indices = sorted(closest_indices)\n",
    "\n",
    "# 정렬된 인덱스 확인\n",
    "selected_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30262acd",
   "metadata": {},
   "source": "### 선별된 문서 객체 생성\n\n선택된 **10개의 대표 문서** 를 LangChain의 `Document` 객체로 변환합니다. 이는 Map-Refine 체인에서 요구하는 입력 형식입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13828899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# 선별된 인덱스에 해당하는 문서들을 Document 객체로 변환\n",
    "selected_docs = [Document(page_content=split_docs[doc]) for doc in selected_indices]\n",
    "\n",
    "# 생성된 Document 객체들 확인\n",
    "selected_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전에 정의한 Map-Refine 체인을 사용하여 선별된 10개 문서를 요약\n",
    "refined_summary = map_refine_chain.invoke(selected_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4846d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering-Map-Refine 방식의 최종 요약 결과 출력\n",
    "print(refined_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-CdOel15G-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}