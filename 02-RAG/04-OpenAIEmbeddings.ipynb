{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e46434a",
   "metadata": {},
   "source": "# 🔗 OpenAI Embeddings - 텍스트를 숫자로 변환하는 마법\n\n## 📚 개요\n\n**문서 임베딩(Document Embedding)**은 텍스트를 컴퓨터가 이해할 수 있는 **수치형 벡터**로 변환하는 핵심 기술입니다! 🎯\n\n### 🤔 임베딩이 뭐길래 이렇게 중요할까?\n\n일상생활로 비유해보면, 임베딩은 **번역기**와 같습니다:\n\n- **📝 텍스트** = 한국어 문장\n- **🔢 벡터** = 영어로 번역된 문장  \n- **🧠 AI 모델** = 번역 전문가\n\n컴퓨터는 숫자만 이해할 수 있기 때문에, 문자를 숫자로 변환해야 AI가 텍스트의 의미를 파악할 수 있습니다!\n\n### 🎯 OpenAI Embeddings의 특별한 점\n\n- **🎨 의미 이해**: 단순한 단어 매칭이 아닌 **문맥적 의미** 파악\n- **📊 고성능**: GPT 모델 기반의 뛰어난 벡터 생성\n- **⚡ 효율성**: 다양한 크기 옵션으로 용도에 맞는 선택 가능\n\n### 🔍 이 튜토리얼에서 배울 내용\n\n1. **⚙️ 환경 설정** - OpenAI API 준비하기\n2. **🔧 모델 정보** - 어떤 모델을 언제 써야 할까?\n3. **🔍 쿼리 임베딩** - 단일 텍스트를 벡터로 변환\n4. **📚 문서 임베딩** - 여러 문서를 한 번에 처리\n5. **📏 차원 조정** - 벡터 크기 최적화하기\n6. **🎯 유사도 계산** - 텍스트 간 유사성 측정하기\n\n### 💡 실생활 활용 예시\n\n- **🔍 검색 엔진**: \"맛있는 파스타 레시피\" → 관련 문서 찾기\n- **🤖 챗봇**: 사용자 질문의 의도 파악\n- **📊 문서 분류**: 이메일 스팸 감지, 뉴스 카테고리 분류\n- **💝 추천 시스템**: 비슷한 상품이나 콘텐츠 추천\n\n[📖 OpenAI 임베딩 공식 문서 더 알아보기](https://platform.openai.com/docs/guides/embeddings/embedding-models)"
  },
  {
   "cell_type": "markdown",
   "id": "d8748488",
   "metadata": {},
   "source": "---\n\n# 1. 환경 설정 🛠️\n\nOpenAI Embeddings를 사용하기 위한 **필수 준비 작업**을 진행해봅시다! API 키 설정과 추적 기능 활성화가 필요합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b37773",
   "metadata": {},
   "outputs": [],
   "source": "# API KEY를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API KEY 정보로드\nload_dotenv(override=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b64773",
   "metadata": {},
   "outputs": [],
   "source": "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"LangChain-Tutorial\")"
  },
  {
   "cell_type": "markdown",
   "id": "3ce3fe79",
   "metadata": {},
   "source": "# 2. 모델 정보 📊\n\nOpenAI에서 제공하는 **3가지 임베딩 모델**을 비교해보세요! 각 모델마다 **성능**, **비용**, **용도**가 다릅니다.\n\n## 📋 모델 비교표\n\n| 모델명                   | 페이지당 비용  | MTEB 성능 점수 | 최대 입력 길이 | 🎯 추천 용도 |\n|------------------------|-------------|-------------|------------|------------|\n| **text-embedding-3-small** | 62,500      | 62.3%       | 8,191      | ⭐ **일반적 용도** (가성비 최고) |\n| **text-embedding-3-large** | 9,615       | 64.6%       | 8,191      | 🎯 **고정밀 작업** (최고 성능) |\n| **text-embedding-ada-002** | 12,500      | 61.0%       | 8,191      | 📜 **레거시 지원** (구버전) |\n\n## 💡 어떤 모델을 선택해야 할까?\n\n### 🥇 **text-embedding-3-small** (이 튜토리얼의 선택!)\n- **장점**: 뛰어난 가성비, 빠른 속도\n- **추천 상황**: 일반적인 문서 검색, 챗봇, 추천 시스템\n- **💰 비용**: 가장 경제적\n\n### 🏆 **text-embedding-3-large** \n- **장점**: 최고 성능, 미세한 의미 차이도 포착\n- **추천 상황**: 법률 문서, 의료 텍스트 등 정밀도가 중요한 경우\n- **💰 비용**: 가장 비쌈\n\n### 📚 **text-embedding-ada-002**\n- **용도**: 기존 시스템 호환성 유지\n- **⚠️ 주의**: 신규 프로젝트에서는 3세대 모델 사용 권장"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23453c6a",
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import OpenAIEmbeddings\n\n# OpenAI의 \"text-embedding-3-small\" 모델을 사용하여 임베딩 객체를 생성합니다.\n# 가성비가 좋고 일반적인 용도에 적합한 모델입니다.\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00541ca6",
   "metadata": {},
   "outputs": [],
   "source": "# 임베딩 테스트를 위한 한국어 샘플 문장을 정의합니다.\n# 이 문장이 숫자 벡터로 변환될 예정입니다.\ntext = \"임베딩 테스트를 하기 위한 샘플 문장입니다.\""
  },
  {
   "cell_type": "markdown",
   "id": "758fbebb",
   "metadata": {},
   "source": "# 3. 쿼리 임베딩 🔍\n\n**쿼리 임베딩**은 **하나의 텍스트**를 벡터로 변환하는 가장 기본적인 방법입니다!"
  },
  {
   "cell_type": "markdown",
   "id": "08285a94",
   "metadata": {},
   "source": "## 🎯 embed_query() 메소드란?\n\n`embeddings.embed_query(text)` 함수는 **단일 텍스트를 1536차원의 벡터로 변환**하는 핵심 함수입니다!\n\n### 📚 어떻게 작동하나요?\n\n1. **📝 입력**: 텍스트 문장 (예: \"안녕하세요\")\n2. **🔄 처리**: OpenAI API를 통해 의미 분석\n3. **📊 출력**: 1536개 숫자로 구성된 리스트 (벡터)\n\n### 💡 주요 활용 사례\n\n- **🔍 검색**: 사용자 질문과 가장 유사한 문서 찾기\n- **📊 분석**: 텍스트의 감정, 주제 등 분석\n- **🤖 챗봇**: 사용자 의도 파악하기"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c5ca7",
   "metadata": {},
   "outputs": [],
   "source": "# 텍스트를 OpenAI API를 통해 임베딩 벡터로 변환합니다.\n# 결과는 1536개의 실수로 구성된 리스트가 됩니다.\nquery_result = embeddings.embed_query(text)"
  },
  {
   "cell_type": "markdown",
   "id": "a9c80557",
   "metadata": {},
   "source": "## 📊 결과 확인하기\n\n생성된 임베딩 벡터의 **처음 5개 값**을 확인해봅시다! \n\n**실제 벡터는 1536개의 숫자**로 구성되어 있지만, 너무 길어서 앞부분만 미리보기로 확인해보겠습니다. 🔍"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af37ee3",
   "metadata": {},
   "outputs": [],
   "source": "# 임베딩 벡터의 처음 5개 요소를 슬라이싱으로 확인합니다.\n# 실제로는 1536개의 float 값이 들어있습니다.\nquery_result[:5]"
  },
  {
   "cell_type": "markdown",
   "id": "f93f8fbc",
   "metadata": {},
   "source": "# 4. 문서 임베딩 📚\n\n**문서 임베딩**은 **여러 개의 텍스트를 한 번에** 벡터로 변환하는 효율적인 방법입니다!"
  },
  {
   "cell_type": "markdown",
   "id": "766fcf40",
   "metadata": {},
   "source": "## 🎯 embed_documents() 메소드란?\n\n`embeddings.embed_documents()` 함수는 **여러 문서를 일괄 처리**하는 강력한 기능입니다!\n\n### 📚 쿼리 vs 문서 임베딩의 차이점\n\n| 구분 | 🔍 쿼리 임베딩 | 📚 문서 임베딩 |\n|------|-------------|-------------|\n| **입력** | 하나의 텍스트 | 텍스트 리스트 |\n| **출력** | 단일 벡터 | 벡터 리스트 |\n| **용도** | 검색 쿼리, 사용자 질문 | 문서 컬렉션, 데이터베이스 구축 |\n\n### 🚀 배치 처리의 장점\n\n- **⚡ 속도**: 한 번에 여러 문서 처리로 시간 절약\n- **💰 비용**: API 호출 횟수 감소로 비용 절약  \n- **🛡️ 안정성**: 네트워크 오류 위험 감소"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5d387",
   "metadata": {},
   "outputs": [],
   "source": "# 동일한 텍스트를 4개 포함한 리스트를 일괄 임베딩 처리합니다.\n# 결과는 4개의 벡터로 구성된 리스트가 됩니다.\ndoc_result = embeddings.embed_documents(\n    [text, text, text, text]\n)  # 텍스트를 임베딩하여 문서 벡터를 생성합니다."
  },
  {
   "cell_type": "markdown",
   "id": "aafdee91",
   "metadata": {},
   "source": "## 📊 결과 분석하기\n\n문서 임베딩의 **구조**를 확인해봅시다! \n\n- `doc_result`는 **4개의 벡터를 담은 리스트**입니다\n- 각 벡터는 **1536개의 숫자**로 구성됩니다"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13b154",
   "metadata": {},
   "outputs": [],
   "source": "# 문서 임베딩 결과의 개수를 확인합니다 (4개 문서 = 4개 벡터)\nlen(doc_result)  # 문서 벡터의 길이를 확인합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7565438",
   "metadata": {},
   "outputs": [],
   "source": "# 첫 번째 문서의 임베딩 벡터에서 처음 5개 값을 확인합니다.\n# [0]은 첫 번째 문서, [:5]는 처음 5개 요소를 의미합니다.\ndoc_result[0][:5]"
  },
  {
   "cell_type": "markdown",
   "id": "d79f47ff",
   "metadata": {},
   "source": "# 5. 차원 조정 📏\n\n`text-embedding-3` 시리즈 모델의 **특별한 기능** 중 하나는 **벡터 차원을 자유롭게 조정**할 수 있다는 것입니다!\n\n## 🎯 차원 조정이 왜 필요할까?\n\n### 🏠 아파트 비유로 이해하기\n\n임베딩 벡터를 **아파트**에 비유해보세요:\n\n- **🏢 1536차원**: 초고층 아파트 (정보 많음, 저장공간 많이 필요)\n- **🏘️ 1024차원**: 중층 아파트 (적당한 정보, 적당한 공간)\n- **🏘️ 512차원**: 저층 아파트 (기본 정보, 공간 절약)\n\n### 💡 언제 차원을 줄일까?\n\n- **💰 비용 절약**: 저장 공간과 계산 시간 감소\n- **⚡ 속도 향상**: 벡터 연산이 더 빨라짐\n- **🔋 메모리 절약**: 제한된 환경에서 효율적 운용\n\n기본적으로 **text-embedding-3-small**은 **1536차원**의 벡터를 생성합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82bb0f",
   "metadata": {},
   "outputs": [],
   "source": "# 첫 번째 문서 벡터의 차원 수를 확인합니다.\n# text-embedding-3-small 모델의 기본 차원은 1536입니다.\nlen(doc_result[0])"
  },
  {
   "cell_type": "markdown",
   "id": "4d60da32",
   "metadata": {},
   "source": "## ⚙️ 차원(dimensions) 조정하기\n\n`dimensions` 매개변수를 사용하면 벡터 크기를 자유롭게 조정할 수 있습니다! \n\n### 📊 성능 vs 효율성 트레이드오프\n\n| 차원 수 | 🎯 정확도 | ⚡ 속도 | 💾 저장공간 | 🎯 추천 용도 |\n|--------|----------|-------|-----------|------------|\n| **1536** | 최고 | 보통 | 많음 | 고정밀 작업 |\n| **1024** | 높음 | 빠름 | 보통 | **일반적 용도** ⭐ |\n| **512** | 보통 | 매우 빠름 | 적음 | 실시간 처리 |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa8cb2",
   "metadata": {},
   "outputs": [],
   "source": "# 차원 수를 1024로 줄인 임베딩 객체를 생성합니다.\n# 기본 1536차원 대비 약 33% 메모리 절약 효과가 있습니다.\nembeddings_1024 = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fbcdaa",
   "metadata": {},
   "outputs": [],
   "source": "# 1024차원으로 설정된 임베딩의 실제 벡터 길이를 확인합니다.\n# 결과는 1024가 나와야 합니다.\nlen(embeddings_1024.embed_documents([text])[0])"
  },
  {
   "cell_type": "markdown",
   "id": "66a8ca34",
   "metadata": {},
   "source": "# 6. 유사도 계산 🎯\n\n임베딩의 **진짜 힘**은 텍스트 간의 **의미적 유사성**을 수치로 측정할 수 있다는 것입니다!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbb423",
   "metadata": {},
   "outputs": [],
   "source": "# 유사도 계산을 위한 다양한 테스트 문장들을 정의합니다.\n# 의미적 유사성을 비교하기 위해 한국어와 영어 문장을 혼합했습니다.\nsentence1 = \"안녕하세요? 반갑습니다.\"\nsentence2 = \"안녕하세요? 반갑습니다!\"\nsentence3 = \"안녕하세요? 만나서 반가워요.\"\nsentence4 = \"Hi, nice to meet you.\"\nsentence5 = \"I like to eat apples.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662aafb",
   "metadata": {},
   "outputs": [],
   "source": "# 코사인 유사도 계산을 위해 scikit-learn 라이브러리를 사용합니다.\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# 모든 테스트 문장들을 리스트로 묶어서 관리합니다.\nsentences = [sentence1, sentence2, sentence3, sentence4, sentence5]\n\n# 5개 문장을 모두 1024차원 벡터로 변환합니다 (배치 처리).\nembedded_sentences = embeddings_1024.embed_documents(sentences)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e24a4",
   "metadata": {},
   "outputs": [],
   "source": "# 두 벡터 간의 코사인 유사도를 계산하는 헬퍼 함수입니다.\n# 결과는 -1(완전 반대) ~ 1(완전 동일) 사이의 값으로 나타납니다.\ndef similarity(a, b):\n    return cosine_similarity([a], [b])[0][0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbf371",
   "metadata": {},
   "outputs": [],
   "source": "# 모든 문장 쌍에 대해 유사도를 계산하고 결과를 출력합니다.\n# 중복을 피하기 위해 i < j 조건을 사용합니다.\n\nfor i, sentence in enumerate(embedded_sentences):\n    for j, other_sentence in enumerate(embedded_sentences):\n        if i < j:  # 중복 비교를 피하기 위한 조건\n            print(\n                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n            )"
  },
  {
   "cell_type": "markdown",
   "id": "v5043pm4cx",
   "source": "## 📊 유사도 결과 해석하기\n\n위 결과를 통해 OpenAI Embeddings가 얼마나 **똑똑하게** 문장의 의미를 파악하는지 확인해보세요!\n\n### 🎯 예상되는 결과 패턴\n\n1. **🥇 최고 유사도** (0.99+): `\"안녕하세요? 반갑습니다.\"` ↔ `\"안녕하세요? 반갑습니다!\"`\n   - 거의 동일한 문장 (구두점만 다름)\n\n2. **🥈 높은 유사도** (0.9+): `\"안녕하세요? 반갑습니다.\"` ↔ `\"안녕하세요? 만나서 반가워요.\"`\n   - 같은 의미, 다른 표현\n\n3. **🥉 중간 유사도** (0.7-0.9): 한국어 인사말 ↔ `\"Hi, nice to meet you.\"`\n   - 같은 의미, 다른 언어 (놀라운 다국어 이해!)\n\n4. **🚫 낮은 유사도** (0.3 이하): 인사말 ↔ `\"I like to eat apples.\"`\n   - 완전히 다른 주제\n\n### 💡 실생활 활용 아이디어\n\n- **🔍 검색 엔진**: 정확히 일치하지 않아도 의미가 비슷한 문서 찾기\n- **🤖 챗봇**: 다양한 방식으로 표현된 질문을 같은 의도로 인식\n- **📚 문서 분류**: 비슷한 주제의 문서들을 자동으로 그룹핑\n- **🌐 번역**: 언어는 다르지만 의미가 같은 문장 찾기\n\n---\n\n## 🎉 마무리\n\n축하합니다! 이제 **OpenAI Embeddings**의 핵심 기능들을 모두 경험해보셨습니다:\n\n- ✅ **환경 설정** - API 키와 추적 시스템 구성\n- ✅ **모델 선택** - 용도에 맞는 적절한 모델 고르기\n- ✅ **쿼리 임베딩** - 단일 텍스트를 벡터로 변환\n- ✅ **문서 임베딩** - 여러 문서를 일괄 처리\n- ✅ **차원 조정** - 성능과 효율성의 균형 맞추기\n- ✅ **유사도 계산** - 텍스트 간 의미적 유사성 측정\n\n### 🚀 다음 단계\n\n이제 배운 내용을 활용해서:\n- **RAG (Retrieval-Augmented Generation)** 시스템 구축하기\n- **벡터 데이터베이스**와 연동하기  \n- **실제 프로젝트**에 임베딩 기능 적용하기\n\n**Happy Coding!** 🎯",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}