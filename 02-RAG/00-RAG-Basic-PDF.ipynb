{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b82288e",
   "metadata": {},
   "source": [
    "# 📚 RAG 기본 구조 완전 정복 가이드\n",
    "\n",
    "## 🎯 개요\n",
    "\n",
    "**RAG(Retrieval-Augmented Generation)** 는 **검색 기반 증강 생성**이라고 불리며, AI가 더 정확하고 신뢰할 수 있는 답변을 제공하기 위한 핵심 기술입니다! \n",
    "\n",
    "### 🤔 RAG가 왜 필요할까요?\n",
    "\n",
    "일반적인 AI 모델은 **훈련된 데이터**에만 의존하기 때문에:\n",
    "- **최신 정보 부족**: 2023년 이후 정보를 모름\n",
    "- **특정 도메인 지식 한계**: 회사 내부 문서나 전문 자료 접근 불가\n",
    "- **환각(Hallucination) 문제**: 없는 정보를 만들어내는 경우 발생\n",
    "\n",
    "**RAG는 이 문제를 해결**합니다! 📖→🔍→🤖\n",
    "\n",
    "### 🏗️ RAG의 핵심 아이디어\n",
    "\n",
    "> **\"아는 것만 답하자! 모르면 문서에서 찾아보자!\"**\n",
    "\n",
    "마치 **시험을 볼 때 교과서를 참고**하는 것처럼, AI가 답변하기 전에 **관련 문서를 먼저 검색**해서 참고한 후 답변하는 방식입니다.\n",
    "\n",
    "### 📋 목차\n",
    "\n",
    "1. **📖 RAG 기본 개념 이해하기**\n",
    "   - 사전작업(Pre-processing) - 1~4단계\n",
    "   - RAG 수행(RunTime) - 5~8단계\n",
    "\n",
    "2. **🛠️ 환경 설정**\n",
    "   - API KEY 설정\n",
    "   - LangSmith 추적 설정\n",
    "\n",
    "3. **🚀 RAG 기본 파이프라인 구현**\n",
    "   - 단계별 구현 (1~8단계)\n",
    "   - 전체 코드 통합\n",
    "\n",
    "4. **💡 실습 및 테스트**\n",
    "   - 실제 PDF 문서 처리\n",
    "   - 질의응답 테스트\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf05522",
   "metadata": {},
   "source": [
    "# 📖 RAG 기본 개념 이해하기\n",
    "\n",
    "## 🏭 RAG의 전체 과정 - 공장 라인처럼!\n",
    "\n",
    "RAG는 마치 **자동차 공장의 생산 라인**처럼 단계별로 체계적으로 진행됩니다.\n",
    "\n",
    "### 1️⃣ 사전작업(Pre-processing) - 📚 도서관 구축하기 (1~4단계)\n",
    "\n",
    "![rag-1.png](./assets/rag-1.png)\n",
    "\n",
    "![rag-1-graphic](./assets/rag-graphic-1.png)\n",
    "\n",
    "**\"AI가 참고할 수 있는 스마트 도서관을 미리 만들어두는 과정\"**\n",
    "\n",
    "- **1단계 문서로드(Document Load)** 📄: 참고할 문서들을 불러옵니다\n",
    "  - 마치 **도서관에 책을 들여놓는** 과정\n",
    "  - PDF, 텍스트, 웹페이지 등 다양한 형태의 문서 지원\n",
    "\n",
    "- **2단계 분할(Text Split)** ✂️: 문서를 적절한 크기로 나눕니다  \n",
    "  - **책을 장(chapter) 단위로 나누는** 것과 같음\n",
    "  - 너무 크면 검색이 어렵고, 너무 작으면 맥락을 잃음\n",
    "\n",
    "- **3단계 임베딩(Embedding)** 🧮: 텍스트를 숫자 벡터로 변환합니다\n",
    "  - **각 책에 ISBN 번호를 매기는** 것처럼 컴퓨터가 이해할 수 있는 형태로 변환\n",
    "  - 의미가 비슷한 문장들은 비슷한 숫자 패턴을 가짐\n",
    "\n",
    "- **4단계 벡터DB 저장** 💾: 변환된 벡터를 데이터베이스에 저장합니다\n",
    "  - **도서관 카탈로그 시스템**처럼 빠른 검색이 가능한 형태로 저장\n",
    "\n",
    "### 2️⃣ RAG 수행(RunTime) - 🤖 실시간 질의응답 (5~8단계)\n",
    "\n",
    "![rag-2.png](./assets/rag-2.png)\n",
    "\n",
    "![](./assets/rag-graphic-2.png)\n",
    "\n",
    "**\"사용자 질문이 들어오면 즉시 답변을 만들어내는 과정\"**\n",
    "\n",
    "- **5단계 검색기(Retriever)** 🔍: 질문과 관련된 문서를 찾아옵니다\n",
    "  - **사서가 질문에 맞는 책을 찾아주는** 역할\n",
    "  - Dense 검색 (의미 기반) + Sparse 검색 (키워드 기반) 활용\n",
    "\n",
    "- **6단계 프롬프트 생성** 📝: AI에게 줄 명령어를 만듭니다\n",
    "  - **\"이 자료를 참고해서 질문에 답변해줘\"** 라는 지시사항\n",
    "  - 검색된 문서 내용을 context로 포함\n",
    "\n",
    "- **7단계 LLM 실행** 🤖: AI 모델이 답변을 생성합니다  \n",
    "  - **똑똑한 비서가 자료를 보고 답변을 작성**하는 단계\n",
    "  - GPT-4, Claude 등 다양한 모델 사용 가능\n",
    "\n",
    "- **8단계 Chain 구성** 🔗: 전체 과정을 하나로 연결합니다\n",
    "  - **검색→프롬프트→AI→출력**의 전체 파이프라인\n",
    "  - 사용자는 질문만 하면 모든 과정이 자동으로 실행\n",
    "\n",
    "### 💡 핵심 포인트\n",
    "\n",
    "> **\"사전에 도서관을 잘 만들어두면, 실시간으로 정확한 답변을 얻을 수 있다!\"**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c423a8",
   "metadata": {},
   "source": [
    "## 📄 실습용 문서 소개\n",
    "\n",
    "### 🏢 소프트웨어정책연구소(SPRi) AI 브리프\n",
    "\n",
    "이번 실습에서는 **실제 업무에서 자주 접하는 PDF 문서**를 사용하여 RAG 시스템을 구현해보겠습니다.\n",
    "\n",
    "**문서 정보** 📋\n",
    "- **발행처**: 소프트웨어정책연구소(SPRi) \n",
    "- **제목**: 2023년 12월호 AI Brief\n",
    "- **저자**: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- **링크**: https://spri.kr/posts/view/23669\n",
    "- **파일명**: `SPRI_AI_Brief_2023년12월호_F.pdf`\n",
    "\n",
    "### 💡 왜 이 문서를 선택했을까요?\n",
    "\n",
    "✅ **실용성**: 실제 업무 환경에서 자주 접하는 **정책 보고서** 형태  \n",
    "✅ **적절한 길이**: 너무 짧지도, 너무 길지도 않은 적당한 분량  \n",
    "✅ **구조화된 내용**: 제목, 본문, 표 등 다양한 요소 포함  \n",
    "✅ **최신 정보**: 2023년 AI 동향 정보로 RAG의 장점 확인 가능\n",
    "\n",
    "### 📁 파일 준비\n",
    "\n",
    "_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n",
    "\n",
    "```\n",
    "프로젝트 폴더/\n",
    "├── data/\n",
    "│   └── SPRI_AI_Brief_2023년12월호_F.pdf  ← 여기에 파일 저장\n",
    "└── 02-RAG/\n",
    "    └── 00-RAG-Basic-PDF.ipynb\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🛠️ 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224fd32",
   "metadata": {},
   "source": [
    "### 🔑 API KEY 설정\n",
    "\n",
    "RAG 시스템을 구동하려면 **OpenAI API 키**가 필요합니다. 마치 **건물 출입증**처럼 AI 서비스를 이용하기 위한 인증키라고 생각하시면 됩니다.\n",
    "\n",
    "#### 💡 환경변수로 관리하는 이유?\n",
    "\n",
    "- **보안**: 코드에 직접 키를 노출하지 않음\n",
    "- **편의성**: 한 번 설정하면 모든 프로젝트에서 재사용\n",
    "- **안전**: 실수로 GitHub에 업로드해도 키가 유출되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ab505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv(override=True)  # 기존 환경변수보다 .env 파일 우선 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024d0c5",
   "metadata": {},
   "source": [
    "### 📊 LangSmith 추적 설정 (선택사항)\n",
    "\n",
    "**LangSmith**는 AI 애플리케이션의 **블랙박스 같은 역할**을 합니다! 🛫\n",
    "\n",
    "#### 🤔 LangSmith가 왜 필요할까요?\n",
    "\n",
    "RAG 시스템은 여러 단계를 거치며 복잡하게 작동합니다:\n",
    "- **문서 검색** → **프롬프트 생성** → **AI 응답** → **최종 출력**\n",
    "\n",
    "만약 결과가 이상하다면 **어느 단계에서 문제가 생겼는지** 파악하기 어렵죠. 🤷‍♂️\n",
    "\n",
    "#### ✨ LangSmith의 장점\n",
    "\n",
    "- **🔍 투명성**: 각 단계별로 무슨 일이 일어났는지 실시간 확인\n",
    "- **🐛 디버깅**: 문제가 생긴 지점을 정확히 찾아냄  \n",
    "- **📈 성능 분석**: 어느 부분이 느린지, 비용이 많이 드는지 파악\n",
    "- **📝 대화 기록**: 모든 질의응답 과정이 자동으로 기록됨\n",
    "\n",
    "#### 💡 설정 방법\n",
    "\n",
    "LangSmith를 사용하고 싶다면:\n",
    "1. https://smith.langchain.com 에서 가입\n",
    "2. API 키를 환경변수에 설정\n",
    "3. 아래 코드로 추적 활성화\n",
    "\n",
    "**필수는 아니지만 강력 추천**합니다! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LangChain-Tutorial\")  # LangSmith 프로젝트명 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d050a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🚀 RAG 기본 파이프라인 구현 (1~8단계)\n",
    "\n",
    "이제 **실제 코드로 RAG 시스템**을 구현해봅시다! 마치 **레고 블록을 조립**하듯이 단계별로 차근차근 만들어보겠습니다. 🧱\n",
    "\n",
    "## 📚 필요한 라이브러리 import\n",
    "\n",
    "RAG 구현에 필요한 도구들을 불러옵니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 분할을 위한 텍스트 스플리터\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# PDF 문서 로더\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "# 벡터 저장소 (FAISS - 페이스북이 개발한 고속 유사도 검색 라이브러리)\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# 출력 파서 (AI 응답을 문자열로 변환)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# 체인 연결을 위한 패스스루 (데이터를 그대로 전달)\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# 프롬프트 템플릿\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# OpenAI 모델들 (ChatGPT와 임베딩 모델)\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e783c4",
   "metadata": {},
   "source": [
    "## 💡 RAG 핵심 구조 - 뼈대코드(Skeleton Code)\n",
    "\n",
    "아래는 **RAG의 핵심 구조를 이해하기 위한 뼈대코드**입니다! \n",
    "\n",
    "### 🎯 뼈대코드의 장점\n",
    "\n",
    "- **📖 학습 용이**: 전체적인 흐름을 한눈에 파악\n",
    "- **🔧 모듈식 설계**: 각 단계별로 독립적으로 개선 가능  \n",
    "- **⚡ 빠른 프로토타이핑**: 기본 구조 위에 기능 추가\n",
    "- **🎨 커스터마이징**: 상황에 맞게 각 단계를 변경 가능\n",
    "\n",
    "### 🚀 시작해봅시다!\n",
    "\n",
    "각 단계별로 어떤 일이 일어나는지 주석과 함께 확인해보세요:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377894c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 1: 문서 로드(Load Documents)\n",
    "# PDF 파일을 읽어서 텍스트로 변환하는 로더 생성\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "# 문서를 실제로 로드 (페이지별로 분리되어 저장됨)\n",
    "docs = loader.load()\n",
    "print(f\"문서의 페이지수: {len(docs)}\")  # 총 페이지 수 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34f4fe",
   "metadata": {},
   "source": [
    "### 📄 문서 내용 확인하기\n",
    "\n",
    "로드된 문서가 제대로 읽혔는지 **10번째 페이지 내용**을 확인해봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10번째 페이지(인덱스 10)의 내용을 출력하여 문서가 제대로 로드되었는지 확인\n",
    "print(docs[10].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2963b",
   "metadata": {},
   "source": [
    "### 🏷️ 메타데이터 확인하기\n",
    "\n",
    "각 문서 페이지에는 **메타데이터(Metadata)** 가 함께 저장됩니다. 이는 **도서관 도서카드**와 같은 역할로, 출처 정보를 담고 있어 나중에 답변의 근거를 제시할 때 유용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10번째 페이지 문서의 전체 정보(메타데이터 포함)를 딕셔너리 형태로 출력\n",
    "# page_content: 실제 텍스트 내용, metadata: 파일명, 페이지 번호 등 부가 정보\n",
    "docs[10].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 2: 문서 분할(Split Documents)\n",
    "# 긴 문서를 검색하기 적절한 크기로 나누는 텍스트 스플리터 생성\n",
    "# chunk_size=500: 각 청크의 최대 글자 수\n",
    "# chunk_overlap=50: 인접한 청크 간 겹치는 글자 수 (문맥 유지를 위함)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# 로드된 문서들을 설정된 크기로 분할\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"분할된 청크의수: {len(split_documents)}\")  # 분할된 총 청크 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "# 텍스트를 벡터(숫자 배열)로 변환하는 임베딩 모델 생성\n",
    "# text-embedding-3-small: OpenAI의 최신 임베딩 모델 (성능 좋고 비용 효율적)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f47754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# 분할된 문서들을 임베딩으로 변환하여 FAISS 벡터 데이터베이스에 저장\n",
    "# FAISS: Facebook이 개발한 고속 유사도 검색 라이브러리\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 데이터베이스에서 \"구글\"과 유사한 문서 검색 테스트\n",
    "# similarity_search: 쿼리와 가장 유사한 문서들을 반환 (기본 4개)\n",
    "for doc in vectorstore.similarity_search(\"구글\"):\n",
    "    print(doc.page_content)  # 검색된 각 문서의 내용 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 벡터 데이터베이스를 검색 가능한 retriever로 변환\n",
    "# retriever는 쿼리를 받아 관련 문서를 자동으로 찾아주는 역할\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29da7b4",
   "metadata": {},
   "source": [
    "### 🔍 검색기 테스트\n",
    "\n",
    "생성된 검색기가 제대로 작동하는지 **실제 질문**으로 테스트해봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c0ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색기에 질문을 넣어서 관련 문서 청크들을 검색\n",
    "# invoke: 검색기를 실행하는 메서드\n",
    "# 질문과 관련된 문서들을 유사도 순으로 반환\n",
    "retriever.invoke(\"삼성전자가 자체 개발한 AI 의 이름은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 6: 프롬프트 생성(Create Prompt)  \n",
    "# AI에게 검색된 문서를 바탕으로 질문에 답변하도록 지시하는 프롬프트 템플릿\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question accurately and comprehensively.\n",
    "\n",
    "Instructions:\n",
    "- Only use information from the provided context\n",
    "- If you don't know the answer based on the context, say \"죄송하지만 제공된 문서에서는 해당 정보를 찾을 수 없습니다.\"\n",
    "- Answer in Korean with a clear and professional tone\n",
    "- Cite specific parts of the context when possible\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ed5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 질문-답변을 수행할 ChatGPT 모델 설정\n",
    "# gpt-4.1: 최신 GPT-4 모델, temperature=0: 일관된 답변 생성 (창의성 최소화)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 8: 체인(Chain) 생성\n",
    "# 검색→프롬프트→LLM→출력 파싱의 전체 파이프라인을 하나의 체인으로 연결\n",
    "# {context: retriever, question: RunnablePassthrough()}: 질문을 받아 검색 결과와 함께 전달\n",
    "# |: 파이프라이프 연산자로 각 단계를 순차적으로 연결\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}  # 질문을 검색기에 전달하고 결과를 context로 설정\n",
    "    | prompt  # 검색 결과와 질문을 프롬프트 템플릿에 삽입\n",
    "    | llm  # 완성된 프롬프트를 LLM에 전달하여 답변 생성\n",
    "    | StrOutputParser()  # LLM 응답을 문자열로 변환\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f4aeb",
   "metadata": {},
   "source": [
    "### 🎯 RAG 시스템 실행하기\n",
    "\n",
    "드디어 **완성된 RAG 체인**을 실행할 시간입니다! 🚀\n",
    "\n",
    "이제 사용자가 질문을 입력하면:\n",
    "1. **🔍 검색기**가 관련 문서를 찾고\n",
    "2. **📝 프롬프트**가 질문과 문서를 조합하여\n",
    "3. **🤖 AI**가 답변을 생성합니다\n",
    "\n",
    "모든 과정이 **자동으로** 실행됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 실행(Run Chain)\n",
    "# 완성된 RAG 파이프라인에 질문을 입력하여 실행\n",
    "question = \"삼성전자가 자체 개발한 AI 의 이름은?\"\n",
    "# invoke: 체인을 실행하는 메서드 (질문 → 검색 → 프롬프트 → LLM → 답변)\n",
    "response = chain.invoke(question)\n",
    "print(response)  # 최종 답변 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8444e43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🔧 전체 코드 통합\n",
    "\n",
    "## 💡 완성된 RAG 시스템\n",
    "\n",
    "지금까지 단계별로 구현한 내용을 **하나의 완전한 코드**로 통합했습니다! \n",
    "\n",
    "### 🎯 이 코드의 특징\n",
    "\n",
    "- **📖 Production Ready**: 실제 프로젝트에서 바로 사용 가능\n",
    "- **⚙️ 최적화된 설정**: chunk_size를 1000으로 증가시켜 더 많은 맥락 포함\n",
    "- **🔄 재사용 가능**: 다른 PDF 문서에도 바로 적용 가능\n",
    "- **📝 완전한 파이프라인**: Import부터 실행까지 모든 과정 포함\n",
    "\n",
    "### 🚀 실행 방법\n",
    "\n",
    "아래 코드를 **한 번에 실행**하면 완전한 RAG 시스템이 구동됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc45dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# 단계 1: 문서 로드(Load Documents)\n",
    "loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)  \n",
    "# chunk_size를 1000으로 설정하여 더 많은 맥락 정보 포함\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "# text-embedding-3-small 모델 사용 (성능과 비용의 최적 균형)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장\n",
    "# FAISS 벡터 데이터베이스 생성 및 문서 저장\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 질의와 유사한 문서를 검색하는 retriever 생성\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 개선된 프롬프트 템플릿으로 더 정확한 답변 유도\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question accurately and comprehensively.\n",
    "\n",
    "Instructions:\n",
    "- Only use information from the provided context\n",
    "- If you don't know the answer based on the context, say \"죄송하지만 제공된 문서에서는 해당 정보를 찾을 수 없습니다.\"\n",
    "- Answer in Korean with a clear and professional tone\n",
    "- Cite specific parts of the context when possible\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# GPT-4o 모델 사용, temperature=0으로 일관된 답변 생성\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성  \n",
    "# 전체 RAG 파이프라인을 하나의 실행 가능한 체인으로 구성\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5986cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 실행(Run Chain) - 최종 테스트\n",
    "# RAG 시스템에 질문을 입력하여 완전한 파이프라인 실행\n",
    "question = \"삼성전자가 자체 개발한 AI 의 이름은?\"\n",
    "# 질문 → 문서검색 → 프롬프트생성 → AI답변 → 결과반환의 전체 과정이 자동 실행\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
