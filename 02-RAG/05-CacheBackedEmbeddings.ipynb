{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c6eb01",
   "metadata": {},
   "source": "# 📚 CacheBackedEmbeddings - 임베딩 캐싱으로 성능 최적화하기\n\n## 📖 목차\n1. [CacheBackedEmbeddings란?](#1-cachebackedembeddings란)\n2. [LocalFileStore를 사용한 영구 캐싱](#2-localfilestore를-사용한-영구-캐싱)\n3. [InMemoryByteStore를 사용한 임시 캐싱](#3-inmemorybytestore를-사용한-임시-캐싱)\n\n---\n\n## 📋 개요\n\n**CacheBackedEmbeddings**는 임베딩 계산 결과를 저장해두고 재사용할 수 있게 해주는 **똑똑한 캐싱 시스템**입니다! 💡\n\n### 🤔 왜 캐싱이 필요할까요?\n\n임베딩 생성은 **시간과 비용이 많이 드는 작업**입니다:\n- **⏱️ 시간 소모**: 같은 텍스트를 반복해서 임베딩으로 변환\n- **💰 비용 발생**: OpenAI API 호출할 때마다 요금 부과\n- **🔄 중복 작업**: 이미 처리한 텍스트를 다시 처리하는 비효율\n\n### 🎯 CacheBackedEmbeddings의 핵심 기능\n\n**CacheBackedEmbeddings**는 마치 **사전(辭典)**처럼 작동합니다:\n\n1. **📝 첫 번째 요청**: \"사과\"라는 단어의 임베딩을 계산하고 저장\n2. **⚡ 두 번째 요청**: \"사과\"를 다시 만나면 저장된 결과를 즉시 반환\n3. **🔍 해시 기반**: 텍스트를 해시값으로 변환하여 키로 사용\n\n### 🏗️ 주요 구성 요소\n\n#### `from_bytes_store` 메소드의 핵심 매개변수:\n\n- **`underlying_embeddings`** 🤖: 실제 임베딩을 생성하는 모델 (예: OpenAIEmbeddings)\n- **`document_embedding_cache`** 💾: 임베딩을 저장할 저장소 (ByteStore)\n- **`namespace`** 🏷️: 캐시 충돌을 방지하기 위한 구분자\n\n### ⚠️ 중요한 주의사항\n\n**namespace 매개변수**를 반드시 설정하세요! 다른 임베딩 모델을 사용할 때 **캐시 충돌**을 방지할 수 있습니다.\n\n**예시**: `text-embedding-3-small`과 `text-embedding-ada-002`를 구분하여 저장"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5aed2b",
   "metadata": {},
   "outputs": [],
   "source": "# API KEY를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API KEY 정보로드\nload_dotenv(override=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a6341",
   "metadata": {},
   "outputs": [],
   "source": "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n# !pip install langchain-teddynote\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"LangChain-Tutorial\")"
  },
  {
   "cell_type": "markdown",
   "id": "bf94c21a",
   "metadata": {},
   "source": "## 2. LocalFileStore를 사용한 영구 캐싱 💾\n\n**LocalFileStore**는 임베딩 결과를 **컴퓨터 하드디스크에 파일로 저장**하는 방식입니다.\n\n### 🏪 실생활 비유: 동네 편의점 창고\n\n동네 편의점을 생각해보세요:\n- **📦 창고**: LocalFileStore (컴퓨터의 폴더)\n- **🏷️ 상품 바코드**: 텍스트 해시값 (임베딩을 찾는 키)\n- **📋 재고 목록**: 저장된 임베딩 데이터\n\n### ✅ LocalFileStore의 장점\n- **🔒 영구 보관**: 프로그램을 종료해도 캐시가 남아있음\n- **💰 비용 절약**: 한 번 계산한 임베딩은 계속 재사용\n- **⚡ 빠른 속도**: 두 번째부터는 즉시 로드\n\n### ⚠️ LocalFileStore의 단점  \n- **💾 저장 공간**: 하드디스크 용량을 차지\n- **🧹 관리 필요**: 오래된 캐시 파일 정리 필요\n\n이제 실제로 로컬 파일 시스템을 사용하여 임베딩을 저장하고 **FAISS 벡터 스토어**를 사용하여 검색하는 예제를 살펴보겠습니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d2d88",
   "metadata": {},
   "outputs": [],
   "source": "from langchain.storage import LocalFileStore\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.embeddings import CacheBackedEmbeddings\nfrom langchain_community.vectorstores.faiss import FAISS\n\n# OpenAI 임베딩을 사용하여 기본 임베딩 설정\nembedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# 로컬 파일 저장소 설정 - \"./cache/\" 폴더에 캐시 파일 저장\nstore = LocalFileStore(\"./cache/\")\n\n# 캐시를 지원하는 임베딩 생성\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(\n    underlying_embeddings=embedding,  # 실제 임베딩을 수행할 모델\n    document_embedding_cache=store,   # 캐시를 저장할 저장소\n    namespace=embedding.model,        # 모델별로 캐시를 구분하기 위한 네임스페이스\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99fa76",
   "metadata": {},
   "outputs": [],
   "source": "# 현재 캐시 저장소에 저장된 키들을 확인 (아직 아무것도 없음)\nlist(store.yield_keys())"
  },
  {
   "cell_type": "markdown",
   "id": "a873ccce",
   "metadata": {},
   "source": "### 📄 문서 처리 과정\n\n이제 **문서를 로드하고 처리하는 전체 과정**을 단계별로 진행해보겠습니다:\n\n1. **📖 문서 로드**: 텍스트 파일에서 내용 읽기\n2. **✂️ 청크 분할**: 긴 텍스트를 작은 단위로 나누기  \n3. **🔢 임베딩 생성**: 각 청크를 벡터로 변환 (캐싱 적용!)\n4. **🗃️ 벡터 저장소**: FAISS를 사용해 검색 가능한 형태로 저장"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e6873",
   "metadata": {},
   "outputs": [],
   "source": "from langchain.document_loaders import TextLoader\nfrom langchain_text_splitters import CharacterTextSplitter\n\n# 텍스트 파일에서 문서 로드\nraw_documents = TextLoader(\"./data/appendix-keywords.txt\").load()\n\n# 문자 단위로 텍스트 분할 설정 (1000자씩 나누고, 겹치는 부분은 0자)\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n\n# 로드된 문서를 설정된 크기로 분할\ndocuments = text_splitter.split_documents(raw_documents)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442c979",
   "metadata": {},
   "outputs": [],
   "source": "# 첫 번째 실행 - 모든 문서를 임베딩하고 FAISS 데이터베이스 생성 (시간 측정)\n%time db = FAISS.from_documents(documents, cached_embedder)"
  },
  {
   "cell_type": "markdown",
   "id": "487b6402",
   "metadata": {},
   "source": "### ⚡ 캐시 효과 확인하기\n\n이제 **캐싱의 진짜 위력**을 확인해볼 시간입니다! \n\n같은 문서로 벡터 저장소를 다시 생성하면, **이미 계산된 임베딩을 캐시에서 가져와서** 훨씬 더 빠르게 처리됩니다.\n\n### 🏃‍♂️ 속도 비교 실험\n- **첫 번째 실행**: OpenAI API 호출 + 임베딩 계산 + 캐시 저장\n- **두 번째 실행**: 캐시에서 바로 로드 ⚡\n\n결과적으로 **몇 초에서 몇 밀리초로** 처리 시간이 대폭 단축됩니다!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914f363",
   "metadata": {},
   "outputs": [],
   "source": "# 두 번째 실행 - 캐싱된 임베딩을 사용하여 훨씬 빠르게 FAISS 데이터베이스 생성\n%time db2 = FAISS.from_documents(documents, cached_embedder)"
  },
  {
   "cell_type": "markdown",
   "id": "3cf59fa6",
   "metadata": {},
   "source": "## 3. InMemoryByteStore를 사용한 임시 캐싱 🧠\n\n**InMemoryByteStore**는 **컴퓨터 메모리(RAM)에만 임시로 저장**하는 방식입니다.\n\n### 🧠 실생활 비유: 사람의 단기 기억\n\n사람의 기억을 생각해보세요:\n- **📝 단기 기억**: InMemoryByteStore (프로그램 실행 중에만 유지)\n- **📚 장기 기억**: LocalFileStore (파일로 영구 보관)\n- **😴 잠들면 잊어버림**: 프로그램 종료 시 모든 캐시 사라짐\n\n### ✅ InMemoryByteStore의 장점\n- **🚀 초고속**: 메모리 접근이 파일 접근보다 빠름\n- **🧹 자동 정리**: 프로그램 종료 시 자동으로 정리됨  \n- **💾 공간 절약**: 하드디스크 용량을 사용하지 않음\n\n### ⚠️ InMemoryByteStore의 단점\n- **⏰ 일시적**: 프로그램을 재시작하면 캐시가 모두 사라짐\n- **🧠 메모리 사용**: RAM을 많이 사용할 수 있음\n\n### 🎯 언제 사용할까?\n\n- **임시 작업**: 한 번의 실행 중에만 캐시가 필요한 경우\n- **테스팅**: 개발 중 빠른 테스트를 위해\n- **메모리 풍부**: RAM이 충분하고 디스크 사용을 피하고 싶을 때\n\n아래에서는 **비영구적인 InMemoryByteStore**를 사용하여 동일한 캐시된 임베딩 객체를 생성하는 예시를 보여줍니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58c94a",
   "metadata": {},
   "outputs": [],
   "source": "from langchain.embeddings import CacheBackedEmbeddings\nfrom langchain.storage import InMemoryByteStore\n\n# 메모리 내 바이트 저장소 생성 (프로그램 종료시 사라짐)\nstore = InMemoryByteStore()\n\n# 캐시 지원 임베딩 생성 (메모리 기반)\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(\n    embedding,                # 기본 임베딩 모델\n    store,                    # 메모리 내 저장소\n    namespace=embedding.model # 네임스페이스로 캐시 구분\n)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}