{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00f269d",
   "metadata": {},
   "source": "# 🧠 SemanticChunker: 의미 기반 텍스트 분할의 혁신\n\n## 📚 개요\n\n**SemanticChunker**는 단순히 글자 수나 문장 개수로 텍스트를 자르는 것이 아닌, **의미론적 유사성**을 기반으로 텍스트를 분할하는 혁신적인 도구입니다! 🎯\n\n### 🤔 기존 방식의 한계점\n\n기존의 텍스트 분할 방식들을 생각해보세요:\n\n- **📏 고정 길이 분할**: \"500글자마다 자르기\" → 문장이 중간에 끊어짐\n- **📄 문단 기반 분할**: \"문단마다 분할\" → 관련된 내용이 흩어짐\n- **🔢 문장 개수 분할**: \"3문장씩 묶기\" → 맥락과 상관없이 기계적 분할\n\n### 🎯 SemanticChunker의 똑똑한 접근법\n\n**SemanticChunker**는 마치 **숙련된 편집자**처럼 텍스트를 읽고 의미 있는 단위로 분할합니다:\n\n1. **🔍 문장 분석**: 각 문장을 임베딩으로 변환하여 의미 파악\n2. **📊 유사도 계산**: 인접한 문장들 간의 의미적 유사도 측정  \n3. **✂️ 스마트 분할**: 의미가 급격히 변하는 지점에서 분할\n4. **🧩 의미 단위 생성**: 비슷한 내용끼리 묶어서 응집성 높은 청크 생성\n\n### 🏆 왜 SemanticChunker인가?\n\n✅ **문맥 보존**: 관련된 내용이 함께 유지됨  \n✅ **검색 정확도 향상**: RAG 시스템에서 더 정확한 정보 검색  \n✅ **자연스러운 분할**: 의미의 흐름에 따른 자연스러운 구분  \n✅ **유연성**: 다양한 분할 기준 설정 가능\n\n### 📖 이 튜토리얼에서 배울 것들\n\n1. **🚀 기본 사용법** - SemanticChunker 생성과 실행\n2. **⚙️ Breakpoints 이해** - 분할 지점을 결정하는 다양한 방법들\n3. **📊 Percentile 방식** - 백분위수 기반 분할\n4. **📈 Standard Deviation 방식** - 표준편차 기반 분할  \n5. **📋 Interquartile 방식** - 사분위수 기반 분할\n\n### 🎨 실생활 비유로 이해하기\n\n**SemanticChunker**를 **도서관 사서**에 비유해보세요:\n\n```\n📚 전체 텍스트 (큰 책)\n    ↓\n👩‍🏫 SemanticChunker (숙련된 사서)\n    ↓\n📑 의미별 챕터 (관련 내용끼리 묶인 청크들)\n```\n\n숙련된 사서가 책의 내용을 읽고 주제별로 관련된 페이지들을 함께 묶어주는 것처럼, SemanticChunker도 의미적으로 연관된 문장들을 함께 묶어줍니다!\n\n**Reference**\n- [Greg Kamradt의 노트북](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)"
  },
  {
   "cell_type": "markdown",
   "id": "90b48cce",
   "metadata": {},
   "source": "---\n\n## 📄 샘플 데이터 준비\n\nSemanticChunker의 동작을 확인하기 위해 실제 텍스트 파일을 로드해봅시다! \n\n**실험 데이터**: `appendix-keywords.txt` 파일 - 다양한 키워드와 설명이 포함된 텍스트 데이터입니다. 이 파일을 통해 SemanticChunker가 어떻게 의미적으로 관련된 내용들을 함께 묶는지 확인할 수 있습니다! 🔍"
  },
  {
   "cell_type": "markdown",
   "id": "s0o4c9x9vq",
   "source": "---\n\n## 🛠️ 환경 설정\n\nSemanticChunker를 사용하기 위한 필수 설정들을 준비해봅시다!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1dw7neu0foq",
   "source": "# API KEY를 환경변수로 관리하기 위한 설정 파일\nfrom dotenv import load_dotenv\n\n# API KEY 정보로드\nload_dotenv(override=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "in4uee9ysvi",
   "source": "# LangSmith 추적을 설정합니다. https://smith.langchain.com\nfrom langchain_teddynote import logging\n\n# 프로젝트 이름을 입력합니다.\nlogging.langsmith(\"LangChain-Tutorial\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170dd43",
   "metadata": {},
   "outputs": [],
   "source": "# 샘플 텍스트 파일을 로드합니다\nwith open(\"./data/appendix-keywords.txt\") as f:\n    file = f.read()  # 파일의 전체 내용을 읽어서 file 변수에 저장\n\n# 파일 내용의 일부분을 확인해봅시다 (처음 350글자)\nprint(file[:350])"
  },
  {
   "cell_type": "markdown",
   "id": "e1817a14",
   "metadata": {},
   "source": "---\n\n## 🧠 SemanticChunker 생성\n\n**SemanticChunker**는 LangChain의 **실험적 기능** 중 하나로, 마치 **AI 편집자**처럼 텍스트를 의미론적으로 유사한 청크로 분할하는 역할을 합니다! \n\n### 🎯 핵심 동작 원리\n\nSemanticChunker는 다음과 같은 **3단계 프로세스**를 통해 작동합니다:\n\n1. **🔍 임베딩 변환**: 각 문장을 벡터 공간의 점으로 변환\n2. **📏 유사도 측정**: 인접한 문장들 간의 거리(의미적 차이) 계산\n3. **✂️ 분할 결정**: 거리가 특정 임계값을 초과하는 지점에서 분할\n\n### 🏗️ 왜 임베딩 모델이 필요한가?\n\n**임베딩 모델**은 SemanticChunker의 **핵심 엔진**입니다:\n\n- **🧠 의미 이해**: \"사과\"와 \"애플\"이 비슷한 의미임을 인식\n- **📊 정량화**: 추상적인 \"의미\"를 숫자로 표현  \n- **🎯 정확도**: 더 나은 임베딩 모델 = 더 정확한 분할\n\n### 💡 실생활 비유\n\n**SemanticChunker + 임베딩**을 **음성인식 시스템**에 비유해보세요:\n\n```\n🎤 음성 입력 (텍스트)\n    ↓\n🔄 음성인식 엔진 (임베딩 모델)  \n    ↓\n📝 의미 단위 인식 (SemanticChunker)\n    ↓  \n📚 자연스러운 문단들 (청크들)\n```\n\n음성인식이 소리를 의미 있는 단어로 변환하듯이, SemanticChunker는 텍스트를 의미 있는 단위로 변환합니다!"
  },
  {
   "cell_type": "markdown",
   "id": "ab33ae70",
   "metadata": {},
   "source": "### 🚀 기본 SemanticChunker 생성하기\n\n이제 **OpenAI의 임베딩 모델**을 사용하여 SemanticChunker를 만들어봅시다! \n\n**임베딩 모델**은 텍스트의 의미를 이해하는 **AI의 눈**과 같습니다. 같은 의미의 문장들은 벡터 공간에서 가깝게, 다른 의미의 문장들은 멀리 배치되어 자동으로 의미적 경계를 찾아줍니다! 🎯"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e3aae",
   "metadata": {},
   "outputs": [],
   "source": "# SemanticChunker와 임베딩 모델을 import합니다\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\n# OpenAI의 임베딩 모델을 사용하여 SemanticChunker를 생성합니다\n# text-embedding-3-small: 효율적이면서 정확한 임베딩 모델\ntext_splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
  },
  {
   "cell_type": "markdown",
   "id": "dab515b0",
   "metadata": {},
   "source": "---\n\n## ✂️ 텍스트 분할 실행\n\n이제 준비한 SemanticChunker로 실제 텍스트를 분할해봅시다! 🎯\n\n### 🔍 분할 과정 이해하기\n\nSemanticChunker가 텍스트를 분할할 때:\n\n1. **📝 문장 분리**: 전체 텍스트를 문장 단위로 나누기\n2. **🧠 임베딩 변환**: 각 문장을 벡터로 변환  \n3. **📊 유사도 분석**: 인접 문장들 간의 의미적 거리 계산\n4. **✂️ 분할점 결정**: 의미가 급변하는 지점에서 청크 분리\n5. **📚 청크 완성**: 의미적으로 응집된 텍스트 블록 생성"
  },
  {
   "cell_type": "markdown",
   "id": "b0c9b20b",
   "metadata": {},
   "source": "### 🎯 split_text() 메소드 사용하기\n\n`split_text()` 메소드는 텍스트 문자열을 입력받아서 **의미적으로 연관된 청크들의 리스트**를 반환합니다.\n\n**💡 Tip**: 결과는 **문자열 리스트**로 반환되며, 각 청크는 의미적으로 응집된 내용을 담고 있습니다!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5870d",
   "metadata": {},
   "outputs": [],
   "source": "# SemanticChunker를 사용하여 텍스트를 의미 기반으로 분할합니다\nchunks = text_splitter.split_text(file)"
  },
  {
   "cell_type": "markdown",
   "id": "14a777bc",
   "metadata": {},
   "source": "### 📋 분할 결과 확인하기\n\n첫 번째 청크를 확인해서 SemanticChunker가 어떻게 의미적으로 관련된 내용들을 하나로 묶었는지 살펴봅시다! 🔍"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec69bff",
   "metadata": {},
   "outputs": [],
   "source": "# 분할된 청크들 중 첫 번째 청크를 확인해봅시다\nprint(chunks[0])"
  },
  {
   "cell_type": "markdown",
   "id": "8f03b26b",
   "metadata": {},
   "source": "### 📄 create_documents() - 문서 객체로 변환\n\n`create_documents()` 함수를 사용하면 단순한 문자열이 아닌 **Document 객체**로 청크를 생성할 수 있습니다!\n\n#### 🔄 split_text() vs create_documents()\n\n- **split_text()**: 문자열 리스트 반환 → `[\"청크1\", \"청크2\", \"청크3\"]`\n- **create_documents()**: Document 객체 리스트 반환 → `[Document(page_content=\"청크1\"), ...]`\n\n**Document 객체**는 **메타데이터 저장**이 가능하여 더 풍부한 정보를 담을 수 있습니다! 📊"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadaf823",
   "metadata": {},
   "outputs": [],
   "source": "# create_documents()를 사용하여 Document 객체로 분할합니다\ndocs = text_splitter.create_documents([file])\n\n# 첫 번째 Document 객체의 page_content (실제 텍스트 내용)를 출력합니다\nprint(docs[0].page_content)"
  },
  {
   "cell_type": "markdown",
   "id": "1633cf8e",
   "metadata": {},
   "source": "---\n\n## 🎯 Breakpoints: 분할 지점 결정의 과학\n\n**Breakpoints**는 SemanticChunker의 **핵심 두뇌**입니다! 어디서 텍스트를 나눌지 결정하는 **똑똑한 판단 기준**을 의미합니다.\n\n### 🔍 Breakpoint 동작 원리\n\nSemanticChunker는 다음과 같은 과정으로 분할점을 찾습니다:\n\n1. **📊 유사도 계산**: 인접한 모든 문장 쌍의 의미적 거리 측정\n2. **📈 차이점 분석**: \"이 문장과 다음 문장이 얼마나 다른가?\" \n3. **⚖️ 임계값 적용**: 설정한 기준을 초과하는 지점을 분할점으로 결정\n4. **✂️ 분할 실행**: 의미가 급변하는 지점에서 청크 생성\n\n### 🎨 실생활 비유: 음악 DJ의 선곡\n\n**Breakpoint 설정**을 **DJ의 곡 전환**에 비유해보세요:\n\n#### 🎵 **보수적인 DJ** (높은 임계값)\n- **특징**: 비슷한 장르의 음악만 연결\n- **결과**: 적은 수의 긴 청크 (안전하지만 단조로움)\n\n#### 🎶 **모험적인 DJ** (낮은 임계값)  \n- **특징**: 작은 차이에도 민감하게 반응\n- **결과**: 많은 수의 짧은 청크 (다양하지만 파편화)\n\n#### 🎼 **균형잡힌 DJ** (적절한 임계값)\n- **특징**: 의미있는 전환점에서만 변경\n- **결과**: 적당한 크기의 응집성 있는 청크 (⭐ 최적!)\n\n### 📏 SemanticChunker가 제공하는 3가지 Breakpoint 방식\n\n#### 1️⃣ **Percentile (백분위수)** 📊\n- **기준**: \"상위 N% 차이점에서 분할\"\n- **예시**: 70% → 상위 30% 차이점에서만 분할\n- **장점**: 일관된 청크 개수 보장\n\n#### 2️⃣ **Standard Deviation (표준편차)** 📈  \n- **기준**: \"평균보다 N배 큰 차이점에서 분할\"\n- **예시**: 1.25 → 평균보다 1.25배 큰 차이에서 분할\n- **장점**: 통계적으로 의미있는 분할점 선택\n\n#### 3️⃣ **Interquartile (사분위수)** 📋\n- **기준**: \"중간값 기준으로 N배 벗어난 점에서 분할\"  \n- **예시**: 0.5 → 제3사분위수를 0.5배 초과시 분할\n- **장점**: 이상값에 덜 민감한 안정적 분할\n\n### 💡 어떤 방식을 선택할까?\n\n- **📊 Percentile**: 청크 개수를 예측 가능하게 조절하고 싶을 때\n- **📈 Standard Deviation**: 통계적으로 의미있는 변화점을 찾고 싶을 때  \n- **📋 Interquartile**: 안정적이고 균형잡힌 분할을 원할 때\n\n**참고 영상**: [Greg Kamradt의 Text Splitting 설명](https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580)"
  },
  {
   "cell_type": "markdown",
   "id": "cn613umkoz9",
   "source": "---\n\n## 📊 방법 1: Percentile (백분위수) 방식\n\n**Percentile 방식**은 마치 **시험 성적의 상위권**을 뽑는 것처럼, 가장 큰 차이를 보이는 **상위 N%** 지점에서만 분할하는 방식입니다!\n\n### 🎯 동작 원리\n\n1. **📈 모든 차이 계산**: 인접한 모든 문장 쌍의 의미적 거리를 계산\n2. **📊 순위 매기기**: 차이가 큰 순서대로 정렬  \n3. **✂️ 상위 N% 선택**: 설정한 백분위수에 해당하는 지점에서 분할\n\n### 🏆 백분위수별 특성\n\n- **90%**: 매우 보수적 → 큰 청크들, 적은 분할\n- **70%**: 균형잡힌 → 적절한 크기의 청크들 (⭐ 추천)\n- **50%**: 적극적 → 작은 청크들, 많은 분할\n\n**💡 실무 팁**: 70%부터 시작해서 결과를 보고 조정하는 것을 권장합니다!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bbd95",
   "metadata": {},
   "outputs": [],
   "source": "# Percentile 방식으로 SemanticChunker를 생성합니다\ntext_splitter = SemanticChunker(\n    # OpenAI의 text-embedding-3-small 모델을 사용합니다\n    OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n    # 분할 기준을 백분위수로 설정합니다\n    breakpoint_threshold_type=\"percentile\",\n    # 70%: 상위 30% 차이점에서만 분할 (균형잡힌 설정)\n    breakpoint_threshold_amount=70,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "59aa8318",
   "metadata": {},
   "source": "### 📋 Percentile 방식 분할 결과\n\n70% 백분위수 기준으로 분할한 결과를 확인해봅시다! 각 청크가 의미적으로 어떻게 묶여있는지 관찰해보세요. 🔍"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b3262",
   "metadata": {},
   "outputs": [],
   "source": "# Percentile 방식으로 분할된 문서들을 생성합니다\ndocs = text_splitter.create_documents([file])\n\n# 처음 5개 청크의 내용을 확인해봅시다\nfor i, doc in enumerate(docs[:5]):\n    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n    print(doc.page_content)  # 각 청크의 텍스트 내용을 출력\n    print(\"===\" * 20)  # 청크 구분선"
  },
  {
   "cell_type": "markdown",
   "id": "07e83f74",
   "metadata": {},
   "source": "### 📊 총 청크 개수 확인\n\nPercentile 70% 기준으로 총 몇 개의 청크가 생성되었는지 확인해봅시다!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0cbd0",
   "metadata": {},
   "outputs": [],
   "source": "# Percentile 방식으로 생성된 총 청크 개수를 출력합니다\nprint(f\"총 청크 개수: {len(docs)}개\")"
  },
  {
   "cell_type": "markdown",
   "id": "21c1c9e8",
   "metadata": {},
   "source": "---\n\n## 📈 방법 2: Standard Deviation (표준편차) 방식\n\n**Standard Deviation 방식**은 **통계학의 정석**을 따르는 방법입니다! 평균적인 차이보다 **N배 더 큰 차이**를 보이는 지점에서만 분할하는 과학적 접근법입니다.\n\n### 🧮 동작 원리\n\n1. **📊 평균 계산**: 모든 인접 문장 간 차이의 평균값 계산\n2. **📐 표준편차 계산**: 차이들이 평균에서 얼마나 흩어져 있는지 측정  \n3. **⚖️ 임계값 적용**: `평균 + (N × 표준편차)`보다 큰 차이에서 분할\n4. **✂️ 분할 실행**: 통계적으로 유의미한 변화점에서 청크 생성\n\n### 🎯 표준편차 배수별 특성\n\n- **2.0**: 매우 보수적 → 극단적 차이에서만 분할\n- **1.25**: 균형잡힌 → 적당히 의미있는 변화점에서 분할 (⭐ 추천)\n- **0.5**: 적극적 → 평균보다 조금만 크면 분할\n\n### 🔬 왜 표준편차 방식인가?\n\n✅ **통계적 신뢰성**: 수학적으로 검증된 방법  \n✅ **이상값 감지**: 진짜 의미있는 변화점만 포착  \n✅ **일관성**: 텍스트 특성에 관계없이 안정적 성능  \n✅ **해석 용이성**: \"평균보다 1.25배 큰 차이\" = 명확한 기준\n\n**💡 실무 팁**: 1.25부터 시작해서 결과를 보며 조정하세요!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8d823",
   "metadata": {},
   "outputs": [],
   "source": "# Standard Deviation 방식으로 SemanticChunker를 생성합니다\ntext_splitter = SemanticChunker(\n    # OpenAI의 text-embedding-3-small 모델을 사용합니다\n    OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n    # 분할 기준을 표준편차로 설정합니다\n    breakpoint_threshold_type=\"standard_deviation\",\n    # 1.25: 평균보다 1.25배 큰 차이에서 분할 (적절한 민감도)\n    breakpoint_threshold_amount=1.25,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "690db96c",
   "metadata": {},
   "source": "### 📊 Standard Deviation 방식 분할 결과\n\n표준편차 1.25배 기준으로 분할한 결과를 확인해봅시다! Percentile 방식과 어떤 차이가 있는지 비교해보세요. 📈"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764de39",
   "metadata": {},
   "outputs": [],
   "source": "# Standard Deviation 방식으로 문서를 분할합니다\ndocs = text_splitter.create_documents([file])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0743d8f6",
   "metadata": {},
   "outputs": [],
   "source": "# Standard Deviation 방식으로 분할된 문서들을 생성합니다\ndocs = text_splitter.create_documents([file])\n\n# 처음 5개 청크의 내용을 확인해봅시다\nfor i, doc in enumerate(docs[:5]):\n    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n    print(doc.page_content)  # 각 청크의 텍스트 내용을 출력\n    print(\"===\" * 20)  # 청크 구분선"
  },
  {
   "cell_type": "markdown",
   "id": "095170af",
   "metadata": {},
   "source": "### 📊 총 청크 개수 비교\n\nStandard Deviation 1.25배 기준으로 총 몇 개의 청크가 생성되었는지 확인해봅시다!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f46ad",
   "metadata": {},
   "outputs": [],
   "source": "# Standard Deviation 방식으로 생성된 총 청크 개수를 출력합니다\nprint(f\"총 청크 개수: {len(docs)}개\")"
  },
  {
   "cell_type": "markdown",
   "id": "c5b03d9b",
   "metadata": {},
   "source": "---\n\n## 📋 방법 3: Interquartile (사분위수) 방식\n\n**Interquartile 방식**은 **이상값에 강인한** 통계적 방법입니다! 전체 데이터의 **중간 50%** 영역을 기준으로 분할점을 결정하는 안정적인 접근법입니다.\n\n### 🎯 사분위수란?\n\n**사분위수**는 데이터를 **4등분**하는 지점들입니다:\n\n- **Q1 (1사분위수)**: 하위 25% 경계점\n- **Q2 (2사분위수)**: 중간값 (50% 경계점)  \n- **Q3 (3사분위수)**: 상위 25% 경계점\n- **IQR**: Q3 - Q1 (중간 50%의 범위)\n\n### 🧮 동작 원리\n\n1. **📊 IQR 계산**: 모든 인접 문장 간 차이의 사분위수 범위 계산\n2. **📐 임계값 설정**: `Q3 + (N × IQR)` 계산\n3. **⚖️ 분할 조건**: 이 임계값을 초과하는 차이에서 분할\n4. **✂️ 안정적 분할**: 극값에 덜 민감한 분할점 선택\n\n### 🎯 사분위수 배수별 특성\n\n- **1.5**: 통계학 표준 → 이상값 탐지의 황금비율 (⭐ 추천)\n- **0.5**: 민감함 → 작은 변화에도 반응\n- **2.0**: 보수적 → 매우 큰 변화에만 반응\n\n### 🛡️ 왜 Interquartile 방식인가?\n\n✅ **이상값 저항성**: 극단적 차이값에 덜 민감  \n✅ **안정성**: 다양한 텍스트 유형에서 일관된 성능  \n✅ **검증된 방법**: 통계학에서 널리 사용되는 표준 방식  \n✅ **균형성**: 너무 세분화되지도, 너무 뭉뜨어지지도 않음\n\n**💡 실무 팁**: 0.5부터 시작해서 안정적인 결과를 원하면 1.5로 조정하세요!"
  },
  {
   "cell_type": "markdown",
   "id": "fb408177",
   "metadata": {},
   "source": "### 🚀 Interquartile 방식 SemanticChunker 생성\n\n이제 사분위수 범위를 기준으로 하는 **가장 안정적인** SemanticChunker를 만들어봅시다!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f5fe8",
   "metadata": {},
   "outputs": [],
   "source": "# Interquartile 방식으로 SemanticChunker를 생성합니다\ntext_splitter = SemanticChunker(\n    # OpenAI의 text-embedding-3-small 모델을 사용합니다\n    OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n    # 분할 기준을 사분위수 범위로 설정합니다\n    breakpoint_threshold_type=\"interquartile\",\n    # 0.5: Q3 + (0.5 × IQR) 초과시 분할 (민감한 설정)\n    breakpoint_threshold_amount=0.5,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0d2d6",
   "metadata": {},
   "outputs": [],
   "source": "# Interquartile 방식으로 문서를 분할합니다\ndocs = text_splitter.create_documents([file])\n\n# 처음 5개 청크의 내용을 확인해봅시다\nfor i, doc in enumerate(docs[:5]):\n    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n    print(doc.page_content)  # 각 청크의 텍스트 내용을 출력\n    print(\"===\" * 20)  # 청크 구분선"
  },
  {
   "cell_type": "markdown",
   "id": "9d186bb7",
   "metadata": {},
   "source": "### 📊 최종 청크 개수 비교\n\nInterquartile 0.5배 기준으로 총 몇 개의 청크가 생성되었는지 확인하고, 이전 방식들과 비교해봅시다!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c693c11",
   "metadata": {},
   "outputs": [],
   "source": "# Interquartile 방식으로 생성된 총 청크 개수를 출력합니다\nprint(f\"총 청크 개수: {len(docs)}개\")"
  },
  {
   "cell_type": "markdown",
   "id": "9is76grsn29",
   "source": "---\n\n## 🎯 SemanticChunker 완전 정복: 종합 정리\n\n축하합니다! 🎉 SemanticChunker의 모든 기능을 성공적으로 학습하셨습니다!\n\n### 📊 세 가지 방식 비교 요약\n\n| 방식 | 특징 | 장점 | 추천 상황 |\n|------|------|------|-----------|\n| **📊 Percentile** | 상위 N% 차이점에서 분할 | 예측 가능한 청크 개수 | 청크 개수를 조절하고 싶을 때 |\n| **📈 Standard Deviation** | 평균보다 N배 큰 차이에서 분할 | 통계적 신뢰성 | 과학적이고 일관된 분할을 원할 때 |\n| **📋 Interquartile** | 사분위수 범위 기준 분할 | 이상값에 강인함 | 안정적이고 균형잡힌 분할을 원할 때 |\n\n### 💡 실무에서의 선택 가이드\n\n#### 🚀 **초보자라면?**\n- **Percentile 70%**부터 시작하세요!\n- 직관적이고 이해하기 쉽습니다.\n\n#### 🔬 **정확성이 중요하다면?**  \n- **Standard Deviation 1.25**를 선택하세요!\n- 통계적으로 검증된 방법입니다.\n\n#### 🛡️ **안정성이 우선이라면?**\n- **Interquartile 1.5**를 선택하세요!\n- 다양한 텍스트에서 일관된 성능을 보입니다.\n\n### 🎯 핵심 포인트 정리\n\n✅ **SemanticChunker는 의미 기반 분할의 혁신**  \n✅ **임베딩 모델이 핵심 엔진 역할**  \n✅ **세 가지 Breakpoint 방식 각각의 장점 존재**  \n✅ **상황에 맞는 방식 선택이 중요**  \n✅ **실험을 통한 최적값 탐색 필요**\n\n### 🚀 다음 단계\n\n이제 여러분만의 텍스트 데이터로 SemanticChunker를 실험해보세요! 다른 임베딩 모델이나 매개변수 조합도 시도해보며 최적의 분할 전략을 찾아보시기 바랍니다! 💪\n\n---\n\n**🎓 수고하셨습니다! SemanticChunker 마스터가 되신 것을 축하드립니다!** 🎉",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}