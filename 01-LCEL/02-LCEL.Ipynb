{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚗️ LCEL 핵심 개념 이해하기\n",
    "\n",
    "### 🔗 파이프라인(Pipeline)이란?\n",
    "\n",
    "**LCEL의 핵심은 파이프라인 연산자 `|`** 입니다. 이는 **Unix 파이프라인**에서 영감을 받은 개념으로, 하나의 출력이 다음 단계의 입력으로 자연스럽게 전달됩니다.\n",
    "\n",
    "#### 🏭 **공장 생산라인으로 이해하기**\n",
    "\n",
    "```\n",
    "📦 원재료 → 🔨 가공1 → ⚙️ 가공2 → 📋 검수 → ✅ 완제품\n",
    "```\n",
    "\n",
    "LCEL도 똑같은 방식으로 동작합니다:\n",
    "\n",
    "```python\n",
    "# 🔗 LCEL 체인 구성\n",
    "chain = prompt_template | model | output_parser\n",
    "#      ↑              ↑     ↑\n",
    "#   1단계: 프롬프트 생성 → 2단계: AI 처리 → 3단계: 결과 파싱\n",
    "```\n",
    "\n",
    "### 🎯 기본 체인의 3단계 구조\n",
    "\n",
    "#### 1️⃣ **PromptTemplate** - 📝 지시사항 준비\n",
    "- **역할**: 사용자 입력을 AI가 이해할 수 있는 형태로 변환\n",
    "- **입력**: 딕셔너리 형태의 변수들 (`{\"topic\": \"인공지능\"}`)\n",
    "- **출력**: 완성된 프롬프트 문자열\n",
    "\n",
    "#### 2️⃣ **Model** - 🤖 AI 처리 \n",
    "- **역할**: 프롬프트를 받아 AI가 답변 생성\n",
    "- **입력**: 포맷팅된 프롬프트 텍스트\n",
    "- **출력**: AIMessage 객체 (내용 + 메타데이터)\n",
    "\n",
    "#### 3️⃣ **OutputParser** - 🎁 결과 정리\n",
    "- **역할**: AI 응답을 사용하기 쉬운 형태로 변환  \n",
    "- **입력**: AIMessage 객체\n",
    "- **출력**: 순수 텍스트 또는 구조화된 데이터\n",
    "\n",
    "### 💡 파이프라인의 강력함\n",
    "\n",
    "```python\n",
    "# 각 단계를 개별적으로 실행하는 전통적 방식 ❌\n",
    "formatted_prompt = prompt_template.format(country=\"대한민국\")\n",
    "ai_response = model.invoke(formatted_prompt)  \n",
    "final_result = output_parser.parse(ai_response)\n",
    "\n",
    "# LCEL로 한 번에 실행하는 방식 ✅\n",
    "result = chain.invoke({\"country\": \"대한민국\"})\n",
    "```\n",
    "\n",
    "**한 줄로 전체 과정을 완료!** 이것이 LCEL의 매력입니다. 🚀\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 PromptTemplate 완전 정복\n",
    "\n",
    "**PromptTemplate**은 **동적 프롬프트**를 만들어주는 강력한 도구입니다. 마치 **편지 양식**에 이름만 바꿔서 여러 사람에게 보내는 것과 같습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LangChain-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 PromptTemplate의 구성 요소\n",
    "\n",
    "**PromptTemplate**은 **템플릿 엔진**의 역할을 하며, 사용자 입력을 받아 **동적으로 프롬프트를 생성**합니다.\n",
    "\n",
    "#### 📋 **핵심 구성 요소**\n",
    "\n",
    "1. **🎯 template**: 실제 프롬프트 내용이 담긴 문자열 템플릿\n",
    "   ```python\n",
    "   template = \"{country}의 수도는 어디인가요?\"\n",
    "   ```\n",
    "\n",
    "2. **🔑 input_variables**: 템플릿에서 사용할 변수들의 이름 목록\n",
    "   ```python\n",
    "   # 중괄호 {} 안의 변수명들이 input_variables가 됨\n",
    "   # 위 예시에서는 [\"country\"]가 자동으로 추출됨\n",
    "   ```\n",
    "\n",
    "#### 💡 **템플릿 변수 사용법**\n",
    "\n",
    "- **중괄호 `{}`** 안에 변수명 작성\n",
    "- **여러 변수** 사용 가능: `\"{name}님, {city}의 날씨는 어떤가요?\"`\n",
    "- **변수명은 영문자로 시작**, 숫자와 언더스코어 사용 가능\n",
    "\n",
    "#### 🚀 **from_template() 메서드의 편리함**\n",
    "\n",
    "```python\n",
    "# ✅ 간단한 방법 - from_template() 사용 (권장)\n",
    "prompt = PromptTemplate.from_template(\"{country}의 수도는 어디인가요?\")\n",
    "\n",
    "# ❌ 복잡한 방법 - 직접 생성 (비권장)  \n",
    "prompt = PromptTemplate(\n",
    "    template=\"{country}의 수도는 어디인가요?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**from_template()의 장점:**\n",
    "- **🔍 자동 변수 추출**: 중괄호 안의 변수를 자동으로 인식\n",
    "- **✨ 간결한 코드**: 한 줄로 템플릿 생성 완료\n",
    "- **🛡️ 오류 방지**: 변수명 오타나 누락 위험 최소화\n",
    "\n",
    "실제로 PromptTemplate을 만들어보겠습니다! 💻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 헬퍼 함수와 PromptTemplate 클래스 임포트\n",
    "from langchain_teddynote.messages import stream_response\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏗️ PromptTemplate 객체 생성하기\n",
    "\n",
    "`from_template()` 메서드를 사용하면 간단하게 PromptTemplate 객체를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동적 프롬프트 템플릿 정의 - {country} 부분이 변수로 대체됨\n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# from_template 메서드를 이용하여 PromptTemplate 객체 생성\n",
    "# 중괄호 안의 변수들이 자동으로 input_variables로 인식됨\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# 생성된 PromptTemplate 객체 확인\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 템플릿에 구체적인 값을 대입하여 완성된 프롬프트 생성\n",
    "prompt = prompt_template.format(country=\"대한민국\")\n",
    "print(f\"생성된 프롬프트: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 국가로 변경하여 프롬프트 생성 테스트\n",
    "prompt = prompt_template.format(country=\"미국\")\n",
    "print(f\"생성된 프롬프트: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOpenAI 모델 임포트 및 초기화\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI GPT 모델 객체 생성\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1\",  # 사용할 모델 지정\n",
    "    temperature=0.1,  # 창의성 조절 (낮을수록 일관된 답변)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔗 LCEL 체인 생성 - 파이프라인의 마법\n",
    "\n",
    "### 🎯 LCEL(LangChain Expression Language) 심화 이해\n",
    "\n",
    "![LCEL Pipeline](./images/lcel.png)\n",
    "\n",
    "**LCEL의 핵심은 `|` (파이프) 연산자**입니다. 이를 통해 여러 구성 요소를 **체인처럼 연결**하여 하나의 통합된 워크플로우를 만들 수 있습니다.\n",
    "\n",
    "### 🔄 파이프라인 연산자의 작동 원리\n",
    "\n",
    "```python\n",
    "chain = prompt_template | model | output_parser\n",
    "```\n",
    "\n",
    "#### 📊 데이터 흐름 과정\n",
    "\n",
    "1. **📥 입력**: `{\"topic\": \"인공지능\"}` (딕셔너리)\n",
    "2. **📝 1단계**: `prompt_template` → 완성된 프롬프트 텍스트\n",
    "3. **🤖 2단계**: `model` → AIMessage 객체 (AI 응답)  \n",
    "4. **🎁 3단계**: `output_parser` → 최종 텍스트 결과\n",
    "\n",
    "### 🛠️ Unix 파이프라인과의 유사점\n",
    "\n",
    "**Unix 명령어**와 개념이 매우 유사합니다:\n",
    "\n",
    "```bash\n",
    "# Unix 파이프라인 예시\n",
    "cat file.txt | grep \"keyword\" | wc -l\n",
    "```\n",
    "\n",
    "```python\n",
    "# LCEL 파이프라인 예시  \n",
    "chain = prompt | model | parser\n",
    "```\n",
    "\n",
    "**공통점:**\n",
    "- **⬅️ 왼쪽에서 오른쪽**으로 데이터 흐름\n",
    "- **🔗 각 단계의 출력**이 다음 단계의 입력이 됨\n",
    "- **🧩 모듈화**: 각 구성요소를 독립적으로 테스트하고 교체 가능\n",
    "\n",
    "### 💡 LCEL이 혁신적인 이유\n",
    "\n",
    "#### ✨ **자동 최적화**\n",
    "- **병렬 처리**: 가능한 부분은 동시에 실행\n",
    "- **메모리 효율성**: 중간 결과의 스마트한 관리\n",
    "- **스트리밍**: 실시간 결과 출력 지원\n",
    "\n",
    "#### 🔧 **개발자 친화적**\n",
    "- **직관적 문법**: 데이터 흐름을 한눈에 파악\n",
    "- **디버깅 용이**: 각 단계별 결과 추적 가능\n",
    "- **재사용성**: 구성요소를 다른 체인에서도 활용\n",
    "\n",
    "이제 실제로 간단한 체인을 만들어봅시다! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(\"{topic}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "# ChatOpenAI 모델 객체 생성\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0.1)\n",
    "\n",
    "# 기본 체인 구성 (출력 파서 없이) - 프롬프트와 모델만 연결\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ invoke() 메서드 - 체인 실행하기\n",
    "\n",
    "**invoke()** 는 LCEL 체인을 실행하는 가장 기본적인 메서드입니다.\n",
    "\n",
    "#### 📋 **사용법**\n",
    "- **입력 형태**: Python 딕셔너리 `{\"변수명\": \"값\"}`\n",
    "- **실행 방식**: 동기식 (결과가 나올 때까지 대기)\n",
    "- **반환값**: 체인의 최종 출력 (출력 파서에 따라 달라짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 실행을 위한 입력 딕셔너리 정의\n",
    "# 키는 템플릿의 변수명과 일치해야 함\n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL 체인 실행: 프롬프트 생성 → 모델 처리 → AIMessage 반환\n",
    "# invoke() 메서드로 전체 파이프라인을 한 번에 실행\n",
    "result = chain.invoke(input)\n",
    "\n",
    "# 결과 출력 (AIMessage 객체 형태로 반환됨)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌊 스트리밍 출력의 강력함\n",
    "\n",
    "**스트리밍**은 AI가 응답을 생성하는 **실시간 과정을 관찰**할 수 있게 해주는 기능입니다. 마치 사람이 말하는 것처럼 단어 하나씩 나타납니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 방식으로 체인 실행 - 실시간으로 응답 생성 과정 확인\n",
    "answer = chain.stream(input)\n",
    "\n",
    "# langchain_teddynote의 헬퍼 함수로 스트리밍 출력을 깔끔하게 표시\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎁 OutputParser - 결과를 원하는 형태로\n",
    "\n",
    "**OutputParser**는 AI의 복잡한 응답을 **사용하기 쉬운 형태로 변환**해주는 마지막 단계입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 출력 파서 임포트\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# StrOutputParser 객체 생성 - AIMessage에서 순수 텍스트만 추출\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔗 완전한 체인 구성하기\n",
    "\n",
    "이제 **3단계 파이프라인**을 완성해봅시다: **PromptTemplate → Model → OutputParser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전한 LCEL 체인 구성: 프롬프트 → 모델 → 출력 파서\n",
    "# 이제 결과가 AIMessage가 아닌 순수 문자열로 반환됨\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 체인으로 invoke 실행 - 이제 순수 문자열이 반환됨\n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\"}\n",
    "result = chain.invoke(input)\n",
    "\n",
    "# 결과 출력 (이제 문자열 형태로 깔끔하게 출력됨)\n",
    "print(\"=== 완성된 체인 결과 ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 체인으로 스트리밍 실행\n",
    "answer = chain.stream(input)\n",
    "\n",
    "print(\"=== 스트리밍 출력 ===\")\n",
    "# 실시간으로 문자열이 생성되는 과정을 관찰\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 실전 프로젝트: 영어 회화 튜터 AI 만들기\n",
    "\n",
    "이제 배운 내용을 활용해서 **실용적인 영어 학습 도우미**를 만들어봅시다! \n",
    "\n",
    "### 💡 프로젝트 개요\n",
    "\n",
    "- **목표**: 상황별 영어 회화 생성 + 한글 번역 제공\n",
    "- **특징**: 체계적인 포맷으로 학습 효과 극대화\n",
    "- **활용**: 다양한 상황에 맞는 영어 표현 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전문적인 영어 회화 튜터 프롬프트 템플릿 설계\n",
    "template = \"\"\"You are an experienced English conversation teacher with 10 years of expertise.\n",
    "Create practical English conversations for the given situation with Korean translations.\n",
    "Please follow the FORMAT exactly as shown below.\n",
    "\n",
    "#SITUATION:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- English Conversation:\n",
    "- Korean Translation:\n",
    "- Useful Expressions:\n",
    "- Cultural Notes (if applicable):\n",
    "\"\"\"\n",
    "\n",
    "# 개선된 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# ChatOpenAI 모델 객체 생성 (최신 모델 사용)\n",
    "model = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "\n",
    "# 문자열 출력 파서 생성\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 회화 튜터 체인 구성\n",
    "# 프롬프트 → 모델 → 출력 파서의 완전한 파이프라인\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 상황: 식당에서 음식 주문하기\n",
    "situation_1 = \"저는 식당에 가서 음식을 주문하고 싶어요\"\n",
    "\n",
    "print(\"🍽️ === 식당 주문 상황 ===\")\n",
    "print(chain.invoke({\"question\": situation_1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 번째 상황: 스트리밍으로 실시간 학습 경험\n",
    "situation_2 = \"미국에서 피자 주문\"\n",
    "\n",
    "print(\"🍕 === 미국 피자 주문 상황 (스트리밍) ===\")\n",
    "# 스트리밍으로 영어 회화가 실시간으로 생성되는 과정 관찰\n",
    "answer = chain.stream({\"question\": situation_2})\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 question 을 '미국에서 피자 주문'으로 설정하여 실행합니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"미국에서 피자 주문\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
