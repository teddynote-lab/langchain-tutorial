{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "z52x0tivhe",
            "metadata": {},
            "source": [
                "# OpenAI ChatGPT API\n",
                "\n",
                "## 개요\n",
                "\n",
                "이 튜토리얼은 OpenAI 호환 Chat API 를 LangChain 과 함께 사용하는 방법을 설명한다. 실행 환경 구성, 기본 호출, 응답 구조, 스트리밍, 멀티모달, 프롬프트 설계를 단계별로 다룬다. 본 문서에서는 OpenRouter 를 통해 모델을 호출한다.\n",
                "\n",
                "### 학습 목표\n",
                "\n",
                "- **LangChain** 과 **LCEL(LangChain Expression Language)** 기본 개념 이해\n",
                "- **ChatOpenAI** 클래스 초기화와 호출 흐름 이해\n",
                "- **모델/파라미터 선택 기준** 이해\n",
                "- **스트리밍, LogProb, 멀티모달** 사용법 습득\n",
                "- **프롬프트 설계** 기본 원칙 정리\n",
                "\n",
                "### 목차\n",
                "\n",
                "1. 환경 설정과 LangSmith 연동\n",
                "2. LangChain 과 LCEL 핵심 개념\n",
                "3. ChatOpenAI 기본 사용법\n",
                "4. 응답 구조 이해\n",
                "5. 고급 기능: LogProb, 스트리밍, 멀티모달\n",
                "6. 프롬프트 엔지니어링\n",
                "---\n",
                "\n",
                "## 환경 설정"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3bf28f9e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# API KEY 정보로드\n",
                "load_dotenv(override=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "93ac45ce",
            "metadata": {},
            "outputs": [],
            "source": [
                "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
                "# .env 파일에 LANGCHAIN_API_KEY를 입력합니다.\n",
                "from langchain_teddynote import logging\n",
                "\n",
                "# 프로젝트 이름을 입력합니다.\n",
                "logging.langsmith(\"LangChain-Tutorial\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4xwrsfd5d6",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## LangChain 과 LCEL 기본 개념\n",
                "\n",
                "### LangChain 이란?\n",
                "\n",
                "**LangChain** 은 Large Language Model(LLM) 기반 애플리케이션 개발을 위한 **통합 프레임워크** 이다.\n",
                "\n",
                "#### LangChain 의 핵심 가치\n",
                "\n",
                "- **표준화**: 다양한 LLM 을 동일한 인터페이스로 사용\n",
                "- **구성요소화**: 프롬프트, 모델, 파서 등 모듈로 재사용\n",
                "- **확장성**: 필요에 따라 컴포넌트 교체/추가 용이\n",
                "- **추적성**: LangSmith 등과 연동하여 실행을 기록\n",
                "\n",
                "### LCEL (LangChain Expression Language)\n",
                "\n",
                "**LCEL** 은 LangChain 컴포넌트들을 **체인처럼 연결** 하는 표현식이다.\n",
                "\n",
                "#### LCEL 의 특징\n",
                "\n",
                "![](images/lcel.png)\n",
                "\n",
                "```python\n",
                "# LCEL 방식 - 간결하고 명확\n",
                "chain = prompt | llm | output_parser\n",
                "result = chain.invoke({\"input\": input_text})\n",
                "```\n",
                "\n",
                "#### LCEL 의 장점\n",
                "\n",
                "- **가독성**: 처리 흐름을 한눈에 파악 가능\n",
                "- **최적화 실행**: 내부적으로 병렬화 및 스트리밍 지원\n",
                "- **견고성**: 예외 처리와 재시도 전략을 손쉽게 적용\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2550920c-09d8-48b3-be2f-b36362c37989",
            "metadata": {},
            "source": [
                "## ChatOpenAI - OpenAI 호환 인터페이스\n",
                "\n",
                "**ChatOpenAI** 는 OpenAI 호환 Chat API 를 LangChain 에서 사용할 수 있도록 제공하는 **통합 인터페이스** 이다. 본 튜토리얼에서는 OpenRouter 를 사용한다.\n",
                "\n",
                "### 주요 설정 옵션\n",
                "\n",
                "| 파라미터 | 목적 | 권장값(예시) |\n",
                "| :-- | :-- | :-- |\n",
                "| temperature | 출력 다양성 조절 | 0.1 |\n",
                "| model | 사용할 모델 | openai/gpt-4.1 |\n",
                "| api_key | 인증 키 | os.getenv('OPENROUTER_API_KEY') |\n",
                "| base_url | API 엔드포인트 | os.getenv('OPENROUTER_BASE_URL') |\n",
                "| max_tokens | 최대 출력 길이 | 필요 시 지정 |\n",
                "\n",
                "#### temperature (창의성 조절)\n",
                "```python\n",
                "temperature=0.1  # 일관되고 정확한 답변\n",
                "temperature=0.7  # 창의적이고 다양한 답변\n",
                "```\n",
                "- **범위**: 0.0 ~ 2.0\n",
                "- **낮은 값 (0.0~0.3)**: 정확하고 일관된 답변\n",
                "- **중간 값 (0.4~0.6)**: 일반적인 대화에 적합\n",
                "- **높은 값 (0.7~0.9)**: 다양한 표현 생성\n",
                "\n",
                "#### max_tokens (최대 출력 길이)\n",
                "```python\n",
                "max_tokens=2048\n",
                "```\n",
                "- **의미**: 생성할 토큰의 상한\n",
                "- **참고**: 과도한 설정은 비용에 영향을 준다\n",
                "\n",
                "#### model (사용할 모델)\n",
                "- **GPT-4.1**: 고성능 모델, 복잡한 추론과 멀티모달 지원\n",
                "- **GPT-4.1-mini**: 성능과 비용의 균형\n",
                "- **GPT-4.1-nano**: 경량 작업에 적합\n",
                "\n",
                "### OpenAI 모델 비교표\n",
                "\n",
                "| 모델 계열       | 모델명 (API Name) | 입력       | 컨텍스트 윈도우  | 최대 출력 토큰 | 지식 마감일 (Cutoff) | 가격 (1M토큰당)                  |\n",
                "| :---------- | :------------- | :------- | :-------- | :------- | :-------------- | :------------------------------ |\n",
                "| **GPT-5**   | `gpt-5`        | 텍스트, 이미지 | 400,000   | 128,000  | 2024년 9월 30일    | **입력:** $1.25<br>**출력:** $10.00 |\n",
                "|             | `gpt-5-mini`   | 텍스트, 이미지 | 400,000   | 128,000  | 2024년 5월 31일    | **입력:** $0.25<br>**출력:** $2.00  |\n",
                "|             | `gpt-5-nano`   | 텍스트, 이미지 | 400,000   | 128,000  | 2024년 5월 31일    | **입력:** $0.05<br>**출력:** $0.40  |\n",
                "| **GPT-4.1** | `gpt-4.1`      | 텍스트, 이미지 | 1,047,576 | 32,768   | 2024년 6월 1일     | **입력:** $2.00<br>**출력:** $8.00  |\n",
                "|             | `gpt-4.1-mini` | 텍스트, 이미지 | 1,047,576 | 32,768   | 2024년 6월 1일     | **입력:** $0.40<br>**출력:** $1.60  |\n",
                "|             | `gpt-4.1-nano` | 텍스트, 이미지 | 1,047,576 | 32,768   | 2024년 6월 1일     | **입력:** $0.10<br>**출력:** $0.40  |\n",
                "| **GPT-4o**  | `gpt-4o`       | 텍스트, 이미지 | 128,000   | 16,384   | 2023년 10월 1일    | **입력:** $2.50<br>**출력:** $10.00 |\n",
                "|             | `gpt-4o-mini`  | 텍스트, 이미지 | 128,000   | 16,384   | 2023년 10월 1일    | **입력:** $0.15<br>**출력:** $0.60  |\n",
                "\n",
                "![OpenAI Models Comparison](./images/gpt-models3-202508.png)\n",
                "\n",
                "### 모델 선택 가이드\n",
                "\n",
                "- **정확성 우선**: `gpt-4.1`\n",
                "- **균형 선택**: `gpt-4.1-mini`\n",
                "- **비용 절약**: `gpt-4.1-nano`\n",
                "\n",
                "> **참고 링크**: [OpenAI 공식 모델 문서](https://platform.openai.com/docs/models)\n",
                "\n",
                "### 기본 사용법\n",
                "\n",
                "이제 ChatOpenAI 를 사용해 기본 호출을 수행한다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5fc161c2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[답변]: content='대한민국의 수도는 서울특별시입니다.  \\n참고로 많은 중앙행정기관이 세종특별자치시로 이전해 행정중심복합도시 역할을 하고 있지만, 공식 수도는 서울입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 184, 'prompt_tokens': 15, 'total_tokens': 199, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'openai/gpt-5', 'system_fingerprint': None, 'id': 'gen-1760283331-L9ykLUyXYpGj9po60szW', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--3e16b21b-da68-4b75-88bb-5f7561fc1698-0' usage_metadata={'input_tokens': 15, 'output_tokens': 184, 'total_tokens': 199, 'input_token_details': {}, 'output_token_details': {}}\n"
                    ]
                }
            ],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "import os\n",
                "\n",
                "# ChatOpenAI 객체 생성\n",
                "llm = ChatOpenAI(\n",
                "    temperature=0.1,  # 창의성\n",
                "    model=\"openai/gpt-4.1\",  # 사용할 모델명\n",
                "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n",
                "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter API URL\n",
                ")\n",
                "\n",
                "# 사용자 질의내용 정의\n",
                "question = \"대한민국의 수도는 어디인가요?\"\n",
                "\n",
                "# LLM에 질의하고 결과 출력\n",
                "print(f\"[답변]: {llm.invoke(question)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2ef2647",
            "metadata": {},
            "source": [
                "### AIMessage 객체\n",
                "\n",
                "ChatOpenAI 의 응답은 단순 텍스트가 아닌 **AIMessage 객체** 로 반환된다. 객체에는 본문과 메타데이터가 포함된다.\n",
                "\n",
                "#### AIMessage 의 구성 요소\n",
                "\n",
                "- **content**: 실제 AI 가 생성한 텍스트 답변\n",
                "- **response_metadata**: 토큰 사용량, 모델 정보, 처리 시간 등 메타데이터\n",
                "- **기타 정보**: 메시지 ID, 타입 등"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2af58a9a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 사용자 질의내용 정의\n",
                "question = \"대한민국의 수도는 어디인가요?\"\n",
                "\n",
                "# LLM에 질의하고 응답 객체를 변수에 저장\n",
                "response = llm.invoke(question)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24ecdeb9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 전체 응답 객체 확인 (AIMessage 형태)\n",
                "response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ffd49c11",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 응답 내용만 텍스트로 추출\n",
                "response.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4df69214",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 응답 메타데이터 확인 (토큰 사용량, 모델 정보 등)\n",
                "response.response_metadata"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "74c4a51a",
            "metadata": {},
            "source": [
                "### LogProb - 토큰 선택 확률 정보\n",
                "\n",
                "**LogProb(로그 확률)** 은 모델이 각 토큰을 선택할 때의 상대적 확률 정보를 제공한다. 답변의 신뢰도 판단 및 품질 관리에 활용할 수 있다.\n",
                "\n",
                "#### 활용 예\n",
                "\n",
                "- **답변 신뢰도 평가**: 낮은 확률 구간을 감지해 후속 확인 수행\n",
                "- **품질 관리**: 불확실한 응답을 필터링하거나 재생성\n",
                "- **모델 분석**: 토큰 수준의 선택 경향 파악\n",
                "- **후처리**: 확률 기준으로 후보 답변 비교\n",
                "\n",
                "#### 해석 방법 (경험칙)\n",
                "\n",
                "- **높은 확률 (-0.1 ~ 0.0)**: 높은 확신\n",
                "- **중간 확률 (-2.0 ~ -0.1)**: 보통 수준의 확신\n",
                "- **낮은 확률 (-5.0 이하)**: 낮은 확신"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fe733438",
            "metadata": {},
            "outputs": [],
            "source": [
                "# LogProb 기능이 활성화된 ChatOpenAI 객체 생성\n",
                "import os\n",
                "\n",
                "llm_with_logprob = ChatOpenAI(\n",
                "    temperature=0.1,  # 창의성 (0.0 ~ 2.0)\n",
                "    max_tokens=2048,  # 최대 출력 토큰 수\n",
                "    model=\"openai/gpt-4.1\",  # 사용할 모델명 (OpenRouter)\n",
                "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n",
                "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter API URL\n",
                ").bind(\n",
                "    logprobs=True,\n",
                "    top_logprobs=1,\n",
                ")  # 토큰별 확률 정보 활성화"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8ae2d627",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 사용자 질의내용 정의\n",
                "question = \"대한민국의 수도는 어디인가요?\"\n",
                "\n",
                "# LogProb가 활성화된 LLM으로 질의\n",
                "response = llm_with_logprob.invoke(question)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6b0b9f7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# LogProb 정보가 포함된 메타데이터 확인\n",
                "response_metadata = response.response_metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "37791993",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "for token in response_metadata[\"logprobs\"][\"content\"]:\n",
                "    token_str = token[\"token\"].strip()\n",
                "    logprob = float(token[\"logprob\"])\n",
                "    print(f\"{token_str}\\t\\t {np.round(np.exp(logprob)*100, 2)}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f8aec3e6",
            "metadata": {},
            "source": [
                "### 스트리밍 출력\n",
                "\n",
                "스트리밍은 모델이 생성하는 토큰을 순차적으로 전송해, 응답을 실시간으로 확인할 수 있도록 한다. 긴 답변도 대기 시간을 줄여 빠르게 피드백을 받을 수 있다.\n",
                "\n",
                "#### 장점\n",
                "\n",
                "- **지연 감소**: 부분 결과를 즉시 확인\n",
                "- **과정 노출**: 생성 진행 상황 파악\n",
                "- **UX 향상**: 대화형 애플리케이션에 적합\n",
                "\n",
                "#### 활용 예\n",
                "\n",
                "- **긴 문서 작성 미리보기**\n",
                "- **실시간 챗봇 응답**\n",
                "- **데모/발표 환경**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1bbc5d03",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 스트리밍 방식으로 LLM에 질의\n",
                "# 실시간으로 토큰이 생성되는 과정을 확인할 수 있음\n",
                "answer = llm.stream(\"대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "87a90e66",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 스트리밍 응답을 실시간으로 출력\n",
                "# 각 토큰이 생성될 때마다 즉시 화면에 표시됨\n",
                "for token in answer:\n",
                "    print(token.content, end=\"\", flush=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c6f079b9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_teddynote.messages import stream_response\n",
                "\n",
                "# 스트리밍 방식으로 LLM에 질의\n",
                "answer = llm.stream(\"대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!\")\n",
                "\n",
                "# langchain_teddynote의 stream_response 함수로 깔끔하게 출력\n",
                "stream_response(answer)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e95ce8b",
            "metadata": {},
            "source": [
                "## 멀티모달 AI - 이미지를 읽는 인공지능\n",
                "\n",
                "**멀티모달(Multimodal)** 은 여러 종류의 데이터를 동시에 처리하는 기술이다. **텍스트, 이미지, 오디오, 비디오** 등 복수 입력을 이해한다.\n",
                "\n",
                "### 처리 가능한 데이터 타입\n",
                "\n",
                "- **텍스트**: 문서, 이메일, 웹페이지 등\n",
                "- **이미지**: 사진, 차트, 표, 스크린샷 등\n",
                "- **오디오**: 음성, 음악, 효과음 등\n",
                "- **비디오**: 동영상, 애니메이션 등\n",
                "\n",
                "### GPT-4.1 의 비전(Vision) 기능\n",
                "\n",
                "**GPT-4.1** 은 강력한 **이미지 인식 능력** 을 갖춘 멀티모달 모델이다.\n",
                "\n",
                "#### 이미지 분석 작업 예\n",
                "\n",
                "- **차트/그래프 해석**: 데이터 시각화 분석\n",
                "- **문서 OCR**: 이미지 속 텍스트 추출 및 해석\n",
                "- **장면 설명**: 사진 속 상황과 객체 인식\n",
                "- **표/양식 처리**: 복잡한 테이블 데이터 이해\n",
                "- **시각 자료 분석**: 그림, 디자인 요소 해석\n",
                "\n",
                "#### 비즈니스 활용 예\n",
                "\n",
                "- **재무제표 분석**: 복잡한 회계 자료 자동 해석\n",
                "- **의료 영상 검토**: X-ray, MRI 이미지 보조 분석\n",
                "- **건축 도면 검토**: 설계도 및 시공 현황 파악\n",
                "- **제품 관리**: 상품 사진을 통한 품질 검사"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a859058d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_teddynote.models import MultiModal\n",
                "from langchain_teddynote.messages import stream_response\n",
                "\n",
                "# 기본 ChatOpenAI 객체 생성\n",
                "import os\n",
                "\n",
                "llm = ChatOpenAI(\n",
                "    temperature=0.1,  # 창의성\n",
                "    model=\"openai/gpt-4.1\",  # 이미지 인식이 가능한 모델 (OpenRouter)\n",
                "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),  # OpenRouter API 키\n",
                "    base_url=os.getenv(\"OPENROUTER_BASE_URL\"),  # OpenRouter API URL\n",
                ")\n",
                "\n",
                "# 멀티모달(이미지 + 텍스트 처리) 객체 생성\n",
                "multimodal_llm = MultiModal(llm)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0c16ef3c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 웹상의 이미지 URL 정의\n",
                "IMAGE_URL = \"https://t3.ftcdn.net/jpg/03/77/33/96/360_F_377339633_Rtv9I77sSmSNcev8bEcnVxTHrXB4nRJ5.jpg\"\n",
                "\n",
                "# 웹 이미지를 직접 분석하여 스트리밍 응답 생성\n",
                "answer = multimodal_llm.stream(IMAGE_URL)\n",
                "\n",
                "# 실시간으로 이미지 분석 결과 출력\n",
                "stream_response(answer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "006ec2bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 로컬 저장된 이미지 파일 경로 정의\n",
                "IMAGE_PATH_FROM_FILE = \"./images/sample-image.png\"\n",
                "\n",
                "# 로컬 이미지 파일을 분석하여 스트리밍 응답 생성\n",
                "answer = multimodal_llm.stream(IMAGE_PATH_FROM_FILE)\n",
                "\n",
                "# 실시간으로 이미지 분석 결과 출력\n",
                "stream_response(answer)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a7b5fc02",
            "metadata": {},
            "source": [
                "## 프롬프트 엔지니어링 - 역할과 지시 설계\n",
                "\n",
                "**프롬프트 엔지니어링** 은 AI 가 원하는 방식으로 동작하도록 **정확한 지시사항을 설계** 하는 작업이다.\n",
                "\n",
                "### System Prompt vs User Prompt\n",
                "\n",
                "#### **System Prompt** - AI 의 정체성과 역할 정의\n",
                "```python\n",
                "system_prompt = \"You are a professional financial analyst...\"\n",
                "```\n",
                "- **목적**: AI 가 **누구인지, 어떤 전문성을 가질지** 정의\n",
                "- **특징**: 대화 세션 동안 유지되는 **기본 설정**\n",
                "- **예시**: \"친절한 고객 서비스 직원\", \"전문 의료진\" 등\n",
                "\n",
                "#### **User Prompt** - 구체적인 작업 지시\n",
                "```python\n",
                "user_prompt = \"Please analyze the financial data and provide insights...\"\n",
                "```\n",
                "- **목적**: **무엇을 수행할지** 구체적으로 지시\n",
                "- **특징**: 요청마다 달라지는 **가변 지시사항**\n",
                "- **예시**: \"요약\", \"비교\", \"대안 제시\"\n",
                "\n",
                "### 효과적인 프롬프트 작성 원칙\n",
                "\n",
                "#### 1. **명확성 (Clarity)**\n",
                "```python\n",
                "# 모호한 지시\n",
                "\"재무제표를 봐주세요\"\n",
                "\n",
                "# 명확한 지시\n",
                "\"재무제표의 수익성 지표를 분석하고 3가지 핵심 인사이트를 제시해주세요\"\n",
                "```\n",
                "\n",
                "#### 2. **구체성 (Specificity)**\n",
                "```python\n",
                "# 추상적 요청\n",
                "\"도움을 주세요\"\n",
                "\n",
                "# 구체적 요청\n",
                "\"매출 증감 원인을 분석하고, 향후 3개월 예측과 개선 방안을 제시해주세요\"\n",
                "```\n",
                "\n",
                "#### 3. **맥락 제공 (Context)**\n",
                "```python\n",
                "# 풍부한 맥락\n",
                "\"당신은 10년 경력의 재무 분석 전문가입니다. 중소기업 CEO를 대상으로 복잡한 재무 용어는 쉽게 설명해주세요.\"\n",
                "```\n",
                "\n",
                "### 기대 효과\n",
                "\n",
                "- **정확도 향상**: 목표에 맞는 응답 유도\n",
                "- **일관성 확보**: 반복 작업 품질 유지\n",
                "- **효율 개선**: 불필요한 재질문 감소\n",
                "- **맞춤화 용이**: 도메인/톤 조정\n",
                "\n",
                "이제 재무제표 분석에 특화된 프롬프트를 적용한다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4be092af",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 시스템 프롬프트: AI의 역할과 행동 방식을 정의\n",
                "system_prompt = \"\"\"You are a professional financial AI assistant specialized in analyzing financial statements and tables.\n",
                "Your mission is to interpret given tabular financial data and provide insightful, interesting findings in a friendly and helpful manner.\n",
                "Focus on key metrics, trends, and notable patterns that would be valuable for business analysis.\n",
                "\n",
                "[IMPORTANT]\n",
                "- 한글로 답변해 주세요.\n",
                "\"\"\"\n",
                "\n",
                "# 사용자 프롬프트: 구체적인 작업 지시사항\n",
                "user_prompt = \"\"\"Please analyze the financial statement provided in the image.\n",
                "Identify and summarize the most interesting and important findings, including key financial metrics, trends, and insights that would be valuable for business decision-making.\"\"\"\n",
                "\n",
                "# 커스텀 프롬프트가 적용된 멀티모달 객체 생성\n",
                "multimodal_llm_with_prompt = MultiModal(\n",
                "    llm,\n",
                "    system_prompt=system_prompt,  # 시스템 역할 정의\n",
                "    user_prompt=user_prompt,  # 사용자 요청 정의\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "51735d05",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 분석할 재무제표 이미지 URL\n",
                "IMAGE_PATH_FROM_FILE = \"https://storage.googleapis.com/static.fastcampus.co.kr/prod/uploads/202212/080345-661/kwon-01.png\"\n",
                "\n",
                "# 커스텀 프롬프트가 적용된 멀티모달 LLM으로 재무제표 분석\n",
                "answer = multimodal_llm_with_prompt.stream(IMAGE_PATH_FROM_FILE)\n",
                "\n",
                "# 재무제표 분석 결과를 실시간으로 출력\n",
                "stream_response(answer)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
