import os
import uuid
from typing import List

import streamlit as st

# í™˜ê²½ ì„¤ì •
from dotenv import load_dotenv
from langchain.embeddings import CacheBackedEmbeddings

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.storage import LocalFileStore
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_core.messages.chat import ChatMessage
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.tools.retriever import create_retriever_tool
from langchain_experimental.tools import PythonREPLTool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_tavily import TavilySearch
from langchain_teddynote import logging
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
load_dotenv(override=True)

# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com
logging.langsmith("LangChain-Tutorial")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")
# ë²¡í„° ì„ë² ë”© ì €ì¥ í´ë”
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

# Streamlit ì•± ì œëª© ì„¤ì •
st.title("ReAct Agent ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "agent" not in st.session_state:
    st.session_state["agent"] = None
if "thread_id" not in st.session_state:
    st.session_state["thread_id"] = str(uuid.uuid4())
if "tools" not in st.session_state:
    st.session_state["tools"] = []
if "memory" not in st.session_state:
    # ì „ì—­ ë©”ëª¨ë¦¬ ì¸ìŠ¤í„´ìŠ¤ - í•œ ë²ˆë§Œ ìƒì„±ë˜ê³  ê³„ì† ìœ ì§€ë¨
    st.session_state["memory"] = MemorySaver()
if "current_tool_config" not in st.session_state:
    # ë„êµ¬ ì„¤ì • ë³€ê²½ ê°ì§€ë¥¼ ìœ„í•œ ìƒíƒœ
    st.session_state["current_tool_config"] = None
if "tavily_topic" not in st.session_state:
    st.session_state["tavily_topic"] = "general"
if "tavily_max_results" not in st.session_state:
    st.session_state["tavily_max_results"] = 3
if "tavily_include_domains" not in st.session_state:
    st.session_state["tavily_include_domains"] = ""
if "tavily_time_range" not in st.session_state:
    st.session_state["tavily_time_range"] = None
if "custom_prompt" not in st.session_state:
    st.session_state["custom_prompt"] = (
        "ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‘ë‹µí•˜ì„¸ìš”.\në¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në‹µë³€ì€ ì¹œê·¼ê° ìˆëŠ” ì–´ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”."
    )


def create_web_search_tool(
    topic: str = "general",
    max_results: int = 3,
    include_domains: str = "",
    time_range: str = None,
) -> TavilySearch:
    """ì›¹ ê²€ìƒ‰ ë„êµ¬ ìƒì„±"""
    # include_domainsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ì²˜ë¦¬)
    include_domains_list = []
    if include_domains and include_domains.strip():
        include_domains_list = [
            domain.strip() for domain in include_domains.split(",") if domain.strip()
        ]

    # TavilySearch ë§¤ê°œë³€ìˆ˜ ì„¤ì •
    search_params = {
        "topic": topic,
        "max_results": max_results,
        "include_answer": False,
        "include_raw_content": False,
        "include_images": False,
        "format_output": False,
        "include_domains": include_domains_list,
    }

    # time_rangeê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€
    if time_range and time_range != "None":
        search_params["time_range"] = time_range

    web_search = TavilySearch(**search_params)
    web_search.name = "web_search"
    web_search.description = (
        "Use this tool to search on the web for current information, news, and general topics. "
        "Perfect for finding real-time data, latest news, or any information not in your training data."
    )
    return web_search


def create_python_repl_tool() -> PythonREPLTool:
    """Python REPL ì½”ë“œ ì‹¤í–‰ ë„êµ¬ ìƒì„±"""
    python_tool = PythonREPLTool()
    python_tool.name = "python_repl"
    python_tool.description = (
        "A Python shell that can execute Python code and return results. "
        "Use this for calculations, data analysis, generating charts, and any computational tasks. "
        "Remember to use print() to see output results."
    )
    return python_tool


def create_pdf_retriever_tool(uploaded_file) -> object:
    """PDF ë¦¬íŠ¸ë¦¬ë²„ ë„êµ¬ ìƒì„±"""
    if uploaded_file is None:
        return None

    # PDF íŒŒì¼ ì €ì¥
    file_content = uploaded_file.read()
    file_path = f"./.cache/files/{uploaded_file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    # PDF ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
    loader = PDFPlumberLoader(file_path)
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_docs = text_splitter.split_documents(documents)

    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ë¡œì»¬ íŒŒì¼ ì €ì¥ì†Œ ì„¤ì • - "./cache/" í´ë”ì— ìºì‹œ íŒŒì¼ ì €ì¥
    store = LocalFileStore(".cache/embeddings")

    # ìºì‹œë¥¼ ì§€ì›í•˜ëŠ” ì„ë² ë”© ìƒì„±
    cached_embedder = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings=embeddings,  # ì‹¤ì œ ì„ë² ë”©ì„ ìˆ˜í–‰í•  ëª¨ë¸
        document_embedding_cache=store,  # ìºì‹œë¥¼ ì €ì¥í•  ì €ì¥ì†Œ
        namespace=embeddings.model,  # ëª¨ë¸ë³„ë¡œ ìºì‹œë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
    )

    vector_store = FAISS.from_documents(split_docs, cached_embedder)

    # ë¡œì»¬ íŒŒì¼ ì €ì¥ì†Œ ì„¤ì • - "./cache/" í´ë”ì— ìºì‹œ íŒŒì¼ ì €ì¥
    store = LocalFileStore(".cache/embeddings")

    # ìºì‹œë¥¼ ì§€ì›í•˜ëŠ” ì„ë² ë”© ìƒì„±
    cached_embedder = CacheBackedEmbeddings.from_bytes_store(
        underlying_embeddings=embeddings,  # ì‹¤ì œ ì„ë² ë”©ì„ ìˆ˜í–‰í•  ëª¨ë¸
        document_embedding_cache=store,  # ìºì‹œë¥¼ ì €ì¥í•  ì €ì¥ì†Œ
        namespace=embeddings.model,  # ëª¨ë¸ë³„ë¡œ ìºì‹œë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
    )

    vector_store = FAISS.from_documents(split_docs, cached_embedder)
    retriever = vector_store.as_retriever(search_kwargs={"k": 6})

    # ë¦¬íŠ¸ë¦¬ë²„ ë„êµ¬ ìƒì„±
    retriever_tool = create_retriever_tool(
        retriever,
        "pdf_retriever",
        f"Search and return information from the uploaded PDF file: {uploaded_file.name}. "
        f"This tool contains the full content of the document and can answer questions about it.",
        document_prompt=PromptTemplate.from_template(
            "<document><content>{page_content}</content><metadata><source>{source}</source><page>{page}</page></metadata></document>"
        ),
    )
    return retriever_tool


def create_react_agent_executor(
    selected_tools: List,
    model_name: str = "openai/gpt-4.1",
    temperature: float = 0.1,
    custom_prompt: str = None,
):
    """ReAct Agent ìƒì„±"""
    # LLM ëª¨ë¸ ì„¤ì • (OpenRouter ì‚¬ìš©)
    model = ChatOpenAI(
        temperature=temperature,
        model_name=model_name,
        api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url=os.getenv("OPENROUTER_BASE_URL"),
    )

    # ê¸°ì¡´ ë©”ëª¨ë¦¬ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (ëŒ€í™” ê¸°ë¡ ìœ ì§€)
    memory = st.session_state["memory"]

    # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if custom_prompt:
        # ì‚¬ìš©ì ë©”ì‹œì§€ì™€ í•¨ê»˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
        system_prompt = ChatPromptTemplate.from_messages(
            [("system", custom_prompt), ("placeholder", "{messages}")]
        )

        # ReAct Agent ìƒì„± (ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì ìš©)
        agent_executor = create_react_agent(
            model, selected_tools, checkpointer=memory, prompt=system_prompt
        )
    else:
        # ReAct Agent ìƒì„± (ê¸°ë³¸ í”„ë¡¬í”„íŠ¸)
        agent_executor = create_react_agent(model, selected_tools, checkpointer=memory)

    return agent_executor


def print_messages():
    """ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— í‘œì‹œ"""
    if st.session_state["messages"]:
        for msg_data in st.session_state["messages"]:
            # ë©”ì‹œì§€ê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš° (ë„êµ¬ í˜¸ì¶œ ì •ë³´ í¬í•¨)
            if isinstance(msg_data, dict):
                role = msg_data.get("role")
                content = msg_data.get("content")
                tool_calls = msg_data.get("tool_calls", [])

                with st.chat_message(role):
                    # ë„êµ¬ í˜¸ì¶œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ë¨¼ì € í‘œì‹œ
                    if tool_calls:
                        with st.expander(f"ğŸ› ï¸ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=False):
                            for i, tool_call in enumerate(tool_calls, 1):
                                st.markdown(f"**{i}. {tool_call['name']}**")

                                # ë„êµ¬ í˜¸ì¶œ ì¸ì í‘œì‹œ
                                if tool_call["args"]:
                                    st.markdown("ğŸ“ **í˜¸ì¶œ ì¸ì**")
                                    for key, value in tool_call["args"].items():
                                        # ê°’ì´ ë„ˆë¬´ ê¸´ ê²½ìš° ì¶•ì•½
                                        if isinstance(value, str) and len(value) > 100:
                                            value = value[:100] + "..."
                                        st.markdown(f"  â€¢ `{key}`: {value}")

                                # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ í‘œì‹œ
                                if "result" in tool_call:
                                    st.markdown("ğŸ“Š **ì‹¤í–‰ ê²°ê³¼**")
                                    st.write(tool_call["result"])

                                if i < len(tool_calls):
                                    st.divider()

                    # AI ì‘ë‹µ í‘œì‹œ
                    st.markdown(content)

            # ê¸°ì¡´ ChatMessage í˜•íƒœì¸ ê²½ìš° (í•˜ìœ„ í˜¸í™˜ì„±)
            else:
                st.chat_message(msg_data.role).write(msg_data.content)
    else:
        st.info(
            "ğŸ’­ ì•ˆë…•í•˜ì„¸ìš”! ReAct Agentì™€ ëŒ€í™”í•´ë³´ì„¸ìš”. ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )


def add_message(role: str, message: str, tool_calls: list = None):
    """ìƒˆë¡œìš´ ëŒ€í™” ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (ë„êµ¬ í˜¸ì¶œ ì •ë³´ í¬í•¨)"""
    msg_data = {"role": role, "content": message, "tool_calls": tool_calls or []}
    st.session_state["messages"].append(msg_data)


# ì‚¬ì´ë“œë°” UI êµ¬ì„±
with st.sidebar:
    st.header("âš™ï¸ Agent ì„¤ì •")

    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.session_state["thread_id"] = str(uuid.uuid4())  # ìƒˆë¡œìš´ thread_id ìƒì„±
        st.rerun()

    st.divider()

    # ëª¨ë¸ ì„¤ì •
    st.subheader("âœ… ëª¨ë¸ ì„¤ì •")
    selected_model = st.selectbox(
        "LLM ëª¨ë¸ ì„ íƒ",
        [
            "openai/gpt-4.1",
            "openai/gpt-oss-120b",
            "anthropic/claude-opus-4.1",
            "qwen/qwen3-235b-a22b-thinking-2507",
            "google/gemini-2.5-flash",
        ],
        index=0,
        help="ì‚¬ìš©í•  ì–¸ì–´ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.",
    )

    temperature = st.slider(
        "ğŸŒ¡ï¸ Temperature (ì°½ì˜ì„±)",
        min_value=0.0,
        max_value=1.0,
        value=0.1,
        step=0.1,
        help="0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •í™•í•˜ê³  ì¼ê´€ëœ ë‹µë³€, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì°½ì˜ì ì¸ ë‹µë³€",
    )

    # ë‹µë³€ ê¸¸ì´ ì¡°ì ˆ ìŠ¬ë¼ì´ë”
    response_length = st.slider(
        "ğŸ“ ë‹µë³€ ê¸¸ì´ ì„¤ì •",
        min_value=1,
        max_value=5,
        value=3,
        help="1: ê°„ë‹¨ (1-2ë¬¸ì¥), 2: ì§§ìŒ (1ë¬¸ë‹¨), 3: ë³´í†µ (2-3ë¬¸ë‹¨), 4: ìì„¸í•¨ (4-5ë¬¸ë‹¨), 5: ë§¤ìš° ìì„¸í•¨ (5ë¬¸ë‹¨ ì´ìƒ)",
    )

    st.divider()

    # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    st.subheader("âœï¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •")
    custom_prompt = st.text_area(
        "Agent í”„ë¡¬í”„íŠ¸ í¸ì§‘",
        value=st.session_state["custom_prompt"],
        height=100,
        help="Agentì˜ ì—­í• ê³¼ í–‰ë™ì„ ì •ì˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    )
    st.session_state["custom_prompt"] = custom_prompt

    st.divider()

    # ë„êµ¬ ì„ íƒ
    st.subheader("ğŸ› ï¸ ë„êµ¬ ì„ íƒ")

    # ì›¹ ê²€ìƒ‰ ë„êµ¬
    use_web_search = st.checkbox(
        "ğŸŒ ì›¹ ê²€ìƒ‰ ë„êµ¬",
        value=True,
        help="ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.",
    )

    # TavilySearch ìƒì„¸ ì„¤ì • (ì›¹ ê²€ìƒ‰ ë„êµ¬ê°€ ì„ íƒëœ ê²½ìš°ì—ë§Œ í‘œì‹œ)
    if use_web_search:
        with st.expander("ğŸ”§ ì›¹ ê²€ìƒ‰ ìƒì„¸ ì„¤ì •", expanded=False):
            # Topic ì„ íƒ
            tavily_topic = st.selectbox(
                "ê²€ìƒ‰ ì£¼ì œ (Topic)",
                options=["general", "news", "finance"],
                index=["general", "news", "finance"].index(
                    st.session_state["tavily_topic"]
                ),
                help="ê²€ìƒ‰í•  ì£¼ì œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.",
            )
            st.session_state["tavily_topic"] = tavily_topic

            # Max results ìŠ¬ë¼ì´ë”
            tavily_max_results = st.slider(
                "ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜",
                min_value=1,
                max_value=10,
                value=st.session_state["tavily_max_results"],
                help="ê²€ìƒ‰ì—ì„œ ë°˜í™˜ë°›ì„ ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.",
            )
            st.session_state["tavily_max_results"] = tavily_max_results

            # Include domains í…ìŠ¤íŠ¸ ì…ë ¥
            tavily_include_domains = st.text_input(
                "í¬í•¨í•  ë„ë©”ì¸ (ì„ íƒì‚¬í•­)",
                value=st.session_state["tavily_include_domains"],
                placeholder="ì˜ˆ: naver.com, daum.net (ì‰¼í‘œë¡œ êµ¬ë¶„)",
                help="íŠ¹ì • ë„ë©”ì¸ì—ì„œë§Œ ê²€ìƒ‰í•˜ë ¤ë©´ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”.",
            )
            st.session_state["tavily_include_domains"] = tavily_include_domains

            # Time range ì„ íƒ
            time_range_options = ["None", "day", "week", "month", "year"]
            time_range_display = ["ì œí•œ ì—†ìŒ", "1ì¼", "1ì£¼", "1ê°œì›”", "1ë…„"]
            current_time_range = st.session_state["tavily_time_range"]
            if current_time_range is None:
                current_index = 0
            else:
                current_index = time_range_options.index(current_time_range)

            tavily_time_range_index = st.selectbox(
                "ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„",
                options=range(len(time_range_display)),
                format_func=lambda x: time_range_display[x],
                index=current_index,
                help="ì–¼ë§ˆë‚˜ ìµœê·¼ ì •ë³´ê¹Œì§€ ê²€ìƒ‰í• ì§€ ì„¤ì •í•˜ì„¸ìš”.",
            )
            tavily_time_range = (
                time_range_options[tavily_time_range_index]
                if tavily_time_range_index != 0
                else None
            )
            st.session_state["tavily_time_range"] = tavily_time_range

    # Python ì½”ë“œ ì‹¤í–‰ ë„êµ¬
    use_python_repl = st.checkbox(
        "ğŸ Python ì½”ë“œ ì‹¤í–‰ ë„êµ¬",
        value=True,
        help="Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ê³„ì‚°, ë°ì´í„° ë¶„ì„, ì°¨íŠ¸ ìƒì„± ë“±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
    )

    # PDF ì—…ë¡œë“œ ë° ê²€ìƒ‰ ë„êµ¬
    st.subheader("ğŸ“„ PDF ë¬¸ì„œ ë„êµ¬")
    uploaded_pdf = st.file_uploader(
        "PDF íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf"],
        help="PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ë‚´ìš©ì„ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ë„êµ¬ê°€ ì¶”ê°€ë©ë‹ˆë‹¤.",
    )

    use_pdf_retriever = uploaded_pdf is not None


# ë„êµ¬ ì„¤ì • ë° Agent ìƒì„±
def setup_agent():
    """ì„ íƒëœ ë„êµ¬ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ Agent ì„¤ì •"""
    # í˜„ì¬ ë„êµ¬ ì„¤ì •ì„ ë¬¸ìì—´ë¡œ ìƒì„± (ë³€ê²½ ê°ì§€ìš©)
    pdf_name = uploaded_pdf.name if uploaded_pdf else None
    current_config = {
        "web_search": use_web_search,
        "python_repl": use_python_repl,
        "pdf_retriever": use_pdf_retriever,
        "pdf_name": pdf_name,
        "model": selected_model,
        "temperature": temperature,
        "response_length": response_length,
        "tavily_topic": st.session_state["tavily_topic"],
        "tavily_max_results": st.session_state["tavily_max_results"],
        "tavily_include_domains": st.session_state["tavily_include_domains"],
        "tavily_time_range": st.session_state["tavily_time_range"],
        "custom_prompt": custom_prompt,
    }
    config_str = str(sorted(current_config.items()))

    # ì„¤ì •ì´ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ Agent ì¬ìƒì„±
    if (
        st.session_state["current_tool_config"] != config_str
        or st.session_state["agent"] is None
    ):

        tools = []

        # ì›¹ ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€
        if use_web_search:
            tools.append(
                create_web_search_tool(
                    topic=st.session_state["tavily_topic"],
                    max_results=st.session_state["tavily_max_results"],
                    include_domains=st.session_state["tavily_include_domains"],
                    time_range=st.session_state["tavily_time_range"],
                )
            )

        # Python REPL ë„êµ¬ ì¶”ê°€
        if use_python_repl:
            tools.append(create_python_repl_tool())

        # PDF ë¦¬íŠ¸ë¦¬ë²„ ë„êµ¬ ì¶”ê°€
        if use_pdf_retriever:
            pdf_tool = create_pdf_retriever_tool(uploaded_pdf)
            if pdf_tool:
                tools.append(pdf_tool)

        # Agent ìƒì„± (ë„êµ¬ê°€ ìˆì„ ë•Œë§Œ)
        if tools:
            agent = create_react_agent_executor(
                selected_tools=tools,
                model_name=selected_model,
                temperature=temperature,
                custom_prompt=custom_prompt,
            )
            # ì„¤ì • ì—…ë°ì´íŠ¸
            st.session_state["current_tool_config"] = config_str
            return agent, tools
        else:
            st.session_state["current_tool_config"] = config_str
            return None, []

    # ì„¤ì •ì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê¸°ì¡´ Agent ì‚¬ìš©
    else:
        return st.session_state["agent"], st.session_state["tools"]


# Agent ì„¤ì •
agent, current_tools = setup_agent()
st.session_state["agent"] = agent
st.session_state["tools"] = current_tools

# ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
print_messages()

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input(
    "ğŸ’¬ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! Agentê°€ í•„ìš”í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ë“œë¦½ë‹ˆë‹¤."
)

# ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬
if user_input:
    if st.session_state["agent"] is None:
        st.error("âš ï¸ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ ì‚¬ìš©í•  ë„êµ¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
        st.chat_message("user").write(user_input)
        add_message("user", user_input)

        # Agent ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
        with st.chat_message("assistant"):

            try:
                # ì„¤ì • ì •ë³´
                config = {"configurable": {"thread_id": st.session_state["thread_id"]}}
                inputs = {"messages": [("human", user_input)]}

                # Agent ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)
                full_response = ""
                tool_calls = []

                with st.spinner("ğŸ¤” Agentê°€ ìƒê°í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    # Agent ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ìƒì„±
                    response = st.session_state["agent"].invoke(inputs, config)

                    # ëª¨ë“  ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë„êµ¬ í˜¸ì¶œ ë° AI ì‘ë‹µ ì¶”ì¶œ
                    if response and "messages" in response:
                        for msg in response["messages"]:
                            # ë„êµ¬ í˜¸ì¶œ ë©”ì‹œì§€ í™•ì¸
                            if hasattr(msg, "tool_calls") and msg.tool_calls:
                                for tool_call in msg.tool_calls:
                                    tool_info = {
                                        "name": tool_call.get("name", "Unknown Tool"),
                                        "args": tool_call.get("args", {}),
                                        "id": tool_call.get("id", "unknown"),
                                    }
                                    tool_calls.append(tool_info)

                            # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ë©”ì‹œì§€ í™•ì¸
                            elif hasattr(msg, "type") and msg.type == "tool":
                                # ê¸°ì¡´ ë„êµ¬ í˜¸ì¶œ ì •ë³´ì— ê²°ê³¼ ì¶”ê°€
                                tool_id = getattr(msg, "tool_call_id", None)
                                content = getattr(msg, "content", "")
                                for tool_call in tool_calls:
                                    if tool_call["id"] == tool_id:
                                        tool_call["result"] = content
                                        break

                        # AIì˜ ìµœì¢… ì‘ë‹µ ì¶”ì¶œ
                        ai_messages = [
                            msg
                            for msg in response["messages"]
                            if hasattr(msg, "type") and msg.type == "ai"
                        ]
                        if ai_messages:
                            full_response = ai_messages[-1].content
                        else:
                            # AIMessage íƒ€ì…ì´ ì•„ë‹Œ ê²½ìš° content ì†ì„± ì§ì ‘ ì ‘ê·¼
                            for msg in reversed(response["messages"]):
                                if hasattr(msg, "content") and msg.content.strip():
                                    full_response = msg.content
                                    break

                # ë„êµ¬ í˜¸ì¶œ ì •ë³´ í‘œì‹œ (í™•ì¥ ê°€ëŠ¥í•œ ì„¹ì…˜ìœ¼ë¡œ)
                if tool_calls:
                    with st.expander(f"ğŸ› ï¸ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=False):
                        for i, tool_call in enumerate(tool_calls, 1):
                            st.markdown(f"**{i}. {tool_call['name']}**")

                            # ë„êµ¬ í˜¸ì¶œ ì¸ì í‘œì‹œ
                            if tool_call["args"]:
                                st.markdown("ğŸ“ **í˜¸ì¶œ ì¸ì:**")
                                for key, value in tool_call["args"].items():
                                    # ê°’ì´ ë„ˆë¬´ ê¸´ ê²½ìš° ì¶•ì•½
                                    if isinstance(value, str) and len(value) > 100:
                                        value = value[:100] + "..."
                                    st.markdown(f"  â€¢ `{key}`: {value}")

                            # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ í‘œì‹œ
                            if "result" in tool_call:
                                st.markdown("ğŸ“Š **ì‹¤í–‰ ê²°ê³¼**")
                                st.markdown(tool_call["result"])

                            if i < len(tool_calls):
                                st.divider()

                # AI ìµœì¢… ì‘ë‹µ í‘œì‹œ
                if full_response:
                    st.markdown(full_response)
                    add_message("assistant", full_response, tool_calls)
                else:
                    st.error("ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                st.info("ğŸ’¡ ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")

# ì‚¬ì´ë“œë°” í•˜ë‹¨ì— í˜„ì¬ ì„¤ì • ì •ë³´ í‘œì‹œ
with st.sidebar:
    st.divider()
    st.markdown("### ğŸ“Š í˜„ì¬ ì„¤ì •")
    st.caption(f"**ëª¨ë¸:** {selected_model}")
    st.caption(f"**Temperature:** {temperature}")
    st.caption(f"**ë‹µë³€ ê¸¸ì´:** {response_length}")
    st.caption(f"**Thread ID:** {st.session_state['thread_id'][:8]}...")
    st.caption(f"**í™œì„± ë„êµ¬ ê°œìˆ˜:** {len(current_tools)}")

    # ë„êµ¬ë³„ ìƒì„¸ ì •ë³´
    if current_tools:
        with st.expander("ğŸ”§ ë„êµ¬ ìƒì„¸ ì •ë³´"):
            for i, tool in enumerate(current_tools):
                tool_name = getattr(tool, "name", f"Tool {i+1}")
                tool_desc = (
                    getattr(tool, "description", "No description available")[:100]
                    + "..."
                )
                st.caption(f"**{tool_name}:** {tool_desc}")
