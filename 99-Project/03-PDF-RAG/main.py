# Streamlit ë° ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st
import os

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.messages.chat import ChatMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.vectorstores import FAISS
from langchain_teddynote.prompts import load_prompt
from langchain_teddynote import logging

# í™˜ê²½ ì„¤ì •
from dotenv import load_dotenv

# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
load_dotenv(override=True)

# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com
logging.langsmith("LangChain-Tutorial")

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± (íŒŒì¼ ì—…ë¡œë“œ ë° ì„ë² ë”© ì €ì¥ì„ ìœ„í•¨)
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# ë²¡í„° ì„ë² ë”© ì €ì¥ í´ë”
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

# Streamlit ì•± ì œëª© ì„¤ì •
st.title("ğŸ“„ PDF ê¸°ë°˜ QA ì‹œìŠ¤í…œ")

# Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì•± ì¬ì‹¤í–‰ ì‹œì—ë„ ëŒ€í™” ê¸°ë¡ ìœ ì§€)
if "messages" not in st.session_state:
    # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    st.session_state["messages"] = []

if "chain" not in st.session_state:
    # RAG ì²´ì¸ ì´ˆê¸°í™” (íŒŒì¼ ì—…ë¡œë“œ ì „ê¹Œì§€ëŠ” None)
    st.session_state["chain"] = None

# ì‚¬ì´ë“œë°” UI êµ¬ì„±
with st.sidebar:
    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
    clear_btn = st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”")

    # PDF íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
    uploaded_file = st.file_uploader(
        "ğŸ“ PDF íŒŒì¼ ì—…ë¡œë“œ",
        type=["pdf"],
        help="PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.",
    )

    # LLM ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
    selected_model = st.selectbox(
        "LLM ëª¨ë¸ ì„ íƒ",
        ["gpt-4.1", "gpt-4.1-mini"],
        index=0,
        help="ì‚¬ìš©í•  ì–¸ì–´ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.",
    )

    # ë‹µë³€ ê¸¸ì´ ì¡°ì ˆ ìŠ¬ë¼ì´ë”
    response_length = st.slider(
        "ğŸ“ ë‹µë³€ ê¸¸ì´ ì„¤ì •",
        min_value=1,
        max_value=5,
        value=3,
        help="1: ê°„ë‹¨ (1-2ë¬¸ì¥), 2: ì§§ìŒ (1ë¬¸ë‹¨), 3: ë³´í†µ (2-3ë¬¸ë‹¨), 4: ìì„¸í•¨ (4-5ë¬¸ë‹¨), 5: ë§¤ìš° ìì„¸í•¨ (5ë¬¸ë‹¨ ì´ìƒ)",
    )

    # ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ ì¡°ì ˆ ìŠ¬ë¼ì´ë”
    search_k = st.slider(
        "ğŸ” ê²€ìƒ‰ ë¬¸ì„œ ê°œìˆ˜ ì„¤ì •",
        min_value=4,
        max_value=10,
        value=6,
        help="ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ëª‡ ê°œê¹Œì§€ ê²€ìƒ‰í• ì§€ ì„¤ì •í•©ë‹ˆë‹¤. ë§ì„ìˆ˜ë¡ ë” ë§ì€ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì§€ë§Œ ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ì§‘ë‹ˆë‹¤.",
    )


# ì´ì „ ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
def print_messages():
    """ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ ìˆœì„œëŒ€ë¡œ í™”ë©´ì— í‘œì‹œ"""
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
def add_message(role, message):
    """ìƒˆë¡œìš´ ëŒ€í™” ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥"""
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# PDF íŒŒì¼ì„ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (ìºì‹œ ì ìš©ìœ¼ë¡œ ì¬ì²˜ë¦¬ ë°©ì§€)
@st.cache_resource(show_spinner="ğŸ“„ ì—…ë¡œë“œëœ PDFë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
def embed_file(file, search_k=6):
    # ì—…ë¡œë“œëœ íŒŒì¼ì„ ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ(Load Documents)
    loader = PDFPlumberLoader(file_path)
    docs = loader.load()

    # ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í•  (ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # ê° ì²­í¬ì˜ ìµœëŒ€ ë¬¸ì ìˆ˜
        chunk_overlap=50,  # ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜ (ë¬¸ë§¥ ì—°ê²°ì„± ìœ ì§€)
    )
    split_documents = text_splitter.split_documents(docs)

    # ë‹¨ê³„ 3: ì„ë² ë”©(Embedding) ìƒì„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ë‹¨ê³„ 4: FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•¨)
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)

    # ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriever) ìƒì„± (ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ì°¾ëŠ” ì—­í• )
    retriever = vectorstore.as_retriever(
        search_kwargs={"k": search_k}  # ì„¤ì •ëœ ê°œìˆ˜ì˜ ê´€ë ¨ ë¬¸ì„œ ë°˜í™˜
    )
    return retriever


# RAG ì²´ì¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ê²€ìƒ‰-ìƒì„± íŒŒì´í”„ë¼ì¸)
def create_chain(retriever, model_name="gpt-4.1", response_length=3):
    """Retrieval-Augmented Generation ì²´ì¸ ìƒì„±"""
    # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
    prompt = load_prompt("prompts/pdf-rag.yaml", encoding="utf-8")

    # ë‹¨ê³„ 7: OpenAI ì–¸ì–´ëª¨ë¸ ì´ˆê¸°í™” (temperature=0ìœ¼ë¡œ ì¼ê´€ëœ ë‹µë³€ ìƒì„±)
    llm = ChatOpenAI(
        model_name=model_name, temperature=0  # ì°½ì˜ì„±ë³´ë‹¤ ì •í™•ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
    )

    # ë‹¨ê³„ 8: RAG ì²´ì¸ êµ¬ì„± (ê²€ìƒ‰ â†’ í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ ì¶œë ¥ íŒŒì‹±)
    chain = (
        {
            "context": retriever,  # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            "question": RunnablePassthrough(),  # ì‚¬ìš©ì ì§ˆë¬¸ ì „ë‹¬
            "response_length": lambda _: response_length,  # ë‹µë³€ ê¸¸ì´ ì„¤ì • ì „ë‹¬
        }
        | prompt  # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
        | llm  # ì–¸ì–´ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
        | StrOutputParser()  # ë¬¸ìì—´ í˜•íƒœë¡œ ê²°ê³¼ íŒŒì‹±
    )
    return chain


# PDF íŒŒì¼ ì—…ë¡œë“œ ì‹œ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
if uploaded_file:
    # ì—…ë¡œë“œëœ íŒŒì¼ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜ (ì„¤ì •ëœ ê²€ìƒ‰ ê°œìˆ˜ ì ìš©)
    retriever = embed_file(uploaded_file, search_k=search_k)
    # ì„ íƒëœ ëª¨ë¸ê³¼ ë‹µë³€ ê¸¸ì´ ì„¤ì •ìœ¼ë¡œ RAG ì²´ì¸ ìƒì„±
    chain = create_chain(
        retriever, model_name=selected_model, response_length=response_length
    )
    st.session_state["chain"] = chain
    st.success(
        f"âœ… '{uploaded_file.name}' íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤! (ê²€ìƒ‰ ë¬¸ì„œ: {search_k}ê°œ)"
    )

# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ í´ë¦­ ì‹œ
if clear_btn:
    st.session_state["messages"] = []
    st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ì°½
user_input = st.chat_input("ğŸ“ PDF ë‚´ìš©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ í‘œì‹œë¥¼ ìœ„í•œ ë¹ˆ ê³µê°„
warning_msg = st.empty()

# ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±
if user_input:
    chain = st.session_state["chain"]

    if chain is not None:
        # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
        st.chat_message("user").write(user_input)

        # AI ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‹¤ì‹œê°„ í‘œì‹œ
        with st.chat_message("assistant"):
            # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ
            container = st.empty()
            ai_answer = ""

            # RAG ì²´ì¸ì„ í†µí•´ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
            response = chain.stream(user_input)
            for token in response:
                ai_answer += token
                # ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ ì—…ë°ì´íŠ¸
                container.markdown(ai_answer)

        # ëŒ€í™” ê¸°ë¡ì„ ì„¸ì…˜ì— ì €ì¥
        add_message("user", user_input)
        add_message("assistant", ai_answer)
    else:
        # PDF íŒŒì¼ ë¯¸ì—…ë¡œë“œ ì‹œ ê²½ê³  ë©”ì‹œì§€
        warning_msg.error("âš ï¸ ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
